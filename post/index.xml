<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Aman Bagrecha</title>
    <link>https://amanbagrecha.github.io/post/</link>
      <atom:link href="https://amanbagrecha.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 22 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://amanbagrecha.github.io/media/icon_hu34b7b96a7941bf879d4219a76e82104f_4254_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://amanbagrecha.github.io/post/</link>
    </image>
    
    <item>
      <title>Vector tiles and Docker using pg_tilerserv</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/vector-tiles-and-docker-using-pg-tilerserv/</link>
      <pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/vector-tiles-and-docker-using-pg-tilerserv/</guid>
      <description>&lt;p&gt;In this blog we look at how to serve your geospatial data as vector tiles using pg_tileserv in a docker container.&lt;/p&gt;
&lt;h2 id=&#34;what-are-vector-tiles&#34;&gt;What are vector tiles?&lt;/h2&gt;
&lt;p&gt;Vector Tiles are similar to raster tiles, but instead of serving images, vector tiles serve geospatial data which are vectors themselves and not images. This allows for reduced data transfer over a network, faster loading while allowing client side rendering. Moreover, vector tiles allow for flexible styling of your geospatial data since it renders on the client side. All this is not possible with raster tiles and hence vector tiles have gained traction in the last few years.&lt;/p&gt;
&lt;p&gt;One of the most popular specifications to serve vector tiles is mapbox vector tiles, utilized by many open source tile servers.&lt;/p&gt;
&lt;p&gt;Because PostGIS can create mapbox vector tiles from vector data, it becomes easy to serve them over the web. Many tileservers use the power of this postGIS functionality to serve vector tiles over the web.&lt;/p&gt;
&lt;p&gt;As for a visual understanding as to what is different between vector and raster tiles, the following image illustrates that. The red bounding box is the response to clients request to serve vector tiles. Notice the format is &lt;code&gt;pbf&lt;/code&gt; as opposed to &lt;code&gt;png&lt;/code&gt; for raster tiles.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/S5uzLpN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;why-use-docker-for-this&#34;&gt;Why use docker for this?&lt;/h2&gt;
&lt;p&gt;Using docker would expedite the process of starting and &amp;ldquo;actually&amp;rdquo; using the applications. It is like sharing your machine with others so that they do not have to install anything to get started. For this reason, it makes complete sense to use docker for moderate to high complexity projects.&lt;/p&gt;
&lt;h2 id=&#34;what-is-pg_tileserve&#34;&gt;What is pg_tileserve?&lt;/h2&gt;
&lt;p&gt;To create vector tiles, and serve them on the web, you need a middleware that can talk to the database and also serve them on the web. Since pg_tileserve uses a postgis function under the hood, it becomes a default choice to add a lightweight service to serve vector tiles. pg_tileserv returns Mapbox Vector tiles on input of vector geometry. In addition to reading tables from the database, it can handle complex functions to meet our needs.&lt;/p&gt;
&lt;p&gt;ST_asMVT, an aggregate function which is used under the hood for pg_tileserv, returns mapbox vector tile format based on google protobuf. While there are other formats such as MBtiles which is sqlite based binary file (can be opened in sqlite), Mapbox Vector Tile format seems to be winning this race and is thus the most popular format currently.&lt;/p&gt;
&lt;h3 id=&#34;to-get-started-with-serving-your-vector-data-to-the-web-using-pg_tileserv-we-follow-the-below-mentioned-steps&#34;&gt;To get started with serving your vector data to the web using pg_tileserv, we follow the below mentioned steps&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Download &lt;a href=&#34;https://github.com/CrunchyData/pg_tileserv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pg_tileserv&lt;/a&gt; folder from &lt;a href=&#34;https://downgit.github.io/#/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;down-git&lt;/a&gt; website and save it to your local directory. 
&lt;img src=&#34;https://i.imgur.com/QkF6OF9.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The folder contains all the files required to start a docker container and serve vector tiles.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;â””â”€â”€â”€data/  â€” would contain all your vector data
â””â”€â”€â”€load-data.sh â€” shell script to load data into PostgreSQL
â””â”€â”€â”€pg_tileserv.env â€” database URL to connect
â””â”€â”€â”€docker-compose.yml â€” 
â””â”€â”€â”€pg.env â€” environment variable for database
â””â”€â”€â”€cleanup.sh â€” assemble multiple containers
â””â”€â”€â”€README â€” guide to setup docker by Just van den Broecke
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Next, Modify &lt;code&gt;docker-compose.yml&lt;/code&gt; file under &lt;strong&gt;build-&amp;gt;context&lt;/strong&gt; to point to the docker file &lt;a href=&#34;https://github.com/CrunchyData/pg_tileserv.git&#34;&gt;https://github.com/CrunchyData/pg_tileserv.git&lt;/a&gt;. Since we did not clone the repository, we specify the Dockerfile using the git link.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/AzclY3c.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Dump all your geospatial data into &lt;code&gt;data&lt;/code&gt; dir. This directory will be &lt;em&gt;mounted&lt;/em&gt; to the container, once we start it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the &lt;code&gt;pg_tileserv.env&lt;/code&gt; environment file as you wish, to specify the name and password of your database.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notes on env files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pg_tilerserv.env&lt;/code&gt; file contains the database url which is of the format &lt;code&gt;postgres://your-username:your-password@localhost:5432/your-database-name&lt;/code&gt; while &lt;code&gt;pg.env&lt;/code&gt; contains credentials for postgres database.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notes on docker-compose file&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are mounting &lt;code&gt;data&lt;/code&gt; dir from our local system to the work dir in the docker container.&lt;/li&gt;
&lt;li&gt;We are mapping port 7800 from our local machine to 7800 to the pg_tileserv container.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Start Docker Desktop and run &lt;code&gt;docker-compose build&lt;/code&gt; in the command line. It will download the image needed from the dockerfile specified. It only downloads the latest alpine image and all other dependencies are installed in the build.&lt;/p&gt;
&lt;p&gt;Once the database setup is done, we now load data into the database by running either &lt;code&gt;load-data.sh&lt;/code&gt; shell script (or) the following command,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#Load data using shp2pgsql 
docker-compose exec pg_tileserv_db sh -c &amp;quot;shp2pgsql -D -s 4326 /work/ne_50m_admin_0_countries.shp | psql -U tileserv -d tileserv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above command opens a terminal inside the pg_tileserv_db container and runs the &lt;code&gt;shp2pgsql&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;ogr2ogr&lt;/code&gt; command line tool if your data is anything other than shapefile. Read this blog by &lt;a href=&#34;https://blog.crunchydata.com/blog/loading-data-into-postgis-an-overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kat Batuigas&lt;/a&gt; to know how to do it.&lt;/p&gt;
&lt;p&gt;Finally, run &lt;code&gt;docker-compose up&lt;/code&gt; to start the service. You&amp;rsquo;d see both containers starting up and your web app being served on port 7800. If you do not see this, stop the container and run again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Gy4QlTL.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;On running the web app in the browser we see our tables visible under Table Layers and the schema it belongs to. We added a few additional layers (public.hydrants and a function layer following steps from &lt;code&gt;README.md&lt;/code&gt;) to play around with it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/CwmhUdK.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;endnote&#34;&gt;Endnote&lt;/h2&gt;
&lt;p&gt;We looked at serving vector data as tiles using pg_tileserv and docker container. Docker enables reproducibility and expedites the process of running a web app. Although there are numerous open-source tile servers available, each has its use case and would require testing them out to identify the best tileserver for your use case. You can read a long list of tileservers &lt;a href=&#34;https://github.com/mapbox/awesome-vector-tiles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So next time you think to serve large vector data on the web app, make sure to use vector tiles built inside a docker container. It will surely simplify things!&lt;/p&gt;
&lt;p&gt;Source:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;CrunchyData/pg_tileserv: A very thin PostGIS-only tile server in Go. Takes in HTTP tile requests, executes SQL, returns MVT tiles. (&lt;a href=&#34;https://github.com/CrunchyData/pg_tileserv/&#34;&gt;https://github.com/CrunchyData/pg_tileserv/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lightweight PostGIS Web Services Using pg tileserv and pg featureserv (&lt;a href=&#34;https://www.youtube.com/watch?v=TXPtocZWr78&amp;amp;t=1s&amp;amp;ab_channel=CrunchyData&#34;&gt;https://www.youtube.com/watch?v=TXPtocZWr78&amp;amp;t=1s&amp;amp;ab_channel=CrunchyData&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reference | Vector tiles | Mapbox (&lt;a href=&#34;https://docs.mapbox.com/vector-tiles/reference/&#34;&gt;https://docs.mapbox.com/vector-tiles/reference/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vector Tiles â€“ Geoinformation HSR (&lt;a href=&#34;https://giswiki.hsr.ch/Vector_Tiles&#34;&gt;https://giswiki.hsr.ch/Vector_Tiles&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Polygonize Raster and Compute Zonal-Statistics in Python</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</guid>
      <description>&lt;p&gt;The output of a clustering algorithm is a raster. But when you want to compute statistics of the clustered raster, it needs to be polygonized.&lt;/p&gt;
&lt;p&gt;A simple way to perform this action is using the gdal command line &lt;code&gt;gdal_polygonize.py&lt;/code&gt; script. This script requires the output file format, input raster file and output name of the vector file. You can additionally mask pixel values which you don&amp;rsquo;t want to convert to polygons. For this example, we would consider a single band image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python gdal_polygonize.py raster_file -f &amp;quot;ESRI Shapefile&amp;quot; vector_file.shp  layername atrributefieldname
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;--nomask&lt;/code&gt; allows to include nodata values in the shapefile&lt;/p&gt;
&lt;p&gt;&lt;code&gt;atrributefieldname&lt;/code&gt; should always be preceded with &lt;code&gt;layername&lt;/code&gt; else it would result in an error.&lt;/p&gt;
&lt;p&gt;The output would result in a vector layer. The number of output polygons is equal to the number of non-NA values. Each neighbouring cell (pixel) which is connected in the raster having the same value is combined to form a single polygon.&lt;/p&gt;
&lt;p&gt;For instance, consider this 4 x 4 raster. When converted to vector, it resulted in 6 polygons. Note that disconnected similar values form an independent polygon. Each polygon will have an attribute as its pixel value from the raster, in the data type of the image. These would end up being a pair of (polygon, value) for each feature found in the image.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xeJ4BGa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Converting Raster to Vector using GDAL. The output polygon has attribute associated with its raster value &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Another way to polygonize raster programmatically is to use the &lt;code&gt;rasterio&lt;/code&gt; library. Since rasterio utilizes GDAL under the hood, it also performs similar action and results in a pair of geometry and raster value. We create a tuple of dictionaries to store each feature output.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# code to polygonize using rasterio
from rasterio import features

# read the raster and polygonize
with rasterio.open(cluster_image_path) as src:
    image = src.read(1, out_dtype=&#39;uint16&#39;) 
    #Make a mask!
    mask = image != 0
# `results` contains a tuple. With each element in the tuple representing a dictionary containing the feature (polygon) and its associated raster value
results = ( {&#39;properties&#39;: {&#39;cluster_id&#39;: int(v)}, &#39;geometry&#39;: s} 
            for (s, v) in (features.shapes(image, mask=mask, transform=src.transform)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the raster polygonized, we can use &lt;code&gt;rasterstats&lt;/code&gt; library to calculate zonal statistics. We use this library since there is no in-built functionality for rasterio to calculate it.&lt;/p&gt;
&lt;p&gt;This library has a function &lt;code&gt;zonal_stats&lt;/code&gt; which takes in a vector layer and a raster to calculate the zonal statistics. Read more &lt;a href=&#34;https://pythonhosted.org/rasterstats/manual.html#zonal-statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The parameters to the function are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;vectors: path to an vector source or geo-like python objects&lt;/li&gt;
&lt;li&gt;raster: ndarray or path to a GDAL raster source&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and various other options which can be found &lt;a href=&#34;https://github.com/perrygeo/python-rasterstats/blob/master/src/rasterstats/main.py#L34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To create a vector layer from the tuple &lt;code&gt;results&lt;/code&gt;, we use geopandas. There are other libraries (such as fiona) which can also create vector geometry from shapely objects.&lt;/p&gt;
&lt;p&gt;For raster, we pass the &lt;code&gt;.tif&lt;/code&gt; file directly to &lt;code&gt;zonal_stats&lt;/code&gt;. The final code looks like the following&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rasterstats import zonal_stats

in_shp = gpd.GeoDataFrame.from_features(results).set_crs(crs=src.crs)

# stats parameter takes in various statistics that needs to be computed 
statistics= zonal_stats(in_shp,image,stats=&#39;min, max, mean, median&#39;,
                geojson_out=True, nodata = -999)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is a geojson generator when &lt;code&gt;geojson_out&lt;/code&gt; is True. we can convert the geojson to dataframe and export as csv for further processing.&lt;/p&gt;
&lt;p&gt;This way, with the help of geopandas, rasterstats and rasterio, we polygonize the raster and calculate zonal statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three ways to Programmatically change projection of raw CSV</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/three-ways-to-change-projection-of-raw-csv/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/three-ways-to-change-projection-of-raw-csv/</guid>
      <description>&lt;p&gt;Often, field values are collected in the Geographic Coordinate System as ASCII or CSV so that it can be universally used. But when you want to perform any kind of analysis on these points, there is a need to reproject them into a Projected Coordinate Reference System for the specific area. Although there are many ways that exist now with desktop GIS, these methods can be cumbersome if you have thousands of files to reproject.&lt;/p&gt;
&lt;p&gt;This task of reprojecting raw CSV can be accomplished using GDAL although it is not straightforward. It requires an indication of geographic data of a CSV file which is provided using VRT (GDAL virtual Raster). More advanced tools now exist which are either built on top of GDAL or are very similar. pyproj and GeoPandas are two such libraries which can help us reproject our raw CSV on-the-fly.&lt;/p&gt;
&lt;p&gt;We first look at how this task can be accomplished using the GDAL command line.&lt;/p&gt;
&lt;h3 id=&#34;reproject-csv-using-ogr2ogr&#34;&gt;Reproject CSV using ogr2ogr&lt;/h3&gt;
&lt;p&gt;This example shows using ogr2ogr to reproject the Coordinate Reference System of a CSV file with the longitude, latitude coordinates stored as columns in the &lt;code&gt;.csv&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ogr2ogr -f CSV -lco GEOMETRY=AS_XY -t_srs EPSG:4326 output.csv input.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;-lco GEOMETRY=AS_XY&lt;/code&gt; : Layer creation option with XY columns added in output csv.&lt;/p&gt;
&lt;p&gt;In the above code, &lt;code&gt;input.vrt&lt;/code&gt; is a GDAL virtual raster which has to be created prior to running the command. It points to the CSV file which has the location data stored as columns (lon, lat)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!--input.vrt pointing to the filename.csv--&amp;gt;
&amp;lt;OGRVRTDataSource&amp;gt; 
  &amp;lt;OGRVRTLayer name=&amp;quot;layername&amp;quot;&amp;gt; 
    &amp;lt;SrcDataSource&amp;gt;filename.csv&amp;lt;/SrcDataSource&amp;gt; 
    &amp;lt;GeometryType&amp;gt;wkbPoint&amp;lt;/GeometryType&amp;gt; 
    &amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt; 
    &amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt; 
  &amp;lt;/OGRVRTLayer&amp;gt; 
&amp;lt;/OGRVRTDataSource&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Why did we create this vrt file?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, VRT is a virtual gdal driver which allows for lazy processing. Often, we have to save intermediary outputs on our local disk, which could potentially take a lot of space. To avoid that, VRT allows to store the processing in an xml encoding and performs all intermediary action at once, in the final step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But what does the above xml mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first line &lt;code&gt;&amp;lt;OGRVRTDataSource&amp;gt;&lt;/code&gt; is the root element. &lt;code&gt;&amp;lt;OGRVRTLayer name=&amp;quot;layername&amp;quot;&amp;gt;&lt;/code&gt; corresponds with the &lt;code&gt;&amp;lt;SrcDataSource&amp;gt; filename.csv &amp;lt;/SrcDataSource&amp;gt;&lt;/code&gt; and points to the CSV we want to reproject. &lt;code&gt;&amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt;&lt;/code&gt; should correspond with the EPSG of the coordinates stored in the CSV file. &lt;code&gt;&amp;lt;GeometryType&amp;gt; wkbPoint &amp;lt;/GeometryType&amp;gt;&lt;/code&gt; is the format that coordinates are stored as. Lastly, &lt;code&gt;&amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt;&lt;/code&gt; indicates the columns corresponding to lon and lat in csv.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/HefHXvu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Reprojecting CSV from EPSG:4326 to EPSG:32644 &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Hence, by running the above GDAL command, we would be able to reproject our csv. By writing a bash script, this method can be scaled to thousands of files. But the intermediary &lt;code&gt;.vrt&lt;/code&gt; file is messy to handle and it would be nice to avoid it. Luckily for us, there are high-level libraries in python which would avoid such hassle.&lt;/p&gt;
&lt;h2 id=&#34;using-geopandas&#34;&gt;Using GeoPandas&lt;/h2&gt;
&lt;p&gt;The modern geospatial libraries are &lt;em&gt;low code tools&lt;/em&gt;. One such excellent library is GeoPandas, which is built on top of fiona, which in-turn is built on top of GDAL. Geopandas allows us to read, project and modify files on-the-fly. Additionally, the ability to create geometry from &lt;code&gt;lat, lon&lt;/code&gt; allows us to pass our CSV files and modify its CRS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;in_path = &#39;./&#39;
out_path = &#39;./output&#39;
files= [f for f in os.listdir(in_path) if f.endswith(&#39;.csv&#39;)]
input_crs = &#39;EPSG:4326&#39;
output_crs = &#39;EPSG:32644&#39;

if not os.path.exists(out_path):
    os.mkdir(out_path)

for file in files:
    df = pd.read_csv(file, header=None)
    gdf = gpd.GeoDataFrame(
        df, crs=input_crs , geometry=gpd.points_from_xy(df.iloc[:,0], df.iloc[:,1]))

    gdf.to_crs(output_crs, inplace=True)
    gdf.iloc[:,0] = gdf.geometry.x # replace x
    gdf.iloc[:,1] = gdf.geometry.y # replace y
    
    # export reprojected csv 
    gdf.iloc[:,:-1].to_csv(os.path.join(out_path, file), index=False )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we loop through our CSV files. For each file, we create a geodataframe and change the CRS. Lastly, we replace the coordinates with reprojected one.&lt;/p&gt;
&lt;p&gt;There is another way using pyproj library which does this exact thing, but geopandas is more intuitive for me personally.&lt;/p&gt;
&lt;p&gt;To read about the pyproj method, refer &lt;a href=&#34;https://gis.stackexchange.com/a/168496&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;endnote&#34;&gt;EndNote&lt;/h2&gt;
&lt;p&gt;Although there are several other ways to reproject raw CSV, I found these to be concise and efficient. Thanks to &lt;a href=&#34;https://gis.stackexchange.com/&#34;&gt;https://gis.stackexchange.com/&lt;/a&gt; and awesome GIS community, these methods work like a charm ðŸ˜Š&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overlay cropped raster with vector layer</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</guid>
      <description>&lt;p&gt;I recently faced a problem of having to plot &amp;ldquo;cropped raster&amp;rdquo; layer and a vector layer on the same axes. It is known that we first need to identify the spatial extent of each layer, having the same coordinate reference system.&lt;br&gt;
Rasterio does offer a plotting function &lt;code&gt;show&lt;/code&gt; which can plot a raster layer with the correct spatial extent for you when we pass the dataset reader object.&lt;/p&gt;
&lt;p&gt;When we pass a reader object, the spatial extent is automatically read by &lt;code&gt;show&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:  # import rasterio as rs
    
    f, ax = plt.subplots(figsize=(9,9))
    _ = show(src, ax=ax)            # from rasterio.plot import show
    _ = vector_layer.plot(ax=ax)    # `vector_layer` is a geodataframe (geopandas)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/A33Vopw.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Overlay raster with vector layer. Notice the spatial extent&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Moreover, if we pass a numpy array to the &lt;code&gt;show&lt;/code&gt; function,  the spatial extent of that array has to be explicitly passed using the &lt;code&gt;transform&lt;/code&gt; parameter of the &lt;code&gt;show&lt;/code&gt; function since the numpy array does not know the corner location of the raster and thus the plot would begin with x,y: 0,0 as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1) # img is a numpy array

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if you want to plot a subset of the raster image, in the sense that you would like to slice the image arbitrarily and plot it. When you slice the image, the affine transformation is not the same anymore and thus plotting the sliced image would result in a plot having the spatial extent of the original image while the sliced image being magnified (Fig. 2).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1)[1:-1,1:-1]

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ePTM6q0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 - Overlaid cropped raster and vector layer with incorrect spatial extents&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To avert this problem, we need to find the new affine transformation of the cropped image. Luckily rasterio has a &lt;code&gt;window_transform&lt;/code&gt;  method on the dataset reader which can compute the new transformation from the old one by passing the bounds of the layer. The &lt;code&gt;window_transform&lt;/code&gt; function can either take a 2D N-D array indexer in the form of a tuple &lt;code&gt;((row_start, row_stop), (col_start, col_stop))&lt;/code&gt; or provide offset as written in its &lt;a href=&#34;https://rasterio.readthedocs.io/en/latest/api/rasterio.windows.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;cropped-raster-and-vector-overlay&#34;&gt;Cropped raster and vector overlay&lt;/h2&gt;
&lt;p&gt;The above method returns the new affine transformation, which can be passed to the &lt;code&gt;show&lt;/code&gt; function for the numpy array through the &lt;code&gt;transform&lt;/code&gt; parameter. We also change the read method instead of slicing the array by window parameter to maintain uniformity&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load raster
with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:
    # window =  (((row_start), (row_stop)), ((col_start), (col_stop)))
    img = src.read(1, window = ((1,-1), (1,-1)))
    f, ax = plt.subplots(figsize=(9,9))
    show(img, transform=src.window_transform(((1,-1), (1,-1))), ax=ax)

    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/uwVnq4z.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 - Overlay of cropped raster and vector. Notice the updated spatial extent &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The &lt;code&gt;show&lt;/code&gt; method is helpful for plotting rasters or even RGB images for that matter. One of the differences with matplotlib&amp;rsquo;s plotting is the order of axes. &lt;code&gt;show&lt;/code&gt; expects it the bands to be the last axis while matplotlib, the first. It can also plot 4-band image, which is almost always the for satellite images.
While there is an &lt;code&gt;extent&lt;/code&gt; paramter in matplotlib&amp;rsquo;s plotting function, &lt;code&gt;show&lt;/code&gt; function is much tidier and straight-forward to implement cropped raster and overlay vector layer on it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SMAP Time Series</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/smap-time-series/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/smap-time-series/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Farmers in parts of India still rely on groundwater for irrigation. For them to help understand the present condition of their farm, NASAâ€™s Soil Moisture Active Passive (&lt;strong&gt;SMAP&lt;/strong&gt;) satellite data could fill a significant void.&lt;/p&gt;
&lt;p&gt;The mission collects the kind of local data agricultural and water managers worldwide need.&lt;/p&gt;
&lt;p&gt;The main output of this data set is &lt;strong&gt;surface soil moisture&lt;/strong&gt; (SSM)(representing approximately the top 5 cm of the soil column on average, given in cm3 /cm3 ) presented on the global 36 km EASE-Grid 2.0. While there are other measurements, we are only restricting ourselves to SSM&lt;/p&gt;
&lt;p&gt;The SSM product has three main levels. L1, L2, and the latest being L3. SMAP uses a radiometer to detect microwave signals and process to obtain soil moisture. It initially had radar onboard but failed in 2015. Although the product is primarily available in 36 km resolution, with the help of Sentinel-1 Radar product, we now have access to 9 km resolution daily global product as well post 2016.&lt;/p&gt;
&lt;p&gt;We are going to work with a 36 km product since a time-series can be computationally intensive to download.&lt;/p&gt;
&lt;p&gt;One such product is &lt;strong&gt;L3_SM_P&lt;/strong&gt;, a daily global product, which is an abbreviation of L3 soil moisture 36 km resolution.&lt;/p&gt;
&lt;p&gt;We choose Bengaluru as our area of interest and perform the following three steps in sequence&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Download the SMAP L3 data for the latest one month ( &lt;strong&gt;August 2021&lt;/strong&gt; here).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extraction of the soil moisture values from SMAP L3 data over Lat, Lon of Bengaluru in python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot the time series plot for the extracted soil moisture values for the latest one month.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To download the SMAP L3 data, we head over to &lt;a href=&#34;https://nsidc.org/data/SPL3SMP/versions/7&#34;&gt;https://nsidc.org/data/SPL3SMP/versions/7&lt;/a&gt; and select a time-period ( in our case for the entire month of August 2021) under the download tab. We then click on the download script button as a python file.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/aoKJnay.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Downloading python script for the month of August from NSIDC&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As can be seen in the picture, we have a download size of 980 mb once we run the python script.
The next step would be to download the actual files and extract soil moisture value for the selected lat long.
One thing to note is, since the product has a resolution of 36 km and that the entire pixel represents one value, we have to couple together a set of pixels around Bengaluru since the entire region does not overlay in one pixel size.&lt;/p&gt;
&lt;p&gt;We would be using colaboratory in this entire process since it allows for smooth use of the command line within the notebook itself.&lt;/p&gt;
&lt;h3 id=&#34;run-the-downloaded-script-it-will-ask-you-for-your-earth-data-credentials-username-and-password&#34;&gt;Run the downloaded script. It will ask you for your earth data credentials (username and password)&lt;/h3&gt;
&lt;p&gt;We move the downloaded files to data directory and delete any associated files that comes along with it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Download data: Enter credentials for earth data
%run /content/download_SPL3.py
 
# move files to data dir
!mkdir -p data/L3_SM_P
!mv *h5 data/L3_SM_P
!rm *.h5*
 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, get the lat long for EASE grid 2.0. Since we have to locate Bengaluru (study area) and SMAP uses a specific grid system, we download these files.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lats.964x406x1.double
!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lons.964x406x1.double
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;extract-soil-moisture&#34;&gt;Extract soil moisture&lt;/h3&gt;
&lt;p&gt;We define a python class since there are two half-orbit passes (ascending and descending pass) and we could later combine them easily.&lt;/p&gt;
&lt;p&gt;We create a &lt;code&gt;read_SML3P&lt;/code&gt; method which reads the hdf5 files using the h5py library as an array and removes noisy elements as defined by the user guide. The filename contains the date of acquisition and we extract that.&lt;/p&gt;
&lt;p&gt;We next define the &lt;code&gt;generate_time_series&lt;/code&gt; method to subset the array to our area of interest (Bengaluru) while also taking the mean since there might be more than 1 pixel intersecting the AOI and then return a dataframe with date and the value of Soil Moisture.&lt;/p&gt;
&lt;p&gt;There are some additional method we define to run and initialise the class which can be read from &lt;a href=&#34;https://github.com/amanbagrecha/smap_time_series_analysis/blob/main/main.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SML3PSoilMoist:
  &amp;quot;&amp;quot;&amp;quot;
  get soil moisture from L3 SMAP SCA-V algo for the specified date
  Parameters
  ----------
  soil_moisture: numpy.array
  flag_id:  [str] Quality flag of retrieved soil moisture using SCA-V
  var_id: [str] can be replaced with scva algorithm which is the default (baseline)
  group_id: [str] retrive soil moisture for Ascending or descending pass
  file_list: [list] of downloaded files; File path of a SMAP L3 HDF5 file
  -------
  Returns Soil moisture values and time period as a DataFrame
  &amp;quot;&amp;quot;&amp;quot;
 
  def __init__(self, file_list : &#39;list&#39;, orbit_pass: &#39;str&#39;):
    
    ...
 
  def run_(self):
    &amp;quot;&amp;quot;&amp;quot;read files and return 3d array and time&amp;quot;&amp;quot;&amp;quot;
    ...
 
 
  def read_SML3P(self, filepath):
    &#39;&#39;&#39; This function extracts soil moisture from SMAP L3 P HDF5 file.
    # refer to https://nsidc.org/support/faq/how-do-i-interpret-surface-and-quality-flag-information-level-2-and-3-passive-soil

    &#39;&#39;&#39;    
    with h5py.File(filepath, &#39;r&#39;) as f:
 
        group_id = self.group_id 
        flag_id = self.flag_id
        var_id = self.var_id
 
        flag = f[group_id][flag_id][:,:]
 
        soil_moisture = f[group_id][var_id][:,:]        
        soil_moisture[soil_moisture==-9999.0]=np.nan;
        soil_moisture[(flag&amp;gt;&amp;gt;0)&amp;amp;1==1]=np.nan # set to nan expect for 0 and even bits
 
        filename = os.path.basename(filepath)
        
        yyyymmdd= filename.split(&#39;_&#39;)[4]
        yyyy = int(yyyymmdd[0:4]); mm = int(yyyymmdd[4:6]); dd = int(yyyymmdd[6:8])
        date=dt.datetime(yyyy,mm,dd)
 
    return soil_moisture, date
 
  def generate_time_series(self, bbox: &#39;list -&amp;gt; [N_lat, S_lat, W_lon, E_lon]&#39;):
    
    N_lat, S_lat, W_lon, E_lon = bbox
    subset = (lats&amp;lt;N_lat)&amp;amp;(lats&amp;gt;S_lat)&amp;amp;(lons&amp;gt;W_lon)&amp;amp;(lons&amp;lt;E_lon)
    sm_time = np.empty([self.time_period]);
    
    sm_data_3d, times = self.run_()
    for i in np.arange(0,self.time_period):
        sm_2d = sm_data_3d[:,:,i]
        
        sm_time[i] = np.nanmean(sm_2d[subset]);
 
    return pd.DataFrame({&#39;time&#39; : times, self.orbit_pass: sm_time })
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we plot the dataframe using pandas method &lt;code&gt;plot&lt;/code&gt; and the result is to be shown to the world.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/PngGsda.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This blog helps demonstrates use of SMAP product to generate time series for an entire month of August. You can read more about the specification of the product &lt;a href=&#34;https://nsidc.org/support/faq/how-do-i-interpret-surface-and-quality-flag-information-level-2-and-3-passive-soil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data courtesy: O&amp;rsquo;Neill et al. doi: &lt;a href=&#34;https://doi.org/10.5067/HH4SZ2PXSP6A&#34;&gt;https://doi.org/10.5067/HH4SZ2PXSP6A&lt;/a&gt;. [31st August, 2021].&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Contour Maps in QGIS</title>
      <link>https://amanbagrecha.github.io/post/qgis/contour-maps-in-qgis/</link>
      <pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/qgis/contour-maps-in-qgis/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Most of the time, we are equipped with a discrete set of sample points (of temperature, rainfall etc) and are tasked with generating a continuous surface.
This is where spatial interpolation comes into picture. The objective is to estimate the most probable value at an unknown location with a set of known points within the extent of sample points.&lt;/p&gt;
&lt;p&gt;Methods to perform spatial interpolation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;TIN: Triangular Irregular Network forms contiguous, non-overlapping triangles by dividing the geographic space on set of sample points&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;IDW: Inverse Distance Weighted interpolation method estimates cell values by weighted average of sample data. The closer the point, the more weight assigned. We can fix the radius of influence or the total sample points to weigh for cell value.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spline: Also called french curves. It uses a mathematical function that minimizes overall surface curvature, resulting in a smooth surface that passes through the input points.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kriging: A group of geostatistical techniques to interpolate the value of a random field at an unobserved location from observations of its value at a nearby location. It is implemented using semi-variogram.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this blog, we create surface plots for Rainfall Correction Factors, which is indicative of how much the climate impacts a hydraulic structure based on the return period it is designed for.&lt;/p&gt;
&lt;p&gt;These RCF are useful for hydraulic structures such as dams, storm water drains, and spillways.
These RCF are derived from Global Climate Models (GCMs) which models future scenarios. Not considering these factors can lead to reduced life time of the structure.&lt;/p&gt;
&lt;p&gt;We calculate the RCF for each point for a grid of lat,lon around the indian subcontinent. These RCF are as a result of intensive computational simulations run in matlab which is out of scope for this blog.&lt;/p&gt;
&lt;h3 id=&#34;1-load-points-in-qgis&#34;&gt;1. Load points in QGIS&lt;/h3&gt;
&lt;p&gt;Our data is in the csv format with each column of the RP_ family representing the return period the Rainfall Correction Factor is estimated for.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/fZpYAK9.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -sample data points with key location and return period of RCF&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This file can be imported into qgis from the layers panel and adding a delimited text layer. Once the layer is added, we export as shapefile so as to ease the process of automating the workflow which comes in handy later at the end of the blog.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/MvHcCZx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -Add the csv file using Add delimited text layer&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We load the sampled points and add an India boundary as the base vector to later clip the features to our area of Interest.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/IuoE9pb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -Points equally spaced around the Indian state. Each point represent a RCF value&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;2-generate-raster-from-points-using-tin-interpolation&#34;&gt;2. Generate Raster from points using TIN interpolation&lt;/h3&gt;
&lt;p&gt;For demonstration let us take an example to run through the entire process of generating surface raster and styling which can be later automated using python in qgis.&lt;/p&gt;
&lt;p&gt;We use these sampled locations of points to generate a surface using TIN Interpolation readily available as a toolbox in qgis. The input parameter for the vector layer is our shapefile of points while the interpolation attribute is going to be the RP_ family of columns.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/nzE5Vj7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -TIN interpolation in QGIS&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The output of the interpolation with pixel size of 0.01 is shown below. The extent was set to the boundary of Indian state.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/1HAjqQ4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.5 -Output surface raster with 0.01 pixel size&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can go a step further and derive contours using the &lt;code&gt;contour&lt;/code&gt; toolbox provided in qgis.&lt;/p&gt;
&lt;h3 id=&#34;3-generate-contours-from-raster-to-style-the-layer&#34;&gt;3. Generate Contours from raster to style the layer&lt;/h3&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/SOKxUmC.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.6 -Generate contours from the surface raster&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ExfM0MM.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.7 -Output as contour lines with 0.1 as interval&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A better way to get the contour lines is by changing the symbology of the raster to contours and providing an interval. This exact method will be employed later in this post.&lt;/p&gt;
&lt;h3 id=&#34;automating-the-process&#34;&gt;Automating the process&lt;/h3&gt;
&lt;p&gt;So far we have looked into creating surface raster for an individual return period. But we have several other return periods and we do not want to repeat ourselves. Thus we write a tiny python code to automate this workflow.&lt;/p&gt;
&lt;p&gt;We derive the RCFs for return period of 5year, 10year, 25year, 50year&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# specify the output location for saving the files
OUTPATH = &#39;D:\\gcm_qgis\\&#39;

# loop over different return periods from the shapefile
for i,j in enumerate([&#39;2y&#39;, &#39;10y&#39;, &#39;25y&#39;, &#39;50y&#39;], 3):

    # specify the shapefile containing the RCP values
    MYFILE = &#39;D:\\gcm_qgis\\RCP_avg.shp|layername=RCP_avg::~::0::~::{}::~::0&#39;.format(i)

    # Run interpolation and do not save the output permanently
    RESULTS = processing.run(&amp;quot;qgis:tininterpolation&amp;quot;, 
    {&#39;INTERPOLATION_DATA&#39;: MYFILE,
    &#39;METHOD&#39;:1,
    &#39;EXTENT&#39;:&#39;68.205600900,97.395561000,6.755997100,37.084107000 [EPSG:4326]&#39;,
    &#39;PIXEL_SIZE&#39;:0.01,
    &#39;OUTPUT&#39;:&#39;TEMPORARY_OUTPUT&#39;})

    # clip the temporary output from prev step and save the files.
    processing.runAndLoadResults(&amp;quot;gdal:cliprasterbymasklayer&amp;quot;, 
    {&#39;INPUT&#39;:RESULTS[&#39;OUTPUT&#39;],
    &#39;MASK&#39;:&#39;C:/Users/91911/Downloads/india-osm.geojson.txt|layername=india-osm.geojson&#39;,
    &#39;SOURCE_CRS&#39;:None,&#39;TARGET_CRS&#39;:None,&#39;NODATA&#39;:None,
    &#39;ALPHA_BAND&#39;:False,
    &#39;CROP_TO_CUTLINE&#39;:True,
    &#39;KEEP_RESOLUTION&#39;:False,&#39;SET_RESOLUTION&#39;:False,&#39;X_RESOLUTION&#39;:None,
    &#39;Y_RESOLUTION&#39;:None,
    &#39;MULTITHREADING&#39;:False,&#39;OPTIONS&#39;:&#39;&#39;,
    &#39;DATA_TYPE&#39;:0,
    &#39;EXTRA&#39;:&#39;&#39;,
    &#39;OUTPUT&#39;:os.path.join(OUTPATH, &#39;RCP_avg_&#39; + j + &#39;.tif&#39;)})
    iface.messageBar().pushMessage(
        &#39;Success:&#39;, &#39;Output file written at &#39;, level=Qgis.Success)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our output would save and display the contour files with RCP_avg_{return_period} where return period ranges from [2,5,10,25,50]&lt;/p&gt;
&lt;p&gt;The code first fetches our shapefile, which is used to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;create temporary TIN interpolation rasters&lt;/li&gt;
&lt;li&gt;clipped to india boundary using &lt;code&gt;clip raster by mask layer&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once we have the rasters for each return period, we style the raster using singleband pseudocolor in &lt;code&gt;Equal Interval&lt;/code&gt; mode ranging from 1.0 - 1.8 in steps of 0.1&lt;/p&gt;
&lt;p&gt;We make a copy of the raster layer and place it above it, giving it a contour style at an interval of 0.1&lt;/p&gt;
&lt;p&gt;We copy each return period and set the styling to be of contour as seen in the figure. This allows for a better visual representation of the regions with same the values.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tz0DP0k.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.8 -Styling the copy of surface raster&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The final output can be seen in the below figure.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/fiX9RA9.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.9 -Final output with contours overlaid on top of surface themself&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;final-comments&#34;&gt;Final comments&lt;/h2&gt;
&lt;p&gt;We looked at various spatial interpolation technique and automated workflow to derive spatially interpolated surface raster.&lt;/p&gt;
&lt;p&gt;Sources:&lt;/p&gt;
&lt;p&gt;a. &lt;a href=&#34;https://www.intechopen.com/chapters/52704&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comparison of Spatial Interpolation Techniques Using Visualization and Quantitative Assessment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;b. &lt;a href=&#34;https://docs.qgis.org/3.16/en/docs/gentle_gis_introduction/spatial_analysis_interpolation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatial Analysis QGIS&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Query Geoserver Layer using openlayers</title>
      <link>https://amanbagrecha.github.io/post/geoserver/geoserver-query-builder/</link>
      <pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/geoserver/geoserver-query-builder/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This blog demonstrates how to display and query all geoserver layers or from a workspace using geoserver REST API. CQL (Common Query Language) filter provided by geoserver is used to query the layer.&lt;/p&gt;
&lt;p&gt;We create a full stack application, setting up the backend using django and the frontend using vanilla js. The application will later be deployed on aws ec2 instance.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-the-backend-django&#34;&gt;Setting up the backend (Django)&lt;/h2&gt;
&lt;h3 id=&#34;create-virtual-environment-and-activate-it&#34;&gt;Create virtual environment and activate it&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create --name djangoEnv
conda activate djangoEnv
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;start-a-new-project-and-create-app&#34;&gt;Start a new project and create app&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;django-admin startproject DOGP
python manage.py startapp gisapp
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setup-the-database&#34;&gt;Setup the database&lt;/h3&gt;
&lt;p&gt;We set up postgresql for this exercise. Create a new database and add a postgis extension from it. For more info on how to set up the extension, click here.&lt;/p&gt;
&lt;p&gt;Once the database is set up on the localhost server, we make changes to the settings.py module in our application.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# change database

DATABASES = {
	&#39;default&#39;: {
		 &#39;ENGINE&#39;: &#39;django.contrib.gis.db.backends.postgis&#39;,
		 &#39;NAME&#39;: &#39;DOGP&#39;, # our new database name
		 &#39;USER&#39;: &#39;postgres&#39;,
		&#39;PASSWORD&#39;: &#39;1234&#39;,
		&#39;HOST&#39;: &#39;127.0.0.1&#39;,
		&#39;PORT&#39;: &#39;5432&#39;,
	},
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;add-installed-apps&#34;&gt;Add installed apps&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
INSTALLED_APPS = [
    &#39;gisapp.apps.GisappConfig&#39;,
    &#39;django.contrib.gis&#39;,
]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are other setups such as setting up login page and authentication, creating media url root and setting up the url which we are not going to deal with in this blog post.&lt;/p&gt;
&lt;p&gt;Once the setup is done, we run migrations to reflect those changes in the admin page.&lt;/p&gt;
&lt;p&gt;On running &lt;code&gt;python manage.py runserver&lt;/code&gt; you should see this page.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/zSSpoqV.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Page indicating successful installation of Django&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;p&gt;Our focus will be on the frontend, but the full code can be accessed from &lt;a href=&#34;https://github.com/amanbagrecha/openlayers-geoserver-query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For querying and displaying layers from geoserver, we first need geoserver installed and running. For more info on how to do that can be found &lt;a href=&#34;https://docs.geoserver.org/master/en/user/installation/win_binary.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the following steps we setup our basemap layer to be ESRI World Imagery and define an empty vector layer to store the result of query.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// map setup
var maplayer = 	new ol.layer.Tile({
    source: new ol.source.XYZ({
	  attributions: [&#39;Powered by Esri&#39;,&#39;Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community&#39;],
	  attributionsCollapsible: false,
	  url: &#39;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}&#39;,
	  maxZoom: 23
	}),
	zIndex: 0
  })

var view = new ol.View({
	projection: &#39;EPSG:4326&#39;,
	center: [-103.32989447589996, 44.18118547387081],
	zoom: 7,
  });
  
var map = new ol.Map({
	layers: [ maplayer],
	target: &#39;map&#39;,
	view: view,
  });

// define empty vector layer to store query result later
var SearchvectorLayerSource =  new ol.source.Vector({
	  
	})
var SearchvectorLayer = new ol.layer.Vector({
	source:SearchvectorLayerSource
  });
  map.addLayer(SearchvectorLayer);

// define headers for authentication and login
MyHeaders = {&#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Access-Control-Allow-Credentials&#39; : true,
				&#39;Access-Control-Allow-Origin&#39;:&#39;*&#39;,
				&#39;Accept&#39;: &#39;application/json&#39;,
				&#39;Authorization&#39;: &#39;Basic &#39; + btoa(&#39;admin:geoserver&#39;)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To access all layers from a particular workspace, the api end point to do that is as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// https://docs.geoserver.org/latest/en/api/#1.0.0/layers.yaml
/workspaces/{workspaceName}/layers
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see this in action, we display all layers from the &lt;code&gt;sf&lt;/code&gt; workspace, provided in geoserver by default.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/sSjlZkU.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -Geoserver layers from sf workspace&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
var layerList = []; // array to store all the layer info
var loginInfo = [&amp;quot;admin&amp;quot;, &amp;quot;geoserver&amp;quot;]; // username and password for geoserver
var geoserverURL = geoserver_ip + &amp;quot;:&amp;quot; + geoserver_port  

// make ajax call to access the sf layer
$.ajax({
    url: geoserverURL + &#39;/geoserver/rest/workspaces/sf/layers/&#39;,
    type: &#39;GET&#39;,
    dataType: &#39;json&#39;,
    contentType: &amp;quot;application/json&amp;quot;,
    beforeSend: function(xhr) {
         xhr.setRequestHeader (&amp;quot;Authorization&amp;quot;, &amp;quot;Basic &amp;quot; + btoa(loginInfo[0] + &amp;quot;:&amp;quot; + loginInfo[1]));
    },
    success: function(data){
        for (var i = 0; i &amp;lt; data.layers.layer.length; i++) {
            layerList.push([data.layers.layer[i].name, data.layers.layer[i].href]);
        }

    },
    async: false
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of this ajax call returns us a &lt;code&gt;layerList&lt;/code&gt; array containing all the layer name and the url associated with it of size (:, 2)&lt;/p&gt;
&lt;p&gt;This layer can then be displayed on the frontend by looping over the array and inserting into the div element.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ZDctdje.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -The layers of workspace `sf` displayed on the map with some styles applied to it
&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;p&gt;The next step after displaying all the layers of the workspace is to load the features of the layer on selecting a particular layer.&lt;/p&gt;
&lt;p&gt;When the layer is ticked we send a request to geoserver to load the features of that layer and add to the map. If the layer is then unticked, we do the opposite and remove the layer from map.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;  function toggleLayer(input) {
	  if (input.checked) {
		  wmsLayer = new ol.layer.Image({
			source: new ol.source.ImageWMS({
			  url: geoserver_ip+ &#39;:&#39;+geoserver_port + &amp;quot;/geoserver/wms&amp;quot;,
			  imageLoadFunction: tileLoader,
			  params: { LAYERS: input.value },
			  serverType: &amp;quot;geoserver&amp;quot;,
			}),
			name: input.value,
		  });

		map.addLayer(wmsLayer);
					
	  } else {
		  map.getLayers().forEach(layer =&amp;gt; {
			  if (layer.get(&#39;name&#39;) == input.value) {
				 map.removeLayer(layer);
			 }
		 });
	  }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Wgy2YV5.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -Displaying layer on map&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;query-layer&#34;&gt;Query layer&lt;/h3&gt;
&lt;p&gt;We start with the querying the layer by their attributes. We load all the attributes (as columns) and display as dropdown. We use &lt;code&gt;wfs&lt;/code&gt; service and &lt;code&gt;DescribeFeatureType&lt;/code&gt; request to load the attributes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;  function loadprops(layername) {
	  selectedLayer = layername;
	  fetch(
		geoserver_ip+ &#39;:&#39;+geoserver_port+&amp;quot;/geoserver/wfs?service=wfs&amp;amp;version=2.0.0&amp;amp;request=DescribeFeatureType&amp;amp;typeNames=&amp;quot; + 
		  layername +
		  &amp;quot;&amp;amp;outputFormat=application/json&amp;quot;,
		{
		  method: &amp;quot;GET&amp;quot;,
		  headers: MyHeaders,
		}
	  )
		.then(function (response) {
		  return response.json();
		})
		.then(function (json) {
			var allprops = json.featureTypes[0].properties;
		  var ColumnnamesSelect = document.getElementById(&amp;quot;Columnnames&amp;quot;);
			  ColumnnamesSelect.innerHTML = &#39;&#39;
			for (i = 0; i &amp;lt; allprops.length; i++){
				if (allprops[i].name != &#39;the_geom&#39;) {
					ColumnnamesSelect.innerHTML +=
					  &#39;&amp;lt;option value=&amp;quot;&#39; +
					  allprops[i].name +
					  &#39;&amp;quot;&amp;gt; &#39; +
					  allprops[i].name +
					  &amp;quot;&amp;lt;/option&amp;gt;&amp;quot;;
				}
  
			}
		});
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upto this point we have the layer and its features we want to search for. To query the layer we make a fetch call to ows service protocol and pass in the values of feature and the layer we want to query for.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;CQL_filter = column_name + &amp;quot; = &#39;&amp;quot; + query_value + &amp;quot;&#39;&amp;quot;;
  query_url =geoserver_ip+ &#39;:&#39;+geoserver_port + &amp;quot;/geoserver/sf/ows?service=WFS&amp;amp;version=1.0.0&amp;amp;request=GetFeature&amp;amp;typeName=&amp;quot; + selectedLayer +	&amp;quot;&amp;amp;CQL_FILTER=&amp;quot; +	CQL_filter +  &amp;quot;&amp;amp;outputFormat=application%2Fjson&amp;quot;;
    		
  fetch_search_call(query_url).catch((error) =&amp;gt; {
  CQL_filter = column_name + &amp;quot;%20&amp;quot; + &amp;quot;ILIKE&amp;quot; + &amp;quot;%20%27%25&amp;quot; + query_value + &amp;quot;%25%27&amp;quot;;
	});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We define a fetch_search_call function which makes a request to ows service and returns a geojson. We can parse the geojson and display it on the map.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;  
function fetch_search_call(query_url){

	fetch_result = fetch(query_url, {
		method: &amp;quot;GET&amp;quot;,
		headers: MyHeaders,
	  })
		.then(function (response) {
		  return response.json();
		})
		.then(function (json) {
		
				SearchvectorLayerSource.clear()
				SearchvectorLayerSource.addFeatures(
			  new ol.format.GeoJSON({
			  }).readFeatures(json)
			  );
			  if(json.features.length!=0){
			  $(&#39;#searchModal&#39;).modal(&#39;toggle&#39;);
			  }

			SearchvectorLayer.set(&#39;name&#39;,&#39;search_polygon_layer&#39;)
			map.getView().fit(SearchvectorLayerSource.getExtent(),  { duration: 1590, size: map.getSize(), padding: [10, 10, 13, 15], maxZoom:16});
			
		return fetch_result
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function queries a feature and adds it to the map as a new layer. If the search is successful, we are zoomed into that location and only the feature queried gets displayed.
If the fetch call could not find the match it returns an error which is caught by &lt;code&gt;catch&lt;/code&gt; and displays the error to the client.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/syqvzq0.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.5 -Displaying Queried layer by attribute value&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This completes the blog on how to query layer and display on the map. Visit the &lt;a href=&#34;https://github.com/amanbagrecha/openlayers-geoserver-query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github page&lt;/a&gt; to find the working application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Download and preprocess NASA GPM IMERG Data using Python and wget</title>
      <link>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</guid>
      <description>&lt;p&gt;In this blog post we look into how to download precipitation data from NASA website and process it to get information using xarray and wget.&lt;/p&gt;
&lt;p&gt;We are going to work with &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPM IMERG Late Precipitation L3 Half Hourly 0.1 degree x 0.1 degree V06 (GPM_3IMERGHHL)&lt;/a&gt; data provided by NASA which gives half-hourly precipitation values for entire globe.&lt;/p&gt;
&lt;h2 id=&#34;pre-requisites&#34;&gt;Pre-requisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;You must have an Earthdata Account&lt;/li&gt;
&lt;li&gt;Link GES DISC with your account&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refer to &lt;a href=&#34;https://daac.gsfc.nasa.gov/earthdata-login&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; page on how to Link GES DISC to your account.&lt;/p&gt;
&lt;p&gt;First method- We would be downloading netCDF data using the &lt;code&gt;request&lt;/code&gt; module and preprocessing the file using &lt;code&gt;xarray&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Second method- To download netCDF file using wget and using &lt;code&gt;xarray&lt;/code&gt; to preprocess and visualise the data.&lt;/p&gt;
&lt;h2 id=&#34;downloading-link-list&#34;&gt;Downloading link list&lt;/h2&gt;
&lt;p&gt;Visit the GPM IMERG website and click on &lt;strong&gt;subset/ Get Data&lt;/strong&gt; link at right corner. 
&lt;img src=&#34;https://i.imgur.com/3RH1ot2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the popup, select&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Download Method&lt;/em&gt; as &lt;em&gt;&lt;strong&gt;Get File Subsets using OPeNDAP&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Refine Date Range&lt;/em&gt; as the date you want the data for. In my case, I choose 10 days of data.&lt;/li&gt;
&lt;li&gt;Refine Region to subset data for your area of interest. In my case I choose &lt;code&gt;77.45,12.85,77.75,13.10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;em&gt;Variables&lt;/em&gt;, select &lt;em&gt;&lt;strong&gt;precipitationCal&lt;/strong&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;For file format, we choose &lt;em&gt;&lt;strong&gt;xarray&lt;/strong&gt;&lt;/em&gt; and click the Get Data button.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xCp8Shs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This will download a text file, containing all the links to download individual netCDF half hourly data.&lt;/p&gt;
&lt;p&gt;Now we move to google Colab, to download the netCDF files (what we actually require)&lt;/p&gt;
&lt;h2 id=&#34;method-1-using-python-to-read-and-preprocess-the-data-inside-google-colab&#34;&gt;Method 1: Using Python to read and preprocess the data inside google Colab.&lt;/h2&gt;
&lt;p&gt;Open a new google Colab notebook and upload the downloaded text file. Our uploaded text file looks like the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/njlFhPT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As one last requirement, NASA requires authentication to access the data and thus we have to create a &lt;code&gt;.netrc&lt;/code&gt; file and save it at specified location (under &lt;code&gt;/root&lt;/code&gt; dir in our case).&lt;/p&gt;
&lt;h2 id=&#34;creating-netrc-file&#34;&gt;Creating &lt;code&gt;.netrc&lt;/code&gt; file&lt;/h2&gt;
&lt;p&gt;Open your notepad and type in the following text. Make sure to replace &lt;code&gt;your_login_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; with your earthdata credentials. Now save it as &lt;code&gt;.netrc&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine urs.earthdata.nasa.gov login your_login_username password your_password
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upload the &lt;code&gt;.netrc&lt;/code&gt; file to Colab under &lt;code&gt;root&lt;/code&gt; directory as shown in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/oZLeJuY.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we have all the setup done and are ready to code.&lt;/p&gt;
&lt;p&gt;We first load the required libraries. Then, read the text file and loop over every line in it to download from the URL using the &lt;code&gt;request&lt;/code&gt; module. Finally, we save the file to Colab&amp;rsquo;s hard drive. If you do not see the files after running code, make sure to wait for at least a day after registering to earthdata to make your account activated. I was late to read about it and had wasted a long time debugging it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
import xarray as xr
import requests 

# dataframe to read the text file which contains all the download links
ds = pd.read_csv(&#39;/content/subset_GPM_3IMERGHH_06_20210611_142330.txt&#39;, header = None, sep = &#39;\n&#39;)[0]

# Do not forget to add .netrc file in the root dir of Colab. printing `result` should return status code 200
for file in range(2, len(ds)): # skip first 2 rows as they contain metadata files
  URL = ds[file]
  result = requests.get(URL)
  try:
    result.raise_for_status()
    filename = &#39;test&#39; + str(file) + &#39;.nc&#39;
    with open(filename, &#39;wb&#39;) as f:
        f.write(result.content)

  except:
    print(&#39;requests.get() returned an error code &#39;+str(result.status_code))

xr_df = xr.open_mfdataset(&#39;test*.nc&#39;)

xr_df.mean(dim = [&#39;lat&#39;, &#39;lon&#39;]).to_dataframe().to_csv(&#39;results.csv&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above snippet, what is interesting is the method &lt;code&gt;open_mfdataset&lt;/code&gt; which takes in all the netCDF files and gives us a nice, compact output from which we can subset and further process our data.
Here, we take the average of all the values (precipitation) and convert it into a new dataframe. We are ready to export it as CSV.&lt;/p&gt;
&lt;h2 id=&#34;method-2-using-wget-to-download-and-then-preprocess-using-xarray&#34;&gt;Method 2: Using wget to download and then preprocess using xarray&lt;/h2&gt;
&lt;p&gt;In this method, we download all the netCDF files using &lt;code&gt;wget&lt;/code&gt;. These files are then read using xarray which makes it really easy to process and get the information we require.&lt;/p&gt;
&lt;p&gt;Running the following shell command in Google Colab will download all the data from the text file URLs. Make sure to replace &lt;code&gt;your_user_name&lt;/code&gt; , &lt;code&gt;&amp;lt;url text file&amp;gt;&lt;/code&gt; within the command. It will ask for password of your earthdata account on running the cell.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;! wget --load-cookies /.urs_cookies --save-cookies /root/.urs_cookies --auth-no-challenge=on --user=your_user_name --ask-password --content-disposition -i &amp;lt;url text file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the above shell command is run on Colab, the following 2 lines of code will give a nice dataframe which can be exported to csv for further analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xarray as xr
import glob

ds = xr.open_mfdataset(&#39;test*.nc&#39;)
ds.precipitationCal.mean(dim=(&#39;lon&#39;, &#39;lat&#39;)).plot() # calculate the average precipitation on a half-hourly basis.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;final-comments&#34;&gt;Final Comments&lt;/h2&gt;
&lt;p&gt;In this post we looked into how to download and preprocess netCDF data provided by &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA GES DISC&lt;/a&gt;.
We looked at two methods, one with pure Python and the other with wget and xarray. All performed on google Colab. 
It is to be noted that, there is a significant setup required i.e, to create a new &lt;code&gt;.netrc&lt;/code&gt; file and store inside the root directory of Colab else it returns an authorisation error. We looked at how easy it is to process netCDF data in xarray and how wget commands can be run on Colab.&lt;/p&gt;
&lt;p&gt;Watch the video tutorial &lt;a href=&#34;https://www.youtube.com/watch?v=T_Us4hJxSeI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The notebook for reference is located &lt;a href=&#34;https://Colab.research.google.com/drive/1VIKun8K3RT8VvcPJ7DE5uDDC10i10k1T?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Validating LULC classes in QGIS</title>
      <link>https://amanbagrecha.github.io/post/qgis/validating-lulc-classes-in-qgis/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/qgis/validating-lulc-classes-in-qgis/</guid>
      <description>&lt;h2 id=&#34;the-problem-statement&#34;&gt;&lt;strong&gt;The problem statement&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Any land-use land cover classification needs to be validated with ground-truth data to measure the accuracy. A key single-valued statistic to determine the effectiveness of classification is Cohenâ€™s kappa. This validation metric has been fairly widely used for unbalanced classification as well which expresses a level of agreement between two annotators on a classification problem.&lt;/p&gt;
&lt;p&gt;The objective of this quality assessment was to validate the land cover map performed on June, 2020 sentinel-2 imagery by k-means classification algorithm, thus providing a statistical measure of overall class predictions. The validation was done using an independent set of sample points (~500) generated randomly following stratified random sampling design, to capture the variance within the class&lt;/p&gt;
&lt;p&gt;After running the tool, the sample points were manually assigned to the ground-truth class. The ground-truth dataset was taken to be Bing-satellite imagery as a proxy for field data. Each sample point was labelled by visual inspection on the ground-truth dataset.&lt;/p&gt;
&lt;h2 id=&#34;step-1-classify-image&#34;&gt;&lt;strong&gt;Step 1: Classify Image&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Load raster Image&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;K-means clustering for grids&lt;/code&gt; under SAGA tools. Select the raster Image as &lt;code&gt;grid&lt;/code&gt; and in this case we specify 4 classes&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xdx5Tsn.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -K-means clustering on sentinel-2 Image&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;code&gt;Run&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;At this stage we have unsupervised k-means clustering output ready &lt;code&gt;(Fig.2)&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/eW0cOXE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -classification of RR Nagar, Bengaluru. Classes- Forest, Urban, water, Bareland&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-2-convert-to-polygon-vector-format&#34;&gt;&lt;strong&gt;Step 2: Convert to polygon (vector format)&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;Polygonize (Raster to Vector)&lt;/code&gt; tool under &lt;code&gt;GDAL&lt;/code&gt;-&amp;gt;&lt;code&gt;Raster Conversion&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select the classified image as input. Leave everything else as default. The output would be a &lt;code&gt;Vectorised&lt;/code&gt; scratch layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/36nk2tF.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -Convert Raster to vector&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Note the name of the field (&lt;code&gt;DN&lt;/code&gt; here). This will be used later.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Fix geometries (this step is important here to avoid any error in further steps) &lt;code&gt;Vector Geometry&lt;/code&gt;-&amp;gt;&lt;code&gt;Fix Geometry&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/gG9gBIc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -Fixing topology issues with &lt;code&gt;Fix Geometry&lt;/code&gt; Toolbox&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-3-dissolve-the-layer-on-dn-field&#34;&gt;&lt;strong&gt;Step 3: Dissolve the layer on DN field&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this step we dissolve the layer based on the &lt;code&gt;DN&lt;/code&gt; value. This will ensure that each polygon can be evaluated based on the land class type which is needed for stratified random sampling.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/gm1ihfT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.5 -&lt;code&gt;Dissolve&lt;/code&gt; toolbox to dissolve polygon on &lt;code&gt; DN &lt;/code&gt; value&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Make sure to select dissolve field as &lt;code&gt;DN&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-4-create-stratified-random-samples&#34;&gt;&lt;strong&gt;Step 4: Create stratified random samples&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Go to &lt;code&gt;Vector-&amp;gt;research tools-&amp;gt; Random Points inside Polygon&lt;/code&gt; and set &lt;code&gt;Sampling Strategy&lt;/code&gt; = &lt;code&gt;Points Density&lt;/code&gt; and &lt;code&gt;Point count or density&lt;/code&gt; = &lt;code&gt;0.001&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note: The value &lt;code&gt;0.001&lt;/code&gt; signify &lt;code&gt;1&lt;/code&gt; point for &lt;code&gt;1/0.001&lt;/code&gt; m2 of area, given that the units is meters.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/1LB6R5L.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.6 - One sample point is generated for each 1000 m2 of area&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-5-extract-raster-values-to-sample-layer&#34;&gt;&lt;strong&gt;Step 5: Extract raster values to sample layer&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We extract the raster value, which is essentially the land cover class for the classified image. We use &lt;code&gt;Sample Raster Values&lt;/code&gt; function here (&lt;code&gt;Fig.7&lt;/code&gt;). The input layer is the random points we generated earlier and the the raster layer is the classified image. The output adds a new column to the sample points layer with the prediction class of the land-cover (&lt;code&gt;Fig.8&lt;/code&gt;).&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/s2RXNOZ.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.7 -Running &lt;code&gt;Sample Raster Value&lt;/code&gt; to extract Raster values for the input points&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/GG9wgNK.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.8 -The corresponding Attribute Table with Predicted Class &lt;code&gt; PREDICTED_1&lt;/code&gt; for each feature&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-6-ground-truth-labelling-using-bing-maps&#34;&gt;&lt;strong&gt;Step 6: Ground Truth Labelling using Bing maps&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At this stage we are ready to validate the image using Bing maps as ground truth. We turn on the edit mode and create new field named Actual class. THen we visually inspect the class on the map and note the land-cover class. Once we inspect all the sample points we can use cohens Kappa statistics to determine the validation result. Alternatively, simply calculating the accuracy would also suffice the need.&lt;/p&gt;
&lt;h2 id=&#34;step-7-add-other-field-to-the-attribute-table-with-reclassification&#34;&gt;&lt;strong&gt;Step 7: Add other field to the attribute table with reclassification&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We can use the &lt;code&gt;Field Calculator&lt;/code&gt; to generate verbose text for each label in our feature class and display labels for the prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- in field calculator to increase verbosity
CASE WHEN PREDICTED_1 is 2 THEN &#39;Urban&#39; 
WHEN PREDICTED_1 is 1 THEN &#39;Bareland&#39;
WHEN PREDICTED_1 is 4 THEN &#39;Forest&#39;
WHEN PREDICTED_1 is 3 THEN &#39;Urban&#39;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/3CBAK6X.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.9 -Predicted classes (foreground) vs ground truth (background)&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;With this we come to end of the post. Now, validation accuracy can be reported for k-means classification.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geocoding using Mapbox API with Zoom-in map functionality</title>
      <link>https://amanbagrecha.github.io/post/openlayers/geocode-using-mapbox-api-with-zoom-functionality/</link>
      <pubDate>Mon, 24 May 2021 20:04:53 +0530</pubDate>
      <guid>https://amanbagrecha.github.io/post/openlayers/geocode-using-mapbox-api-with-zoom-functionality/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The big picture of this post can be related to google maps, wherein you type the address and it zooms in to the location of interest. We replicate this exact functionality with mapbox API for geocoding and openlayers for client side zoom to the address of interest.&lt;/p&gt;
&lt;h2 id=&#34;main-steps&#34;&gt;Main steps&lt;/h2&gt;
&lt;p&gt;This blog demonstrates how to geocode an address using mapbox api implemented in openlayers v6. Additionally zoom in to the search location as text provided on the search bar.
This one page appication demostrates only key elements, rest of the customisation is at discretion of the viewer.&lt;/p&gt;
&lt;h3 id=&#34;setup-the-project&#34;&gt;Setup the project&lt;/h3&gt;
&lt;p&gt;We first create basic single html file to include all elements (javascript, css and html). Ideally, when the application scales, you would create a seperate file for each component.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create &lt;code&gt;html&lt;/code&gt; file and add basic elements&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;
 &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/css/ol.css&amp;quot; type=&amp;quot;text/css&amp;quot;&amp;gt;
 &amp;lt;style type=&amp;quot;text/css&amp;quot;&amp;gt;
.autocomplete {
  position: relative;
  display: inline-block;
}
input {
  border: 1px solid transparent;
  background-color: #f1f1f1;
  padding: 10px;
  font-size: 16px;
}
input[type=text] {
  background-color: #f1f1f1;
  width: 100%;
}
input[type=submit] {
  background-color: DodgerBlue;
  color: #fff;
  cursor: pointer;
}
 &amp;lt;/style&amp;gt;
 &amp;lt;/head&amp;gt;
 &amp;lt;body&amp;gt;
&amp;lt;!--create search bar for geocoding and style it --&amp;gt;
&amp;lt;h2&amp;gt;Autocomplete&amp;lt;/h2&amp;gt;
&amp;lt;br&amp;gt;
&amp;lt;form  method=&amp;quot;post&amp;quot; &amp;gt;
  &amp;lt;div class=&amp;quot;autocomplete&amp;quot; style=&amp;quot;width:300px;&amp;quot;&amp;gt;
	&amp;lt;input id=&amp;quot;myInput&amp;quot; type=&amp;quot;text&amp;quot; name=&amp;quot;myCountry&amp;quot; placeholder=&amp;quot;Country&amp;quot;&amp;gt;
  &amp;lt;/div&amp;gt;
  &amp;lt;input type=&amp;quot;submit&amp;quot; id = &amp;quot;geocodingSubmit&amp;quot;&amp;gt;
&amp;lt;/form&amp;gt;
&amp;lt;div id=&#39;project_map&#39;, class=&amp;quot;map&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;script src=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/build/ol.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;!-- &amp;lt;script src=&amp;quot;https://cdnjs.cloudflare.com/ajax/libs/proj4js/2.5.0/proj4.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; --&amp;gt;
&amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
	 
	 // create basemap layer
	 var project_maplayer = new ol.layer.Tile({
	// source: new ol.source.OSM(),
	source: new ol.source.XYZ({
		attributions: [&#39;Powered by Esri&#39;,
									 &#39;Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community&#39;],
		attributionsCollapsible: false,
		url: &#39;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}&#39;,
		maxZoom: 23
	}),
	zIndex: 0
});

// create view for the layer
var project_view = new ol.View({
	projection: &#39;EPSG:4326&#39;,
	center: [-81.80808208706726, 27.285095000261222],
	zoom: 7,
});

// add the basemap to the map
var Projectmap = new ol.Map({
	layers: [project_maplayer,],
	target: &#39;project_map&#39;,
	view: project_view,
    constrainOnlyCenter: true,
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We added the following elements,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Search bar: we setup the search function to input values as address and wrap it within a form with post request.&lt;/li&gt;
&lt;li&gt;Map : the div element with &lt;code&gt;id=&amp;quot;project_map&amp;quot;&lt;/code&gt; holds the map element and the script does the following. First, create layer with ESRI basemap. Second, add the layer to the Map object.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this stage the application looks like the following image&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/joNvjw7.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;add-autocomplete-functionality&#34;&gt;Add autocomplete functionality&lt;/h2&gt;
&lt;p&gt;We fetch from the api and populate our top results in a list format on key press. Also, we style the search bar using css.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;style&amp;gt;
.autocomplete-items {
  position: absolute;
  border: 1px solid #d4d4d4;
  border-bottom: none;
  border-top: none;
  z-index: 99;
  /*position the autocomplete items to be the same width as the container:*/
  top: 100%;
  left: 0;
  right: 0;
}

.autocomplete-items div {
  padding: 10px;
  cursor: pointer;
  background-color: #fff; 
  border-bottom: 1px solid #d4d4d4; 
}

/*when hovering an item:*/
.autocomplete-items div:hover {
  background-color: #e9e9e9; 
}

/*when navigating through the items using the arrow keys:*/
.autocomplete-active {
  background-color: DodgerBlue !important; 
  color: #ffffff; 
}
&amp;lt;/style&amp;gt;

&amp;lt;script&amp;gt;
myHeaders =  {&#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Access-Control-Allow-Credentials&#39; : true,
					&#39;Access-Control-Allow-Origin&#39;:&#39;*&#39;,
					&#39;Accept&#39;: &#39;application/json&#39;}

function autocomplete(inp) {
  /*the autocomplete function takes one argument,
  the text field element*/
  var currentFocus;
  /*execute a function when someone writes in the text field:*/
  inp.addEventListener(&amp;quot;input&amp;quot;, function(e) {
	  var a, b, i, val = this.value;
	  var ACCESS_TOKEN_KEY = &#39;your_token_here&#39;
	  /*close any already open lists of autocompleted values*/
	  var URL = `https://api.mapbox.com/geocoding/v5/mapbox.places/${val}.json?access_token=${ACCESS_TOKEN_KEY}&amp;amp;types=address,region,poi,country,district,locality,neighborhood,postcode&amp;amp;country=us`
	 
	  fetch(URL,{
		method: &#39;GET&#39;,
		headers: myHeaders,
	  }).then(response =&amp;gt; response.json())
	  .then(data =&amp;gt; {
		geocode_data = data;
		// console.log(data) 
	  
	  closeAllLists();
	  if (!val) { return false;}
	  currentFocus = -1;
	  /*create a DIV element that will contain the items (values):*/
	  a = document.createElement(&amp;quot;DIV&amp;quot;);
	  a.setAttribute(&amp;quot;id&amp;quot;, this.id + &amp;quot;autocomplete-list&amp;quot;);
	  a.setAttribute(&amp;quot;class&amp;quot;, &amp;quot;autocomplete-items&amp;quot;);
	  /*append the DIV element as a child of the autocomplete container:*/
	  this.parentNode.appendChild(a);
	  /*for each item in the array...*/
	  for (i = 0; i &amp;lt; geocode_data.features.length; i++) {

		  b = document.createElement(&amp;quot;DIV&amp;quot;);
		  /*insert a input field that will hold the current array item&#39;s value:*/
		  b.innerHTML += geocode_data.features[i].place_name;
		  b.innerHTML += `&amp;lt;input type=&#39;hidden&#39; style=&amp;quot;display: none;&amp;quot; id=${i}-center-cc  
		  coordinates=&#39;${geocode_data.features[i].center}&#39; value=&#39;${geocode_data.features[i].place_name}&#39;&amp;gt;`;
		  
		  /*execute a function when someone clicks on the item value (DIV element):*/
		  b.addEventListener(&amp;quot;click&amp;quot;, function(e) {
			  /*insert the value for the autocomplete text field:*/
			  var input_tag = this.getElementsByTagName(&amp;quot;input&amp;quot;)[0]
			  inp.value = input_tag.value;
			  inp.setAttribute(&amp;quot;coordinates&amp;quot;, input_tag.getAttribute(&#39;coordinates&#39;));

			  /*close the list of autocompleted values,
			  (or any other open lists of autocompleted values:*/
			  closeAllLists();
		  });
		  a.appendChild(b);
		}

	  })
	  .catch(error =&amp;gt; {
	console.error(&#39;There has been a problem with your fetch operation:&#39;, error);
	});


	  });
  // });
  /*execute a function presses a key on the keyboard:*/
  inp.addEventListener(&amp;quot;keydown&amp;quot;, function(e) {
	  var x = document.getElementById(this.id + &amp;quot;autocomplete-list&amp;quot;);
	  if (x) x = x.getElementsByTagName(&amp;quot;div&amp;quot;);
	  if (e.keyCode == 40) {
		/*If the arrow DOWN key is pressed,
		increase the currentFocus variable:*/
		currentFocus++;
		/*and and make the current item more visible:*/
		addActive(x);
	  } else if (e.keyCode == 38) { //up
		/*If the arrow UP key is pressed,
		decrease the currentFocus variable:*/
		currentFocus--;
		/*and and make the current item more visible:*/
		addActive(x);
	  } else if (e.keyCode == 13) {
		/*If the ENTER key is pressed, prevent the form from being submitted,*/
		e.preventDefault();
		if (currentFocus &amp;gt; -1) {
		  /*and simulate a click on the &amp;quot;active&amp;quot; item:*/
		  if (x) x[currentFocus].click();
		}
	  }
  });
  function addActive(x) {
	/*a function to classify an item as &amp;quot;active&amp;quot;:*/
	if (!x) return false;
	/*start by removing the &amp;quot;active&amp;quot; class on all items:*/
	removeActive(x);
	if (currentFocus &amp;gt;= x.length) currentFocus = 0;
	if (currentFocus &amp;lt; 0) currentFocus = (x.length - 1);
	/*add class &amp;quot;autocomplete-active&amp;quot;:*/
	x[currentFocus].classList.add(&amp;quot;autocomplete-active&amp;quot;);
  }
  function removeActive(x) {
	/*a function to remove the &amp;quot;active&amp;quot; class from all autocomplete items:*/
	for (var i = 0; i &amp;lt; x.length; i++) {
	  x[i].classList.remove(&amp;quot;autocomplete-active&amp;quot;);
	}
  }
  function closeAllLists(elmnt) {
	/*close all autocomplete lists in the document,
	except the one passed as an argument:*/
	var x = document.getElementsByClassName(&amp;quot;autocomplete-items&amp;quot;);
	for (var i = 0; i &amp;lt; x.length; i++) {
	  if (elmnt != x[i] &amp;amp;&amp;amp; elmnt != inp) {
		x[i].parentNode.removeChild(x[i]);
	  }
	}
  }
  /*execute a function when someone clicks in the document:*/
  document.addEventListener(&amp;quot;click&amp;quot;, function (e) {
	  closeAllLists(e.target);
  });
}

/*initiate the autocomplete function on the &amp;quot;myInput&amp;quot; element */
autocomplete(document.getElementById(&amp;quot;myInput&amp;quot;));

&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following is the explanation of the code&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;autocomplete function: The function takes an element as input which needs to be populated. Then we add an event listner which on change in input field, triggers. A GET request is sent across for the input typed and the result is populated in a form of dropdown. We add some styling on key-down so as to select the search.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, with correct mapbox api access key, we have built the autocomplete functionality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ES8bnSO.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;last-steps&#34;&gt;Last steps&lt;/h2&gt;
&lt;p&gt;We now only need to implement the submit functionality. On click of submit button, the address is located on the map and zoomed in. This is done using a function we call centerMap&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;function CenterMap() {
	var [long, lat] = document.getElementById(&amp;quot;myInput&amp;quot;).getAttribute(&amp;quot;coordinates&amp;quot;).split(&amp;quot;,&amp;quot;).map(Number)
    console.log(&amp;quot;Long: &amp;quot; + long + &amp;quot; Lat: &amp;quot; + lat);
    Projectmap.getView().setCenter(ol.proj.transform([long, lat], &#39;EPSG:4326&#39;, &#39;EPSG:4326&#39;));
    Projectmap.getView().setZoom(5);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we add the centerMap function on click of submit&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;document.getElementById(&amp;quot;geocodingSubmit&amp;quot;).addEventListener(&#39;click&#39;, function(e){

	e.preventDefault();
	CenterMap()
})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The associated running application can be found &lt;a href=&#34;https://amanbagrecha.github.io/mapbox-search-functionality/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Upload Multiple Geotagged Images in Django</title>
      <link>https://amanbagrecha.github.io/post/django/django-image-upload/</link>
      <pubDate>Mon, 24 May 2021 17:25:54 +0530</pubDate>
      <guid>https://amanbagrecha.github.io/post/django/django-image-upload/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, we look into how to upload multiple geo-tagged/non-geotagged images to aws s3 using plain Django and spatialite as databbase. We use GeoDjango to store the latitude, longitude extracted from geo-tagged images into the database.&lt;/p&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;h3 id=&#34;project-setup&#34;&gt;Project setup&lt;/h3&gt;
&lt;p&gt;create django project&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;django-admin startproject login_boiler_plate
create app python manage.py startapp GisMap
create superuser python manage.py createsuperuser
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt; add the app to &lt;code&gt;installed_app&lt;/code&gt; list and setup the default location for media storage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
	...
	&#39;GisMap&#39;,
]

MEDIA_ROOT =  os.path.join(BASE_DIR, &#39;media&#39;) 
MEDIA_URL = &#39;/media/&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setup-the-database-backend-to-postgis-extenstion-of-postgresql&#34;&gt;&lt;strong&gt;Setup the database backend to postgis extenstion of postgresql.&lt;/strong&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# in settings.py file
DATABASES = {
	&#39;default&#39;: {
		 &#39;ENGINE&#39;: &#39;django.contrib.gis.db.backends.postgis&#39;, #imp
		 &#39;NAME&#39;: &#39;database_name_here&#39;,
		 &#39;USER&#39;: &#39;postgres&#39;,
		&#39;PASSWORD&#39;: &#39;password_here&#39;,
		&#39;HOST&#39;: &#39;localhost&#39;,
		&#39;PORT&#39;: &#39;5432&#39;,
	},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;models.py&lt;/code&gt;, create model for uploading images. &lt;code&gt;DateTimeField&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt; are not necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.db import models
from django.contrib.auth.models import User


class ImageUpload(models.Model):
	user = models.ForeignKey(User, null=True, on_delete=models.CASCADE)
	image = models.ImageField( null=False, blank=False, upload_to = &#39;images/&#39;)
	date_created = models.DateTimeField(auto_now_add=True, null=True)

	def __str__(self):
		return self.user.username + &amp;quot; uploaded: &amp;quot;+ self.image.name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;forms.py&lt;/code&gt;, refer to the ImageUpload model for input.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  
from django.forms import ModelForm
from django.contrib.auth.models import User
from .models import ImageUpload

class ImageForm(ModelForm):
	class Meta:
		model = ImageUpload
		fields = (&#39;image&#39;,)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;home.html&lt;/code&gt;, create the form to accept image upload.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;                  &amp;lt;!-- Modal --&amp;gt;
                  &amp;lt;form method = &amp;quot;post&amp;quot; enctype=&amp;quot;multipart/form-data&amp;quot;&amp;gt;
                  &amp;lt;div class=&amp;quot;modal fade&amp;quot; id=&amp;quot;exampleModal&amp;quot; tabindex=&amp;quot;-1&amp;quot; role=&amp;quot;dialog&amp;quot; aria-labelledby=&amp;quot;exampleModalLabel&amp;quot; aria-hidden=&amp;quot;true&amp;quot; &amp;gt;
                    {% csrf_token %}
                    &amp;lt;div class=&amp;quot;modal-dialog&amp;quot; role=&amp;quot;document&amp;quot;&amp;gt;
                      &amp;lt;div class=&amp;quot;modal-content&amp;quot;&amp;gt;
                        &amp;lt;div class=&amp;quot;modal-header&amp;quot;&amp;gt;
                          &amp;lt;h5 class=&amp;quot;modal-title&amp;quot; id=&amp;quot;exampleModalLabel&amp;quot;&amp;gt;Upload Image&amp;lt;/h5&amp;gt;
                          &amp;lt;button type=&amp;quot;button&amp;quot; class=&amp;quot;close&amp;quot; data-dismiss=&amp;quot;modal&amp;quot; aria-label=&amp;quot;Close&amp;quot;&amp;gt;
                            &amp;lt;span aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;amp;times;&amp;lt;/span&amp;gt;
                          &amp;lt;/button&amp;gt;
                        &amp;lt;/div&amp;gt;
                        &amp;lt;div class=&amp;quot;modal-body&amp;quot;&amp;gt;
                          {{ image_form.image }}
                        &amp;lt;/div&amp;gt;
                        &amp;lt;div class=&amp;quot;modal-footer&amp;quot;&amp;gt;
                          &amp;lt;button type=&amp;quot;button&amp;quot; class=&amp;quot;btn btn-secondary&amp;quot; data-dismiss=&amp;quot;modal&amp;quot;&amp;gt;Close&amp;lt;/button&amp;gt;
                          &amp;lt;button type=&amp;quot;submit&amp;quot; class=&amp;quot;btn btn-primary&amp;quot;&amp;gt;Save Image&amp;lt;/button&amp;gt;
                        &amp;lt;/div&amp;gt;
                      &amp;lt;/div&amp;gt;
                    &amp;lt;/div&amp;gt;
                  &amp;lt;/div&amp;gt;
                  &amp;lt;/form&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, accept the HTTP POST request and save to the database. We will alter this to extract latitude, longitude later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@login_required(login_url=&#39;login&#39;)
def home_page(request):

	if request.method == &#39;POST&#39;:
		form = ImageForm(request.POST , request.FILES)
		print(form)
		if form.is_valid():
			print(&amp;quot;is valid&amp;quot;)
			obj = form.save(commit=False)
			obj.user = request.user
			obj.save()
		return redirect(&#39;home&#39;)
	else:
		Imageform = ImageForm()
		return render(request, &amp;quot;GisMap/home.html&amp;quot;, {&#39;Title&#39;: &amp;quot;Home Page&amp;quot;, &amp;quot;image_form&amp;quot;: ImageForm})

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;get-lat-lon-from-image-meta-deta-exchangeable-image-file-format-exif-&#34;&gt;Get Lat, lon from image meta deta (Exchangeable image file format [EXIF] )&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Geodjango is built on top of django and adds spatial functionality such as storing points, lines , polygon and multipolygon. It is prepackaged with Django but requires few additional softwares to make it fully functional. These include- GDAL, PROJ, GEOS, PostGIS. These can be downloaded from osgeo4W which bundles all these libraries. Then application can be added to apps in settings with &lt;code&gt;django.contrib.gis&lt;/code&gt; to the installed apps.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By default geodjango is not installed in the apps list and thus we do it ourself.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install django-geo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE- ensure os4geo is installed: install from &lt;a href=&#34;https://qgis.org/en/site/forusers/download.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; if not done.  And make the following changes in &lt;code&gt;settings.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;An additional setting is required, which is to locate osgeo4w directory in django. If you install osgeo4w in default directory, you need to put the following code within the settings.py file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
...
	&#39;django.contrib.gis&#39;,
]



import os
import posixpath
if os.name == &#39;nt&#39;:
	import platform
	OSGEO4W = r&amp;quot;C:\OSGeo4W&amp;quot;
	if &#39;64&#39; in platform.architecture()[0]:
		OSGEO4W += &amp;quot;64&amp;quot;
	assert os.path.isdir(OSGEO4W), &amp;quot;Directory does not exist: &amp;quot; + OSGEO4W
	os.environ[&#39;OSGEO4W_ROOT&#39;] = OSGEO4W
	os.environ[&#39;GDAL_DATA&#39;] = OSGEO4W + r&amp;quot;\share\gdal&amp;quot;
	os.environ[&#39;PROJ_LIB&#39;] = OSGEO4W + r&amp;quot;\share\proj&amp;quot;
	os.environ[&#39;PATH&#39;] = OSGEO4W + r&amp;quot;\bin;&amp;quot; + os.environ[&#39;PATH&#39;]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;models.py&lt;/code&gt;, add a PointField which can store geospatial information (lat,lon)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.contrib.gis.db import models
class ImageUpload():
  ...  
  geom = models.PointField( null=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, define functions to extract meta data from image and convert into right format for GeoDjango to understand it. Courtesy of &lt;a href=&#34;https://developer.here.com/blog/getting-started-with-geocoding-exif-image-metadata-in-python3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jayson DeLancey&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#________________________________________FUNCTIONS FOR IMAGE EXIF DATA______________________________________________________________________________#



from PIL import Image
from urllib.request import urlopen
from PIL.ExifTags import GPSTAGS
from PIL.ExifTags import TAGS

def get_decimal_from_dms(dms, ref):

	degrees = dms[0]
	minutes = dms[1] / 60.0
	seconds = dms[2] / 3600.0

	if ref in [&#39;S&#39;, &#39;W&#39;]:
		degrees = -degrees
		minutes = -minutes
		seconds = -seconds

	return round(degrees + minutes + seconds, 5)

def get_coordinates(geotags):
	lat = get_decimal_from_dms(geotags[&#39;GPSLatitude&#39;], geotags[&#39;GPSLatitudeRef&#39;])

	lon = get_decimal_from_dms(geotags[&#39;GPSLongitude&#39;], geotags[&#39;GPSLongitudeRef&#39;])

	return (lon, lat)



def get_geotagging(exif):
	if not exif:
		raise ValueError(&amp;quot;No EXIF metadata found&amp;quot;)

	geotagging = {}
	for (idx, tag) in TAGS.items():
		if tag == &#39;GPSInfo&#39;:
			if idx not in exif:
				raise ValueError(&amp;quot;No EXIF geotagging found&amp;quot;)

			for (key, val) in GPSTAGS.items():
				if key in exif[idx]:
					geotagging[val] = exif[idx][key]

	return geotagging

#_______________________________________________________________________________________________________________________________________#


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, update home_page function to extract meta data and save the image to database.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.contrib.gis.geos import Point

@login_required(login_url=&#39;login&#39;)
def home_page(request):
    if request.method == &amp;quot;POST&amp;quot;:
        form = ImageForm(request.POST, request.FILES)
        img = Image.open(request.FILES.get(&amp;quot;image&amp;quot;))
        if form.is_valid():
            try:
                obj = form.save(commit=False)
                obj.user = request.user
                obj.image_url = obj.image.url
                geotags = get_geotagging(img._getexif())
                obj.geom = Point(
                    get_coordinates(geotags)
                )  # X is longitude, Y is latitude, Point(X,Y)
                obj.save()
                messages.success(request, f&amp;quot;image uploaded succesfully&amp;quot;)
            except ValueError as e:
                messages.warning(request, e)
        else:
            messages.warning(request, f&amp;quot;Invalid image type&amp;quot;)
        return redirect(&amp;quot;home&amp;quot;)
    else:
        Imageform = ImageForm()
        return render(
            request, &amp;quot;GisMap/home.html&amp;quot;, {&amp;quot;Title&amp;quot;: &amp;quot;Home Page&amp;quot;, &amp;quot;image_form&amp;quot;: ImageForm}
        )
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;upload-to-s3-bucket&#34;&gt;Upload to S3 bucket&lt;/h2&gt;
&lt;p&gt;Install boto3 package and django-storages. Add to installed packages. Additionally, provide Key:Value AWS credentials to access the bucket and change the default file storage to S3.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install django-storages
pip install boto3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
	...
	&#39;storages&#39;,
]

AWS_ACCESS_KEY_ID = &amp;quot;&amp;quot;
AWS_SECRET_ACCESS_KEY = &amp;quot;&amp;quot;
AWS_STORAGE_BUCKET_NAME = &amp;quot;&amp;quot;

AWS_S3_FILE_OVERWRITE = False
AWS_DEFAULT_ACL = None

DEFAULT_FILE_STORAGE = &#39;storages.backends.s3boto3.S3Boto3Storage&#39;

AWS_QUERYSTRING_AUTH = False // removes the query string
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE: Make the bucket public to be able to make HTTP request&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Provide policy to make our s3 bucket public. By default, the bucket is private and no read/wrtie access is provided for user from outside the s3 page. There are other ways to access private bucket by either Limiting access to specific IP addresses or Restricting access to a specific HTTP referer. For simplicity we make the bucket public.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;Version&amp;quot;:&amp;quot;2012-10-17&amp;quot;,
  &amp;quot;Statement&amp;quot;:[
    {
      &amp;quot;Sid&amp;quot;:&amp;quot;PublicRead&amp;quot;,
      &amp;quot;Effect&amp;quot;:&amp;quot;Allow&amp;quot;,
      &amp;quot;Principal&amp;quot;: &amp;quot;*&amp;quot;,
      &amp;quot;Action&amp;quot;:[&amp;quot;s3:GetObject&amp;quot;,&amp;quot;s3:GetObjectVersion&amp;quot;],
      &amp;quot;Resource&amp;quot;:[&amp;quot;arn:aws:s3:::DOC-EXAMPLE-BUCKET/*&amp;quot;]
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;accept-non-geotagged-images&#34;&gt;Accept non-geotagged images&lt;/h2&gt;
&lt;p&gt;At this point, we should be able to upload geotagged images to s3 bucket. Non-geotagged images are not yet accepted by the model and thus we create seperate model for it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/34006994/how-to-upload-multiple-images-to-a-blog-post-in-django&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Additional resource&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We now make separate model for accepting non-geotagged images similar to &lt;code&gt;ImageUpload&lt;/code&gt; model but without &lt;code&gt;PointField&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Photos(models.Model):

	user = models.ForeignKey(User, null=True, on_delete=models.CASCADE)
	image = models.ImageField(upload_to=&#39;photos/&#39;,null=True,blank=False)
	date_created = models.DateTimeField(auto_now_add=True, null=True)
	image_url = models.URLField(max_length=250, null=True, blank=False)

	class Meta:
		verbose_name = &#39;Photo&#39;
		verbose_name_plural = &#39;Photos&#39;

	def __str__(self):
		return self.user.username + &amp;quot; uploaded image &amp;quot;+ self.image.name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt; file, extend the home_page function to add a fallback for non-geotagged images.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if request.method == &amp;quot;POST&amp;quot;:

    # images will be in request.FILES
    post_request, files_request = request.POST, request.FILES

    form = PhotoForm(post_request or None, files_request or None)
    files = request.FILES.getlist(
        &amp;quot;images&amp;quot;
    )  # returns files: [&amp;lt;InMemoryUploadedFile: Image_name.jpg (image/jpeg)&amp;gt;, &amp;lt;InMemoryUploadedFile: Image_name.jpg (image/jpeg)&amp;gt;]
    if form.is_valid():
        user = request.user
        for f in files:

            # returns &amp;lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x360 at 0x1ED0CCC6280&amp;gt;
            img = Image.open(f)  
            
            try:
                geotags = get_geotagging(img._getexif())
                geoimage = ImageUpload(user=user, image=f)
                geoimageimg_upload.image_url = geoimage.image.url
                # X is longitude, Y is latitude, Point(X,Y) ; returns eg SRID=4326;POINT (11.88454 43.46708)
                geoimage.geom = Point(get_coordinates(geotags))
                geoimage.save()
            except:
                nongeoimage = Photos(user=user, image=f)
                nongeoimage.image_url = nongeoimage.image.url
                nongeoimage.save()
    else:
        print(&amp;quot;Form invalid&amp;quot;)
    return redirect(&amp;quot;home&amp;quot;)
else:
    Imageform = PhotoForm()
    return render(
        request, &amp;quot;GisMap/home.html&amp;quot;, {&amp;quot;Title&amp;quot;: &amp;quot;Home Page&amp;quot;, &amp;quot;image_form&amp;quot;: ImageForm}
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;accept-multiple-images&#34;&gt;Accept multiple images&lt;/h2&gt;
&lt;p&gt;Make a new form which accepts multiple image files to be uploaded at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class PhotoForm(forms.ModelForm):
	images = forms.FileField(widget=forms.ClearableFileInput(attrs={&#39;multiple&#39;: True}))

	class Meta:
		model = Photos
		fields = (&#39;images&#39;,)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;home.html&lt;/code&gt;, add &lt;code&gt;multiple&lt;/code&gt; attribute to allow for multiple selection of images at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;				&amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
				&amp;lt;label for=&amp;quot;note-image&amp;quot;&amp;gt;&amp;lt;/label&amp;gt;
				&amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;images&amp;quot; class=&amp;quot;form-control-file&amp;quot; id=&amp;quot;note-image&amp;quot; multiple&amp;gt;
				&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;final-note&#34;&gt;Final Note:&lt;/h2&gt;
&lt;p&gt;At this point, you should be able to upload multiple Images to the AWS S3 bucket and have coordinates extracted the geo-tagged images and segregate non-geotagged images.&lt;/p&gt;
&lt;p&gt;You learnt-&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to Setup GeoDjango&lt;/li&gt;
&lt;li&gt;How to Setup AWS S3 bucket&lt;/li&gt;
&lt;li&gt;How to Extract meta data from Image and store in database using PointField&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;These steps will ensure you have multiple images uploaded at once and all the geolocation information can be stored in database, which later can be import to QGIS for data visualisation. Although both postgresql and django admin allows users to visualise the data.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Django rest framework PDF creation and email via gmail SMTP and reportLab</title>
      <link>https://amanbagrecha.github.io/post/django/pdf-and-email-creation/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/django/pdf-and-email-creation/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ever wanted to send email with attachements that too in django? And have the attachments created from the user input? This post tries to solve exactly that.&lt;/p&gt;
&lt;h2 id=&#34;main-steps&#34;&gt;Main steps&lt;/h2&gt;
&lt;p&gt;In this blog we create PDF using &lt;code&gt;Report Lab&lt;/code&gt; and email it to the user using gmail SMTP service. All actions are performed in Django.&lt;/p&gt;
&lt;h2 id=&#34;step-1--create-django-view-to-serialize-data&#34;&gt;Step 1 : create django view to serialize data&lt;/h2&gt;
&lt;p&gt;To begin with, we create a view &lt;code&gt;CreatePDF&lt;/code&gt; which accepts &lt;code&gt;POST&lt;/code&gt; request and the data gets passed onto &lt;code&gt;CreatePDFSerializer&lt;/code&gt; which serializes our data and validates it. If our data is valid, we generate PDF using &lt;code&gt;generate_pdf&lt;/code&gt; function and email to the recipent (&lt;code&gt;emailaddress&lt;/code&gt; of the users) using the &lt;code&gt;sendPDF&lt;/code&gt; function. If everything does not execute properly, we return an error response else a success.&lt;/p&gt;
&lt;p&gt;The local variable &lt;code&gt;myresponse&lt;/code&gt; is a dictionary which helps us manage the response for each &lt;code&gt;return&lt;/code&gt; statement in the correct format as expected by &lt;code&gt;response&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
SUCCESS = &#39;success&#39;
ERROR = &#39;error&#39;
message_list = [&#39;response&#39;, &#39;status&#39;, &#39;message&#39;] # eg: [&amp;quot;success&amp;quot;, 201, &amp;quot;successfully upload the file&amp;quot;]

@csrf_exempt
@api_view([&#39;POST&#39;,])
def CreatePDF(request):
    myresponse = {k: [] for k in message_list}

    try:
        myData = request.data
        # serialier the data
        serializer = serializers.CreatePDFSerializer(data=myData)  
        if serializer.is_valid():
            try:
                sendPDF(**myData.dict())  # create pdf and send email
            except Exception as e:
                RequestResponse(
                    myresponse,
                    ERROR,
                    status.HTTP_400_BAD_REQUEST,
                    {&amp;quot;Email&amp;quot;: [&amp;quot;Could not send mail!&amp;quot;]},
                )
                return Response(data=myresponse)
            
            account = serializer.save()
            RequestResponse(
                myresponse,
                SUCCESS,
                status.HTTP_201_CREATED,
                {&amp;quot;Success&amp;quot;: [f&amp;quot;Inspection Report e-mailed to {account.EmailAddress}!&amp;quot;]},
            )
            return Response(data=myresponse)

        RequestResponse(
            myresponse, ERROR, status.HTTP_400_BAD_REQUEST, serializer.errors
        )
        return Response(data=myresponse)
    
    except Exception as e:
        print(e)
        RequestResponse(
            myresponse,
            ERROR,
            status.HTTP_500_INTERNAL_SERVER_ERROR,
            {&amp;quot;Error&amp;quot;: [&amp;quot;Internal Server Error&amp;quot;]},
        )
        return Response(data=myresponse)

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;step-2-generate-pdf-using-report-lab&#34;&gt;step 2: Generate PDF using Report Lab&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt; we create a function to generate pdf using &lt;code&gt;Report Lab&lt;/code&gt; package. This allows us to define the page size and line strings with text placement to be included.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_pdf(**Mydata):
    y = 700
    buffer = io.BytesIO()  # in memory create pdf
    p = canvas.Canvas(buffer, pagesize=letter)
    p.setFont(&#39;Helvetica&#39;, 14)
    p.drawString(220, y, Mydata[&#39;Title&#39;])
    p.drawString(450, y, &#39;Date:&#39; + timezone.now().strftime(&#39;%Y-%b-%d&#39;))
    p.line(30, 675, 550, 675)
    p.drawString(220, y - 300, &#39;Time&#39;
                 + str(Mydata[&#39;time&#39;]))
    p.showPage()
    p.save()
    pdf = buffer.getvalue()
    buffer.close()
    return pdf

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;step-3-send-email-via-smtp-backend&#34;&gt;step 3: Send Email via SMTP backend&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, we create &lt;code&gt;sendPDF&lt;/code&gt; function which calls the &lt;code&gt;generate_pdf&lt;/code&gt;to generate PDF and attaches the pdf to the email using the &lt;code&gt;EmailMessage&lt;/code&gt; class method &lt;code&gt;attach&lt;/code&gt;. We additionally need to setup backend for smtp service and host user which is to be done in &lt;code&gt;settings.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# views.py
def sendPDF(**Mydata):
	pdf = generate_pdf(**Mydata)
	msg = EmailMessage(Mydata[&#39;Title&#39;], &amp;quot; Your Report is ready! &amp;quot;, settings.EMAIL_HOST_USER, to=[Mydata[&#39;EmailAddress&#39;]])
	msg.attach(f&amp;quot;{Mydata[&#39;Title&#39;]}.pdf&amp;quot;, pdf, &#39;application/pdf&#39;)
	msg.content_subtype = &amp;quot;html&amp;quot;
	resp = msg.send()
	print(&amp;quot;resp:&amp;quot; , resp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# settings.py
EMAIL_BACKEND = &#39;django.core.mail.backends.smtp.EmailBackend&#39;
EMAIL_HOST = &amp;quot;smtp.gmail.com&amp;quot;
EMAIL_HOST_USER = &#39;your_email@gmail.com&#39;
EMAIL_HOST_PASSWORD = &#39;your_password&#39;
EMAIL_PORT = 587
EMAIL_USE_TLS = True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have been able to successfully setup and send email with attachment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Polygon Selection and  Area Calculation in Openlayers</title>
      <link>https://amanbagrecha.github.io/post/openlayers/polygon-selection-and-area-calculation-in-openlayers/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/openlayers/polygon-selection-and-area-calculation-in-openlayers/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this small demo-blog we look into how to make polygon selections on the map and calculate the area of that polygon on-the-fly. We use openlayers v6 for client side and geoserver to save our vector layers for this exercise.&lt;br&gt;
I assume readers to have familiarity with setting up geoserver and basics of openlayers.&lt;/p&gt;
&lt;h2 id=&#34;step-1-setup-openlayers&#34;&gt;&lt;strong&gt;Step 1: Setup openlayers&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Openlayers requires you to add these cdns to add their functionality into our application.&lt;/p&gt;
&lt;h3 id=&#34;link-necessary-cdns&#34;&gt;link necessary cdns&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;
 &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/css/ol.css&amp;quot; type=&amp;quot;text/css&amp;quot;&amp;gt;
 &amp;lt;script src=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/build/ol.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are using openlayers to render the request response. since the output of the WFS request is json, we create a new layer with vector source and format as geojson. 
The &lt;code&gt;strategy:ol.loadingstrategy.bbox&lt;/code&gt; tells openlayers to only load features within the bbox. Simply put, if we move to different location, only features within that bbox will appear.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
// setup geoserver port
var geoserver_ip = &#39;http://120.0.0.1&#39;
var geoserver_port = &#39;8080&#39;

// define vector source
var myFlSource = new ol.source.Vector({
	format: new ol.format.GeoJSON(),
		url: function (extent){
			return ( geoserver_ip +&#39;:&#39; + geoserver_port + &#39;/geoserver/dronnav/ows?service=WFS&amp;amp;version=1.1.0&amp;amp;request=GetFeature&amp;amp;typeName=dronnav%3Aflorida_bp&amp;amp;maxFeatures=10000&amp;amp;outputFormat=application/json&amp;amp;srsname=EPSG:4326&amp;amp;&#39; + &#39;bbox=&#39; + extent.join(&#39;,&#39;) + &#39;,EPSG:4326&#39; );
		},
		strategy:ol.loadingstrategy.bbox,
	});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We perform WFS request from geoserver to get our layer &lt;code&gt;florida_bp&lt;/code&gt; in this case. The parameters are as explained as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;service=WFS&lt;/code&gt; : web feature service to perform interaction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;typename=workspace:florida_bp&lt;/code&gt; : specify the workspace and layer name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;version=1.1.0&lt;/code&gt; : version number&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;maxFeatures = 10000&lt;/code&gt; : since WFS request is computationaly expensive, we restrict to only load 10000 features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;request=GetFeature&lt;/code&gt; : request type. There are several other which can be found here&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;outputFormat=application/json&lt;/code&gt; : the output format as response&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;srsname=EPSG:4326&lt;/code&gt; : coordinate reference system to display on the map&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;bbox=&lt;/code&gt; : bounding box&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// define vector layer
var floridaLayer = new ol.layer.Vector({
	source: myFlSource,
	style: new ol.style.Style({
		fill: new ol.style.Fill({
			color: &#39;rgba(1, 1, 255, .2)&#39;,
			}),
		stroke: new ol.style.Stroke({
			color: &#39;rgba(1, 1, 255, .5)&#39;,
			width: 2,
		}),
		}),
		minZoom: 16, // this will allows us to send request only when the zoom is atleast 16
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the layer is defined, we need to add this layer to the map. We can either use &lt;code&gt;map.addLayer(layername)&lt;/code&gt; or add to array in the map (&lt;code&gt;Fig.1&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// add ESRI basemap
var project_maplayer =    new ol.layer.Tile({
	source: new ol.source.XYZ({
		attributions: [&#39;Powered by Esri&#39;,
										&#39;Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community&#39;],
		attributionsCollapsible: false,
		url: &#39;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}&#39;,
		maxZoom: 23
	}),
	zIndex: 0
});

// add view with projection set to EPSG 4326 for the map
var project_view = new ol.View({
	projection: &#39;EPSG:4326&#39;,
	center: [-81.80808208706726, 27.285095000261222],
	zoom: 7,
});

// define the map with all the layers created previously
var Projectmap = new ol.Map({
	layers: [project_maplayer, floridaLayer],
	overlays: [overlay],
	target: &#39;project_map&#39;, // the div element `id` in html page
	view: project_view,
});
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/jodNbPQ.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -The Map layer with building footprints (&lt;code&gt;floridaLayer&lt;/code&gt;) added to the map with the style we specified&lt;/code&gt; for each feature&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;get-feature-info-on-click&#34;&gt;&lt;strong&gt;Get feature info on click&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;After adding basemap and our layer to the map served via geoserver, we are now ready to get information &lt;code&gt;on-click&lt;/code&gt;. We use  &lt;code&gt;forEachFeatureAtPixel&lt;/code&gt; method on our layer to send a WFS request to our geoserver and recive a response in json format. We change the style of the building on click (&lt;code&gt;Fig.2&lt;/code&gt;). The area is calculated using formatArea function which utilises &lt;code&gt;ol.sphere.getArea&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; method to calculate area and change CRS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
  /*  select ploygon the feature and get area and store the features */
  var selected = []; // contains all features
  var selected_area = []; // contains area of feature, one-to-one
  
  Projectmap.on(&#39;singleclick&#39;, function (e) {
  Projectmap.forEachFeatureAtPixel(e.pixel, function (f, l) {
  	var mycoordinate = e.coordinate
  	storef = f  // feature
  	/* if click is on polygon, then select the feature */
  if ( f.getGeometry()   instanceof  ol.geom.MultiPolygon ) {
		  
		var selIndex = selected.indexOf(f);
			// console.log(selIndex)
		if (selIndex &amp;lt; 0) {
			selected.push(f);
			selected_area.push( formatArea(f) ); // formatArea function returns the area in ft2
			f.setStyle(highlightStyle); // change style on click
		} else {
			selected.splice(selIndex, 1);
			selected_area.splice( selIndex, 1);
			f.setStyle(undefined);
		}
 	 }

	  })

	  /* update the tags with no of selected feature and total area combined */
	  document.getElementById(&#39;status-selected&#39;).innerHTML = &#39;&amp;amp;nbsp;&#39; + selected.length + &#39; selected features&#39;;
	  document.getElementById(&#39;status-selected_area&#39;).innerHTML = &#39;&amp;amp;nbsp;&#39; + selected_area.reduce(getSum, 0) + &#39; ft&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt;&#39;;
	  
	});
	

	
  /* style for selected feature on click  */
  var highlightStyle = new ol.style.Style({
  	fill: new ol.style.Fill({
  	color: &#39;#f0b88b&#39;,
  	}),
  	stroke: new ol.style.Stroke({
  	color: &#39;#f0b88b&#39;,
  	width: 3,
  	}),
  });


  /*  function for calculating area of the polygon (feature) selected */
  function formatArea (polygon){
   var area = ol.sphere.getArea(polygon.getGeometry().transform(&#39;EPSG:4326&#39;, &#39;EPSG:3857&#39;)); // transform to projected coordinate system.
   var output;
   output = Math.round(area * 100*10.7639) / 100  ; //in ft2
   polygon.getGeometry().transform(&#39;EPSG:3857&#39;, &#39;EPSG:4326&#39; ) //convert back to geographic crc
   return output;
  }
  
  /*  function for array sum */
  function getSum(total, num) {
  return total + Math.round(num);
  }

&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/TCicCHu.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -The &lt;code&gt;floridaLayer&lt;/code&gt; building footprints selected  with the style we specified&lt;/code&gt; for each feature&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;final-comments&#34;&gt;&lt;strong&gt;Final comments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This post demonstrates the use of &lt;code&gt;strategy:ol.loadingstrategy.bbox&lt;/code&gt; to load only the features that cover the bounding box. We use this strategy since WFS service is resouce intensive and our server cannot handle millions of HTTP request at once.&lt;/p&gt;
&lt;p&gt;We also see the use of &lt;code&gt;forEachFeatureAtPixel&lt;/code&gt; method to select our building footprints. On click of the feature we change the style using &lt;code&gt;setStyle&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Additionally, we saw how to change projection on-the-fly using &lt;code&gt;ol.sphere.getArea&lt;/code&gt; method. A word of caution while using &lt;code&gt;EPSG:3857&lt;/code&gt;. My AOI was on the equator and thus calculating area does not result in significant error. But if the AOI is in temperate zone then adopt suitable projection CRS.&lt;/p&gt;
&lt;p&gt;Layer Credit: Microsoft buidling footprints &lt;a href=&#34;https://github.com/microsoft/USBuildingFootprints&#34;&gt;https://github.com/microsoft/USBuildingFootprints&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Full Fledged CRUD application using DRF and Token Authentication</title>
      <link>https://amanbagrecha.github.io/post/django/crud-in-django-rest-framework/</link>
      <pubDate>Sat, 22 May 2021 15:13:58 +0530</pubDate>
      <guid>https://amanbagrecha.github.io/post/django/crud-in-django-rest-framework/</guid>
      <description>&lt;hr style=&#34;border:1px solid lightgray&#34;&gt; &lt;/hr&gt;
&lt;br&gt;
&lt;h2 id=&#34;what-will-you-learn&#34;&gt;What will you learn&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;color: grey;font-size: 18px;&#34;&gt;Too Long; Didn&amp;rsquo;t Read &lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Markdown&lt;/th&gt;
&lt;th&gt;Less&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DRF&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Create API end points for CRUD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Token Authentication&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Add security and authorised access&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Fetch API calls&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Consume API from front-end&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Password Reset&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Send email to reset your forgotton password&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-step-one--basic-django-project-setup&#34;&gt;1. Step one : Basic Django Project setup&lt;/h2&gt;
&lt;p&gt;Create virtual environment&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name djangoEnv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Activate the environment&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda activate djangoEnv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install the dependencies&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install django
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in your command line&lt;/p&gt;
&lt;p&gt;create project &lt;code&gt;django-admin startproject tutorial&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;create app &lt;code&gt;python manage.py startapp Accountsapp&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;create superuser &lt;code&gt;python manage.py createsuperuser&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now that we have the project and app installed your structure should look like this (insert picture here)&lt;/p&gt;
&lt;p&gt;Register the app in  file as follows&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Installed_apps = [ 
    &#39;Accountsapp.apps.AccountsappConfig&#39;,
    ...
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now create our own custom model named &lt;em&gt;&lt;strong&gt;MyAccounts&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;models.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.db import models
from django.contrib.auth.models import AbstractBaseUser, BaseUserManager

from django.conf import settings
from django.db.models.signals import post_save
from django.dispatch import receiver
from rest_framework.authtoken.models import Token


class MyAccountManager(BaseUserManager):
	def create_user(self, email, username, password=None):
		if not email:
			raise ValueError(&#39;Users must have an email address&#39;)
		if not username:
			raise ValueError(&#39;Users must have a username&#39;)


		user = self.model(
			email=self.normalize_email(email),
			username=username,
		)

		user.set_password(password)
		user.save(using=self._db)
		return user

	def create_superuser(self, email, username, password):
		user = self.create_user(
			email=self.normalize_email(email),
			password=password,
			username=username,
			
		)
		user.is_admin = True
		user.is_staff = True
		user.is_superuser = True
		user.save(using=self._db)
		return user

# creating custom model of &amp;quot;User&amp;quot; base model. 
class MyAccount(AbstractBaseUser):
	email 					= models.EmailField(verbose_name=&amp;quot;email&amp;quot;, max_length=60, unique=True)
	username 				= models.CharField(max_length=30, unique=True)
	date_joined				= models.DateTimeField(verbose_name=&#39;date joined&#39;, auto_now_add=True)
	last_login				= models.DateTimeField(verbose_name=&#39;last login&#39;, auto_now=True)
	is_admin				= models.BooleanField(default=False)
	is_active				= models.BooleanField(default=True)
	is_staff				= models.BooleanField(default=False)
	is_superuser			= models.BooleanField(default=False)


	USERNAME_FIELD = &#39;email&#39;   # username_field is the one which should be unique and will be compared by django for not creating multiple users with same email.

	REQUIRED_FIELDS = [&#39;username&#39;] 

	objects = MyAccountManager()

	def __str__(self):
		return self.email

	# For checking permissions. to keep it simple all admin have ALL permissons
	def has_perm(self, perm, obj=None):
		return self.is_admin

	# Does this user have permission to view this app? (ALWAYS YES FOR SIMPLICITY)
	def has_module_perms(self, app_label):
		return True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To tell django we are overwriting the default user model, we do the following&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;AUTH_USER_MODEL = Accounts.MyAccounts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we makemigrates to register the model in our database&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python manage.py makemigrations
python manage.py migrate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And for the model to be visible in admin section we do the following&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;admin.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.contrib import admin
from .models import MyAccount

admin.site.register(MyAccount) # Register your models here.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For now the our project is setup. We move to Django Rest Framework setup&lt;/p&gt;
&lt;h2 id=&#34;2-setup-django-rest-framework-with-authentication&#34;&gt;2. Setup Django Rest Framework with Authentication&lt;/h2&gt;
&lt;p&gt;Install dependeny&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install djangorestframework
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like any other app, django rest framework is also an app. so we add it to the list of installed apps. 
We additionally add authtoken app for user authentication which we are shortly going to intergrate in our CRUD application&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
    # my apps
    &#39;Accountsapp.apps.AccountsappConfig&#39;,
    # restframework
    &#39;rest_framework&#39;,
    &#39;rest_framework.authtoken&#39;,
    ...
    
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to be using Token Authentication in this application. DRF documentation recommends it as the default. 
Let Us setup the Default authentication class before actually utilising it.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;REST_FRAMEWORKÂ =Â {
Â Â Â Â &#39;DEFAULT_AUTHENTICATION_CLASSES&#39;:Â [
Â Â Â Â Â Â Â Â &#39;rest_framework.authentication.TokenAuthentication&#39;,
Â Â Â Â Â Â Â Â 
Â Â Â Â ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last thing before we actually start writing code is to perform migration. TheÂ &lt;code&gt;rest_framework.authtoken&lt;/code&gt;Â app provides Django database migrations.&lt;/p&gt;
&lt;p&gt;As done previously on command line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python manage.py makemigrations
python manage.py migrate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have completed the logistics for setting up DRF&lt;/p&gt;
&lt;h2 id=&#34;3-building-crud-application&#34;&gt;3. Building CRUD application&lt;/h2&gt;
&lt;p&gt;We would first create a folder called &lt;strong&gt;api&lt;/strong&gt; inside our to seperate codebase for API and vanila CRUD&lt;/p&gt;
&lt;p&gt;Inside API folder create four files,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__init__.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serializers.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;views.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;urls.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In &lt;code&gt;serializers.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rest_framework import serializers 
from Accountsapp.models import MyAccount # import our custom model


# provide fields in meta, expression and in MyAccount. for admin page login and edit,  is_admin and is_staff should be true
class RegistrationSerializer(serializers.ModelSerializer):

    # additional fields 
	password2 = serializers.CharField(style={&#39;input_type&#39;: &#39;password&#39;}, write_only=True)
	is_superuser =serializers.BooleanField(write_only=True)
	
    class Meta:
		model = MyAccount
        # mention the fields you want to display when request is sent. 
		fields = [&#39;id&#39;,&#39;email&#39;, &#39;username&#39;, &#39;password&#39;, &#39;password2&#39;,  &#39;is_superuser&#39;]
		extra_kwargs = {
				&#39;password&#39;: {&#39;write_only&#39;: True},  # tells django to not display the password for others to see
		}	


	def	save(self):

		account = MyAccount(
					email=self.validated_data[&#39;email&#39;],
					username=self.validated_data[&#39;username&#39;],
					# is_admin=self.validated_data[&#39;is_admin&#39;],
					is_superuser= self.validated_data[&#39;is_superuser&#39;],
				)
		password = self.validated_data[&#39;password&#39;]
		password2 = self.validated_data[&#39;password2&#39;]
		if password != password2:
			raise serializers.ValidationError({&#39;password&#39;: &#39;Passwords must match.&#39;})
		account.set_password(password)
		account.save()
		return account


class UpdateSerializer(serializers.ModelSerializer):

	class Meta:
		model = MyAccount
		# mention the fields you want to display when request is sent. 
		fields = [&#39;id&#39;, &#39;username&#39;, &#39;email&#39;]
		extra_kwargs = {
				&#39;password&#39;: {&#39;read_only&#39;: True},  #  password cannot be edited from here
		}


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : Do not try to update the password from serializers. There is another technique which we will deal with in later section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The serializers in REST framework work very similarly to Djangoâ€™s Form and ModelForm classes. The two major serializers that are most popularly used are ModelSerializer and HyperLinkedModelSerialzer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rest_framework import status
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated, IsAdminUser
from django.contrib.auth import authenticate
from rest_framework.authentication import TokenAuthentication
from rest_framework.decorators import api_view, authentication_classes, permission_classes

from . import serializers 
from Accountsapp.models import MyAccount
from rest_framework.authtoken.models import Token

# user views
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from rest_framework.parsers import JSONParser
from django.core.exceptions import ObjectDoesNotExist
import json

# login {built-in django}
from django.contrib.auth import login 
from django.contrib.auth.decorators import login_required



# get all users
@api_view([&amp;quot;GET&amp;quot;])
@csrf_exempt
@permission_classes([IsAuthenticated,])
@authentication_classes([TokenAuthentication])
def get_users(request):
    try:
        user_profile = MyAccount.objects.all() 
        serializer = serializers.RegistrationSerializer(user_profile, many=True)
        return Response( {&#39;USER_PROFILE&#39;:serializer.data}, status= status.HTTP_200_OK)
    except ObjectDoesNotExist:
        return JsonResponse({&#39;Response&#39;: &#39;You do not have authorization to access this page&#39;}, status=status.HTTP_401_UNAUTHORIZED)



# get given user
@api_view([&#39;GET&#39;])
@csrf_exempt
@permission_classes([IsAuthenticated,])
@authentication_classes([TokenAuthentication])
def get_given_user(request, pk):
    try:
        user_profile = MyAccount.objects.get(pk=pk)
    except ObjectDoesNotExist:
        return JsonResponse({&amp;quot;missing&amp;quot;: &amp;quot;The requested object does not exist&amp;quot;}, status=status.HTTP_404_NOT_FOUND)

    if request.method == &#39;GET&#39;:  
        serializer = serializers.RegistrationSerializer(user_profile)
        token = Token.objects.get(user=user_profile).key
        return JsonResponse({&#39;given_user_profile&#39;: serializer.data, &#39;token&#39;:token})
   


# add user
@csrf_exempt
@api_view([&#39;POST&#39;])
def user_add_view(request):
        serializer = serializers.RegistrationSerializer( data=request.data)
        if serializer.is_valid():
            account = serializer.save()
            token, _ = Token.objects.get_or_create(user=account)
            return Response(serializer.data, status=status.HTTP_201_CREATED,  headers={&#39;Authorization&#39;: &#39;Token &#39; + token.key})
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)



# update user
@api_view([&amp;quot;PUT&amp;quot;,&#39;GET&#39;])
@csrf_exempt
@permission_classes([IsAuthenticated,])
@authentication_classes([TokenAuthentication])
def update_user(request, pk):

    try:
        user_profile = MyAccount.objects.get(id=pk)
    except ObjectDoesNotExist:
        return Response({&#39;response&#39;: &amp;quot;given object does not exist&amp;quot;}, status=status.HTTP_404_NOT_FOUND)

    user = request.user
    try:
        data =  {i:j for i,j in request.query_params.items()}
        print(data)
        serializer = serializers.UpdateSerializer(user_profile, data=data)
        if serializer.is_valid():
            user= serializer.save()
            token, _ = Token.objects.get_or_create(user=user)
            return Response({&amp;quot;response&amp;quot;: &amp;quot;success&amp;quot;, &#39;data&#39; :serializer.data}, status=status.HTTP_201_CREATED,  headers={&#39;Authorization&#39;: &#39;Token &#39; + token.key})
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)

    except ObjectDoesNotExist as e:
        return JsonResponse({&#39;error&#39;: str(e)}, safe=False, status=status.HTTP_404_NOT_FOUND)
    except Exception:
        return JsonResponse({&#39;error&#39;: &#39;Something terrible went wrong&#39;}, safe=False, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



# delete user
@api_view([&amp;quot;DELETE&amp;quot;,&#39;GET&#39;]) 
@csrf_exempt
@permission_classes([IsAuthenticated])
@authentication_classes([TokenAuthentication])
def delete_user(request, pk):

    try:
        user_profile = MyAccount.objects.get(id=pk)
    except ObjectDoesNotExist:
        return JsonResponse({&#39;response&#39;: &amp;quot;given object does not exist&amp;quot;}, safe=False, status=status.HTTP_404_NOT_FOUND)

    user = request.user
    if user_profile != user: 
        return JsonResponse({&#39;response&#39;:&amp;quot;You don&#39;t have permission to delete the record.&amp;quot;}, safe=False, status=status.HTTP_401_UNAUTHORIZED)

    try:
        user_profile.delete()  #retuns 1 or 0
        return JsonResponse({&#39;user_delete&#39;: &amp;quot;record deleted&amp;quot;}, safe=False, status=status.HTTP_200_OK)
    except ObjectDoesNotExist as e:
        return JsonResponse({&#39;error&#39;: str(e)}, safe=False, status=status.HTTP_404_NOT_FOUND)
    except Exception:
        return JsonResponse({&#39;error&#39;: &#39;Something terrible went wrong&#39;}, safe=False, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



# login view and get token
@api_view([&amp;quot;POST&amp;quot;, ])
def drflogin(request):

    email = request.data.get(&amp;quot;email&amp;quot;)
    username = request.data.get(&amp;quot;username&amp;quot;)
    password = request.data.get(&amp;quot;password&amp;quot;)
    account = MyAccount.objects.filter(email=email) | MyAccount.objects.filter(username=username)
    if not account:
        return Response({&amp;quot;error&amp;quot;: &amp;quot;Login failed&amp;quot;}, status=status.HTTP_401_UNAUTHORIZED)
    # authenticate(email=email, password=password)  # returns none if not authenticated
    account = authenticate(email=account[0].email, password=password)
    token, _ = Token.objects.get_or_create(user=account)
    login(request,account)  
    renderer= Response({&amp;quot;response&amp;quot; : &amp;quot;Successfully authenticated&amp;quot;,  &amp;quot;pk&amp;quot;: account.pk, &amp;quot;username&amp;quot;: account.username, &amp;quot;token&amp;quot;: token.key }, template_name= &amp;quot;Accountsapp/loginuser.html&amp;quot;, headers={&#39;Authorization&#39;: &#39;Token &#39; + token.key})
    return renderer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Setup end points for our API&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from django.urls import path, include
from . import views as drf_views


app_name = &#39;Accountsapp&#39;

urlpatterns = [

    path(&#39;drf_users/&#39;, drf_views.get_users, name= &#39;drf_users&#39;),
    path(&#39;drf_user/&amp;lt;int:pk&amp;gt;/&#39;, drf_views.get_given_user, name= &#39;drf_get_user&#39;),
    path(&#39;drf_updateuser/&amp;lt;int:pk&amp;gt;/&#39;, drf_views.update_user, name= &#39;drf_updateusers&#39;),
    path(&#39;drf_deleteuser/&amp;lt;int:pk&amp;gt;/&#39;, drf_views.delete_user, name= &#39;drf_deleteuser&#39;),
    path(&#39;drf_adduser/&#39;, drf_views.user_add_view, name= &#39;drf_adduser&#39;),
    path(&#39;drf_login/&#39;, drf_views.drflogin, name=&#39;drf_login&#39;),

    
]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first create users and then test  delete, update and show users functionality of our API. We will use Postman for timebeing. Later we will built the front-end to perform all these actions.&lt;/p&gt;
&lt;h2 id=&#34;span-style-colororange-post-span-request-add-user&#34;&gt;&lt;span style= &#34;color:orange&#34;&gt; POST &lt;/span&gt; REQUEST: &lt;strong&gt;ADD USER&lt;/strong&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_adduser/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Ea8W3Bj.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colorgreen-get-span-request-get-users&#34;&gt;&lt;span style= &#34;color:green&#34;&gt; GET &lt;/span&gt; REQUEST: &lt;strong&gt;GET USERS&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_users/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request GET &#39;http://127.0.0.1:8000/drf_users/&#39; \
--header &#39;Authorization: Token 92cc8c32edb7bd111b89552a3031f918d2df5613&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using postman&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/dPnv4J4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colorred-del-span-request-delete-user&#34;&gt;&lt;span style= &#34;color:RED&#34;&gt; DEL &lt;/span&gt; REQUEST: &lt;strong&gt;DELETE USER&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_deleteuser/&amp;lt;int:pk&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request DELETE &#39;http://127.0.0.1:8000/drf_deleteuser/21&#39; \
--header &#39;Authorization: Token 1529e77c59999f819649828a5e9174ba44bd6bb4&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using postman&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/6IFah1s.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colordodgerblue-put-span-request-update-user&#34;&gt;&lt;span style= &#34;color:dodgerblue&#34;&gt; PUT &lt;/span&gt; REQUEST: &lt;strong&gt;UPDATE USER&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_updateuser/1/?username=updated_username_here&amp;amp;email=updated_email_here
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request PUT &#39;http://127.0.0.1:8000/drf_updateuser/8/?username=rcbfl&amp;amp;email=rcbfl@gmail.com&#39; \
--header &#39;Authorization: Token 506ce0bbf7fa50f613678024586669d9b6bd82a0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;using postman
&lt;img src=&#34;https://i.imgur.com/LhVZ34L.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colorgreen-get-span-request-get-user&#34;&gt;&lt;span style= &#34;color:green&#34;&gt; GET &lt;/span&gt; REQUEST: &lt;strong&gt;GET USER&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_user/&amp;lt;int:pk&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request GET &#39;http://127.0.0.1:8000/drf_user/8&#39; \
--header &#39;Authorization: Token 506ce0bbf7fa50f613678024586669d9b6bd82a0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;using postman&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/LiPdZIe.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;front-end-setup&#34;&gt;Front end setup&lt;/h2&gt;
&lt;p&gt;In root directory create folder  &lt;code&gt;templates\Accountsapp\&lt;/code&gt; and create &lt;code&gt;RegiserUser.html&lt;/code&gt; file in it. Create form field in the file as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;          &amp;lt;form class=&amp;quot;form-horizontal&amp;quot; action=&amp;quot;&amp;quot; method=&amp;quot;post&amp;quot;  id=&amp;quot;myForm&amp;quot; autocomplete=&amp;quot;off&amp;quot;&amp;gt;
          	{% csrf_token %}
            &amp;lt;!-- Name input--&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;username&amp;quot;&amp;gt;Name&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;username&amp;quot; name=&amp;quot;username&amp;quot; type=&amp;quot;text&amp;quot; placeholder=&amp;quot;Your username&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;!-- Email input--&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;email&amp;quot;&amp;gt;Your E-mail&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;email&amp;quot; name=&amp;quot;email&amp;quot; type=&amp;quot;email&amp;quot; placeholder=&amp;quot;Your email&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;!-- password body --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;password&amp;quot;&amp;gt;Password&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;password&amp;quot; name=&amp;quot;password&amp;quot; type=&amp;quot;password&amp;quot; placeholder=&amp;quot;Your password&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;!-- password body --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;password2&amp;quot;&amp;gt;Password2&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;password2&amp;quot; name=&amp;quot;password2&amp;quot; type=&amp;quot;password&amp;quot; placeholder=&amp;quot;confirm password&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            
            &amp;lt;!-- superuser input --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;superuser&amp;quot;&amp;gt;Is superuser&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-3&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;issuperuser&amp;quot; name=&amp;quot;issuperuser&amp;quot; type=&amp;quot;checkbox&amp;quot;  class=&amp;quot;form-control&amp;quot; &amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
 
    
            &amp;lt;!-- Form actions --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-6 text-left&amp;quot;&amp;gt;
                &amp;lt;button type=&amp;quot;submit&amp;quot; class=&amp;quot;btn btn-primary btn-lg&amp;quot;&amp;gt;Submit&amp;lt;/button&amp;gt;
              &amp;lt;/div&amp;gt;


            &amp;lt;/div&amp;gt;
          &amp;lt;/fieldset&amp;gt;
          &amp;lt;/form&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the form is created, we now need to take the input from the form and send to the register user API &lt;code&gt;drf_adduser/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;RegisterUser.html&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;


			function getCookie(name) {
		    var cookieValue = null;
		    if (document.cookie &amp;amp;&amp;amp; document.cookie !== &#39;&#39;) {
		        var cookies = document.cookie.split(&#39;;&#39;);
		        for (var i = 0; i &amp;lt; cookies.length; i++) {
		            var cookie = cookies[i].trim();
		            // Does this cookie string begin with the name we want?
		            if (cookie.substring(0, name.length + 1) === (name + &#39;=&#39;)) {
		                cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
		                break;
		            }
		        }
		    }
		    return cookieValue;
		}
		var csrftoken = getCookie(&#39;csrftoken&#39;);



function fetchcall(event) {

		event.preventDefault();
		console.log(&#39;form submitted&#39;);
	var username = document.getElementById(&amp;quot;username&amp;quot;).value;
	var email = document.getElementById(&amp;quot;email&amp;quot;).value;
	var password = document.getElementById(&amp;quot;password&amp;quot;).value;
	var password2 = document.getElementById(&amp;quot;password2&amp;quot;).value;
	var issuperuser = document.getElementById((&#39;issuperuser&#39;)).checked;
	console.log(issuperuser)

		var url = &#39;/drf_adduser/&#39;;

			fetch(url, {
				method:&#39;POST&#39;,
				headers:{
					&#39;Content-type&#39;:&#39;application/json&#39;,
					&#39;X-CSRFToken&#39;:csrftoken,
				},
				body:JSON.stringify({
					&#39;email&#39;:email,
					&#39;username&#39;:username,
					&amp;quot;password&amp;quot;:password,
					&amp;quot;password2&amp;quot;:password2,
					&amp;quot;is_superuser&amp;quot;: issuperuser
				})
			}
			).then(function(response){
				store_response= response;
				return response.json();

			}).then(function(data){
				store_data =JSON.stringify(data);
				document.getElementById(&amp;quot;message&amp;quot;).innerHTML=  store_data;
			}).catch(function(error){
			console.error(error);
		});

	}
			
	var myForm = document.getElementById(&amp;quot;myForm&amp;quot;);

		console.log(username, password, myForm);
	myForm.addEventListener(&#39;submit&#39;, fetchcall);
	
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make this work in front-end, we need to register the file to &lt;code&gt;Accountsapp/views.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def register_user(request):
	# if request.user.is_authenticated:
	return render(request, &amp;quot;Accountsapp/RegisterUser.html&amp;quot;, {&#39;Title&#39;: &amp;quot;Register new user&amp;quot;})

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
