<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Aman Bagrecha</title>
    <link>https://amanbagrecha.github.io/post/</link>
      <atom:link href="https://amanbagrecha.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 05 Jul 2023 12:04:01 +0000</lastBuildDate>
    <image>
      <url>https://amanbagrecha.github.io/media/icon_hub8f2c4a40c112e5edee4297ae1d00aa4_198578_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://amanbagrecha.github.io/post/</link>
    </image>
    
    <item>
      <title>My first FOSS4G experience</title>
      <link>https://amanbagrecha.github.io/post/event/my-first-foss4g/</link>
      <pubDate>Wed, 05 Jul 2023 12:04:01 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/event/my-first-foss4g/</guid>
      <description>&lt;p&gt;I embarked on a journey to attend the esteemed geospatial conference, FOSS4G 2023, held in Prizren. As I reflect, it was an event that left a profound impact on me, and I want to treasure these memories forever.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/amanbagrecha/amanbagrecha.github.io/assets/76432265/6361e093-e11b-4c4c-b7dc-e0b293f515b0&#34; alt=&#34;WhatsApp Image 2023-07-06 at 01 39 40&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Day1: Entering the Exhibition center at ITP, Prizren&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Just a couple of months ago, I had my doubts about attending FOSS4G, as I had never traveled to a foreign country before and the host country was not very well known. The logistics to get visa and planning the round-trip seemed daunting, and I wasn&amp;rsquo;t sure if it would be worth the investment and energy. However, to my delight, I discovered that there were other indians planning their travel too, making the planning process much smoother.&lt;/p&gt;
&lt;p&gt;So, there I was, hopping on the plane to Pristina, Kosovo, and thinking to myself, &amp;ldquo;Did I really make the right call by shelling out a ton of money for this conference?&amp;rdquo; But, looking back now, I have to say, it was totally worth it! All the energy and effort I put into being a part of this event, it paid off big time. I&amp;rsquo;m filled with a sense of contentment right now.&lt;/p&gt;
&lt;p&gt;During my time at foss4g, I got to hang out with the cool, nerdy core developers, authors, organizers, professors, and users of geospatial technology.  I made some awesome new friends and dove headfirst into the wild nightlife of that stunning city. The experience was overwhelming, and it demanded an extra surge of energy.&lt;/p&gt;
&lt;p&gt;With dense knowledge delivered within every conference room, I had to decide to let go one talk over another. Thankfully the talks are recorded, and will be shared freely and openly, adhering the spirit of foss4g!&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/amanbagrecha/amanbagrecha.github.io/assets/76432265/adf68f3c-277c-439c-9fe1-e551d2e4341e&#34; alt=&#34;WhatsApp Image 2023-07-06 at 00 21 58 (4)&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Day5: Presenting my work to a room full of geonerds&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Not only was this my first foss4g, but also the first time I presented my work. I was thrilled to receive positive feedback after my talk and to connect with individuals who shared my passion and interests. It was like finding my tribe!&lt;/p&gt;
&lt;p&gt;These ten days in Kosovo were one of the most remarkable days of my life. The local people I met, both from Kosovo and Albania, made me feel right at home and added to the incredible experience. I&amp;rsquo;m back home, with a fresh perspective on life, a bunch of goals to accomplish, and a trove of memories that I will cherish forever.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/amanbagrecha/store-info/assets/76432265/e391ae8c-fdae-4435-971b-a90d585143c7&#34; alt=&#34;WhatsApp Image 2023-07-06 at 16 54 10&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Random day: City tour with friends&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Little did I know, a &amp;ldquo;mere&amp;rdquo; conference could dive deep into technical know-how, forge bonds for a lifetime, question my views on status quo, understand cultural differences, and also make me feel blessed to be part of this community!&lt;/p&gt;
&lt;p&gt;I cannot fathom the sheer knowledge I gained during these 10 days! From learning about state of the art innovation in geospatial, to learning life lessons to think beyond technology! I hope to have the opportunity to attend again next year and reunite with the friends I made along the way.&lt;/p&gt;
&lt;p&gt;Thank you foss4g community, the people of Kosovo and to the organizers who made this extraordinary event possible!&lt;/p&gt;
&lt;p&gt;Fingers crossed, hope to meet again next year in Brazil!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PS: This post is a glimpse of what transpired during the conference! Highly encourage everybody to attend this event and experience it yourself!&lt;/em&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://github.com/amanbagrecha/amanbagrecha.github.io/assets/76432265/60ab5cd6-4697-4b5d-990a-791406d6ea65&#34; alt=&#34;WhatsApp Image 2023-07-06 at 01 03 35&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Everyday visit: Stunning landscape of Prizren city, Kosovo&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Jason-3 satellite: Data and its Applications</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/understanding-jason-3-satellite-data-and-its-applications/</link>
      <pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/understanding-jason-3-satellite-data-and-its-applications/</guid>
      <description>&lt;h3 id=&#34;what-is-jason-3&#34;&gt;What is Jason-3?&lt;/h3&gt;
&lt;p&gt;Jason-3 is a satellite which measures topographic height of the entire earth once every ~10 days since the year 2016 and is used in applications such as sea level rise, ocean circulation, and climate change.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It has an &lt;strong&gt;altimeter&lt;/strong&gt; which measures the two-way travel time from the Earth’s surface to satellite.&lt;/li&gt;
&lt;li&gt;It emits a pulse (radar pulse in this case) at a certain frequency to measure time. Thus it can also penetrate clouds.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;450&#34; frameborder=&#34;0&#34; title=&#34;Felt Map&#34; src=&#34;https://felt.com/embed/map/Jason-3-pass-vgfiMu1mRpaWz6BZqg9Cx4D?lat=-6.478218&amp;lon=16.823646&amp;zoom=6.366&#34;&gt;&lt;/iframe&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 - Jason-3 ground track visualised on felt. Each point represents a measurement. The distance between two measurements depends on specific product type. &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;The data format of this satellite is Vector Points, distributed as netCDF4 file format.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;understanding-jason-3-family-of-products&#34;&gt;Understanding Jason-3 Family of products&lt;/h3&gt;
&lt;p&gt;Jason-3 is processed to level-2 from telemetry data (level-0) and is available online for users to download via NOAA, EUMETSAT and CNES. &lt;strong&gt;Jason-3 level-2 product&lt;/strong&gt; has 3 family of products depending on their &lt;strong&gt;latency&lt;/strong&gt;. The near real time data with latency of 3-5 hours is categorised under Operational Geophysical Data Record (&lt;strong&gt;OGDR&lt;/strong&gt;) product family, with latency of 1-2 days under Interim geophysical data record (&lt;strong&gt;IGDR&lt;/strong&gt;) product family, and with latency of 60 days under geophysical data record (&lt;strong&gt;GDR&lt;/strong&gt;) product family.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Higher the latency, more accurate the measurements since GDR products, unlike OGDR and IGDR products, are fully validated and calibrated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Under each family, they are categorised with &lt;strong&gt;reduced&lt;/strong&gt;, &lt;strong&gt;native&lt;/strong&gt; and &lt;strong&gt;sensor&lt;/strong&gt; product. The difference between them is in the amount and type of data included, as described in the figure below.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/216826096-35712afa-a141-4bb5-8dcd-1f2f8eed1365.png&#34; alt=&#34;Untitled-2023-01-27-23021&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 - Describing Jason-3 Product Family and its childs. Reduced product only contains data with 1Hz frequency. Native product contains data with 1Hz + 20Hz. Sensor contains 1Hz + 20Hz + waveforms. GDR product is fully validated and has highest accuracy. &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;which-jason-3-data-product-should-i-use&#34;&gt;Which Jason-3 data product should I use?&lt;/h3&gt;
&lt;p&gt;The answer depends on 3 key factors: 1. Latency, spatial resolution, spectral resolution.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve already mentioned the latency difference between three parent products (see above section). Depending on user requirements and accuracy, one of the three families can be selected.&lt;/p&gt;
&lt;p&gt;Let us now look at two other factors which will help you decide which data product to choose.&lt;/p&gt;
&lt;p&gt;Spatial resolution here is the distance between two measurements, while spectral resolution is the richness of data. &lt;strong&gt;sensor product&lt;/strong&gt; contains information about the photon signals (waveforms) and might not be useful for certain applications, while &lt;strong&gt;reduced product&lt;/strong&gt; has sparsely spaced measurements, i.e, number of total measurements are less and it does not contain waveform information.&lt;/p&gt;
&lt;p&gt;To illustrate which product to choose among the 8 products from 3 family, let us look at few real world use cases to help with our selection  -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If I aim to create a DEM for a large scale area which has relatively relatively terrain surface and do not require near real time data, I&amp;rsquo;d prefer using &lt;strong&gt;reduced product&lt;/strong&gt; from &lt;em&gt;GDR family&lt;/em&gt;. While if the surface is undulating, I&amp;rsquo;d prefer &lt;strong&gt;native product&lt;/strong&gt;, which has measurements at better spatial resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If I am working on a climate variable, say, looking at atmospheric correction which requires raw photon signals as well, I&amp;rsquo;d use &lt;strong&gt;sensor product&lt;/strong&gt; from &lt;em&gt;IGDR family&lt;/em&gt; or &lt;em&gt;GDR family&lt;/em&gt; depending on requirement.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If I am working on mission critical problem, and require near real time data with good spatial resolution, but do not need waveform data, I&amp;rsquo;d use &lt;strong&gt;native product&lt;/strong&gt; from &lt;em&gt;OGDR family&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Earthdata hosts a product called &lt;strong&gt;GPS OGDR SSHA&lt;/strong&gt; which is delivered near real time (8-10 hours) as &lt;strong&gt;reduced product&lt;/strong&gt; (1Hz only). This product is more accurate than &lt;strong&gt;OGDR SSHA&lt;/strong&gt; from the OGDR family described above due to it being processed against GPS orbit rather than DORIS orbit ( which is used for all other products described in figure 2). Though &lt;strong&gt;GPS OGDR SSHA&lt;/strong&gt; product is only available from 2020 october onwards.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that we know which product to download, let us look how and where to download them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Manual Download&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://search.earthdata.nasa.gov/search/granules?p=C2205122298-POCLOUD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Earthdata Search JASON3 GPS OGDR&lt;/a&gt;: It only allows you to visualise the ground track of the product you are going to download.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ncei.noaa.gov/data/oceans/jason3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NCEI NOAA JASON3&lt;/a&gt;: It has a GUI to check out the available products for each family, cycle and pass. Each family is suffixed with the version number.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cmr.earthdata.nasa.gov/virtual-directory/collections/C2205122298-POCLOUD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMR API virtual Directory JASON3 GPS OGDR&lt;/a&gt;: It is same as NCEI NOAA, but only 1 product is available through CMR API virtual directory&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Programmatic download Links&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PODAAC S3 access&lt;/a&gt;: Direct S3 access to only &lt;em&gt;GPS OGDR SSHA&lt;/em&gt; &lt;strong&gt;reduced product&lt;/strong&gt;. To know about bucket information, see &lt;a href=&#34;https://podaac.jpl.nasa.gov/dataset/JASON_3_L2_OST_OGDR_GPS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/granules.json?collection_concept_id=C2205122298-POCLOUD&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMR API JASON3 GPS OGDR&lt;/a&gt;: CMI API access to only &lt;em&gt;GPS OGDR SSHA&lt;/em&gt; &lt;strong&gt;reduced product&lt;/strong&gt;. More details on how to use this API is described below.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;ftp://ftp-oceans.ncei.noaa.gov&#34;&gt;FTP NCEI NOAA JASON3&lt;/a&gt;: This FTP server hosts all the data described in figure 2. More information on how to download it is given below.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Data hosted by NCEI NOAA has all the data available for download, but NASA earthdata only hosts &lt;em&gt;GPS OGDR SSHA&lt;/em&gt; &lt;strong&gt;reduced product&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;download-from-cmr-api&#34;&gt;Download from CMR API&lt;/h3&gt;
&lt;p&gt;The code for downloading jason-3 data using CMR API can be found &lt;a href=&#34;https://github.com/amanbagrecha/jason3/blob/main/jason3_script.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
Broadly, The code sends a request to CMR Search API &lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/granules.json&#34;&gt;https://cmr.earthdata.nasa.gov/search/granules.json&lt;/a&gt; along with parameters to filter by date, region and number of products required. The result is passed to authenticate with earthdata credentials and the file is downloaded to the local machine.&lt;/p&gt;
&lt;h3 id=&#34;download-from-ftp-server&#34;&gt;Download from FTP server&lt;/h3&gt;
&lt;p&gt;The entire code for downloading jason-3 data using FTP Server can be found &lt;a href=&#34;https://github.com/amanbagrecha/jason3/blob/main/jason3_ftp_script.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.
In this code, the FTP directory is fetched for a specific product. To know which parent family folder to choose, we need to understand family versions for the Jason-3 product.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/216826675-886e72bd-bb71-4727-ac76-86d7accd830d.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 - Jason-3 products grouped according to model version as seen in NOAA server directory at ncei.noaa.gov/data/oceans/jason3/. &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The Jason-3 family product is versioned based on whether the product is in calibration/validation phase or intended to be used by the end user. If &lt;strong&gt;&lt;code&gt;T&lt;/code&gt;&lt;/strong&gt; is suffixed with the name of the parent folder, it means cal/val phase otherwise it is for end user.
Product families with no suffix, combines all the versioned family product, keeping the latest for each cycle. It has data for all cycles till the latest available date.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since September 2018, all data products associated with &lt;strong&gt;&lt;code&gt;gdr&lt;/code&gt;&lt;/strong&gt; family have been moved to version &lt;strong&gt;&lt;code&gt;f&lt;/code&gt;&lt;/strong&gt;. If you require historcal jason3 data, it is a no-brainer to use GDR family products since it is at the most accurate among all the families and also has been updated with the latest model version &lt;strong&gt;&lt;code&gt;f&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ospo.noaa.gov/Products/documents/hdbk_j3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jason-3 Product Handbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://podaac.jpl.nasa.gov/dataset/JASON_3_L2_OST_OGDR_GPS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PODAAC JPL NASA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Native Composite, Subset and Processing of Satellite Imagery with STAC and Stackstac</title>
      <link>https://amanbagrecha.github.io/post/xarray/cloud-native-composite-subset-and-processing-of-satellite-imagery-with-stac-and-stackstac/</link>
      <pubDate>Sat, 14 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/xarray/cloud-native-composite-subset-and-processing-of-satellite-imagery-with-stac-and-stackstac/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;If you wanted to collect all Sentinel satellite data for a given region of interest (ROI), say, for a given day or time frame - is there any simple way to do it? That means: Without having to download all the full images manually and cropping the ROI subset manually as well afterwards?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This, well articulated question, was the one which I was facing and made me ponder to think if we could do this using STAC and Python.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I had a road network layer over which I needed satellite imagery. The problem with my road network is that it has a large spatial extent, causing a single satellite imagery to not cover it entirely. Moreover, because of this large extent, I need two adjacent tiles to be in the same Coordinate Reference System.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/FHLQbBo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 - Road network (in red) spanning multiple UTM Zones. Basemap from OSM.&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;What I needed was,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A way to aggregate all the adjacent tiles for a single day&lt;/li&gt;
&lt;li&gt;Convert to a single CRS on the fly&lt;/li&gt;
&lt;li&gt;Subset the data to my region&lt;/li&gt;
&lt;li&gt;Create a composite (merge) and perform analysis on the fly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It turns out Python (and its ecosystem of great geospatial packages) along with STAC allows us to do just that.&lt;/p&gt;
&lt;p&gt;What is STAC?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;STAC (SpatioTemporal Asset Catalog) is an open-source specification for describing satellite imagery and the associated metadata.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will use &lt;code&gt;stackstac&lt;/code&gt;, which is a Python package for efficiently managing and analysing large amounts of satellite imagery data in a cloud computing environment.&lt;/p&gt;
&lt;p&gt;First, we search through the sentinel-2 collection for our area of interest from element84 provided STAC endpoint.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from pystac_client import Client

URL = &#39;https://earth-search.aws.element84.com/v0/&#39;

client = Client.open(URL)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;search = client.search(
    max_items = 10,
    collections = &amp;quot;sentinel-s2-l2a-cogs&amp;quot;,
    intersects = aoi_as_multiline,
    datetime = &#39;2022-01-01/2022-01-24&#39;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resultant &lt;code&gt;search&lt;/code&gt; object is passed to &lt;code&gt;stack&lt;/code&gt; method on &lt;code&gt;stackstac&lt;/code&gt; along with providing the destination CRS, the region of bounds and the assets required.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import stackstac

ds = stackstac.stack(search.get_all_items() ,  epsg=4326, assets=[&amp;quot;B04&amp;quot;, &amp;quot;B03&amp;quot;, &amp;quot;B05&amp;quot;],
bounds_latlon= aoi_as_multiline.bounds )

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above line does a lot of things under the hood. It transforms the CRS of each tile from their native CRS to EPSG:4326. It also clips the tiles to our AOI. It also filters only 3 bands out of the possible 15 sentinel-2 bands. 
The output &lt;code&gt;ds&lt;/code&gt; is a &lt;code&gt;xarray.DataArray&lt;/code&gt; object and it is a known fact how much is possible with very little code in xarray.&lt;/p&gt;
&lt;p&gt;As such, we can group by a date and mosaic those tiles very easily using xarray as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dsf = ds.groupby(&amp;quot;time.date&amp;quot;).median()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/BLCu4xs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 - Our DataArray is 3.37GB with 4 dimensions (time, bands, x, y) respectively.&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Since xarray loads lazily, we did not perform any computation so far. But we can see how much data we are going to end up storing as shown in Figure 2.&lt;/p&gt;
&lt;p&gt;When I run the &lt;code&gt;compute&lt;/code&gt; method on the output, it does the computation in 4 minutes (here) i.e, processing ~3.5GB in 4 mins and computing the median across the dates.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;res = dsf.compute()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of this process, I have 4 images for each of the 4 dates, clipped to my region of interest in the CRS that I desire.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The above method of processing large volume data is super handy and can be scaled very easily with cloud infrastructure. What is unique about this approach is that I did not have to download data, convert or know the CRS of each tile, worrying about the bounds of my region of interest.
Read more about how stackstac works &lt;a href=&#34;https://stackstac.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code can be found &lt;a href=&#34;https://colab.research.google.com/drive/1NcwW7S58PkZFnrGaCyOcA5uLTxymdbZl?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How hard can it be to create 30 Maps?</title>
      <link>https://amanbagrecha.github.io/post/qgis/how-hard-can-it-be-to-create-30-maps/</link>
      <pubDate>Wed, 21 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/qgis/how-hard-can-it-be-to-create-30-maps/</guid>
      <description>&lt;p&gt;I participated in the &lt;a href=&#34;https://twitter.com/search?q=%2330DayMapChallenge&amp;amp;src=typeahead_click&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#30DayMapChallenge&lt;/a&gt; for the first time. I found the experience to be both challenging and rewarding wherein I was able to create a variety of maps using different techniques and tools. I got to use felt, blender, kepler, python and QGIS for my maps.&lt;/p&gt;
&lt;p&gt;Though I did not complete all of them, I realised how vital and hard it is to effectively communicate information through maps and how to use maps to make informed decisions. Hope to complete all 30 next year. Compiled all my maps for the year 2022 below -&lt;/p&gt;
&lt;p&gt;Read more about the challenge &lt;a href=&#34;https://30daymapchallenge.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Day1: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1587517568197480448&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Points&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208978036-69dbbc2c-0edd-4933-bf59-d2cfba3fa7a1.png&#34; alt=&#34;day1-points&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day2: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1587837563150028800&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208978064-800324d6-b53f-4769-870f-1f3069e8e808.png&#34; alt=&#34;day2-lines&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day3: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1588268637038661633&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Polygons&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979145-f943c337-1306-4720-8b95-22ccd8f7917d.png&#34; alt=&#34;day3-polygon&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day4: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1588610806077599744&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colour Friday: Green&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979100-a31db1ca-47be-46ee-a2e0-35d5b9be07da.jpg&#34; alt=&#34;day4-greenday&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day5: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1588961594414927873&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ukraine&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979052-05237f71-549f-4279-a733-2347af076160.jpeg&#34; alt=&#34;day5-ukraine&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day6:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1589194185642762240&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Network&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979247-34f780d5-7e39-4650-8ccf-9026f85eb458.jpeg&#34; alt=&#34;day6-network&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day7:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1589677794841489408&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raster&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979297-933d9403-053e-492a-9e6c-cedd16adbbe0.jpeg&#34; alt=&#34;day7-raster&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day8:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1590047409337692161&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data: OpenStreetMap&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979342-94f48a98-a7bb-4ef2-ac19-e6e660b79f8a.jpeg&#34; alt=&#34;day8-osm&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day10:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1590766714782126080&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A bad map&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979386-62a6ac59-4c9a-4cbe-8eea-adfdebe7f207.jpeg&#34; alt=&#34;day10-badmap&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day11:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1591145003061239809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colour Friday: Red&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979429-f2ddf0cd-2f9b-4f78-8e38-75143b487081.jpeg&#34; alt=&#34;day11-red&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day12:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1591503001671602177&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scale&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208979708-2ef22e06-76ac-4a3f-a603-c7901ebd2186.jpg&#34; alt=&#34;day12-scale&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day13:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1591847798609436672&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;5 minute map&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/119618422/208980231-2689d22d-3e83-42af-9f7d-4b9ed49e8b6e.png&#34; alt=&#34;image&#34;&gt;
Day14:	&lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1592231783864860672&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hexagons&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/119618422/208980692-37a90bf0-c82e-45a6-b765-f248a6af40fb.jpeg&#34; alt=&#34;day14-hex&#34;&gt;
Day16: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1592953349523271680&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimal&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208980908-7eb66d2f-6f01-4681-880a-99a31d56051d.png&#34; alt=&#34;day16-ss&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day21: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1594746083330379776&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data: Kontur Population Dataset&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208980823-154fcbfb-e02d-42e6-a466-977f26a4142c.jpeg&#34; alt=&#34;day21-image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day23: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1595495715983089664&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Movement&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208981041-f0d338e3-aa31-4a45-9f72-646e24950eae.gif&#34; alt=&#34;day23-gif&#34;&gt;&lt;/p&gt;
&lt;p&gt;Day25: &lt;a href=&#34;https://twitter.com/aman_bagrecha/status/1596213863266996225&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Two Colors&lt;/a&gt;
&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/208981134-c2fd8ed9-9769-4612-8db7-167d07b61697.jpeg&#34; alt=&#34;day25-map&#34;&gt;&lt;/p&gt;
&lt;p&gt;The data I used for this challenge is archived on &lt;a href=&#34;https://github.com/amanbagrecha/30DayMapChallenge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Import CSV and OSM data into PostgreSQL using ogr2ogr</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/import-csv-and-osm-data-into-postgresql-using-ogr2ogr/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/import-csv-and-osm-data-into-postgresql-using-ogr2ogr/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://gdal.org/programs/ogr2ogr.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ogr2ogr&lt;/a&gt; is the swiss knife for vector geometry conversion. You can import CSV with latitude and longitude columns as Point geometry into PostgreSQL. This tool also makes it easy to import OSM data to be imported into PostgreSQL with a lot of flexibility.&lt;/p&gt;
&lt;h2 id=&#34;1-insert-csv-to-postgresql&#34;&gt;1. Insert CSV to PostgreSQL&lt;/h2&gt;
&lt;p&gt;Our CSV contains information about retail food stores including cafes, restaurants, grocery information with the location and name. Download the data &lt;a href=&#34;https://github.com/amanbagrecha/amanbagrecha.github.io/files/9592312/filter_all_cat_data.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/190896961-cc985bf1-0b5b-4665-b53a-bcb9eadf50eb.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;We first read the metadata of the CSV using ogrinfo&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ogrinfo -so filter_all_cat_data.csv filter_all_cat_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assuming you have a database already (&lt;code&gt;postgres&lt;/code&gt; here), we run the following command to create &lt;code&gt;postgis&lt;/code&gt; extension for &lt;code&gt;postgres&lt;/code&gt; database. The connection string is of the format as described &lt;a href=&#34;https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;psql -c &amp;quot;create extension postgis;&amp;quot; &amp;quot;postgresql://postgres:1234@localhost:5432/postgres&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we insert the CSV into PostgreSQL table named &lt;strong&gt;cat_data_copy&lt;/strong&gt; and assign CRS of EPSG:4326.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ogr2ogr -f PostgreSQL PG:&amp;quot;host=localhost user=postgres dbname=postgres password=1234&amp;quot; filter_all_cat_data.csv -oo X_POSSIBLE_NAMES=long_url -oo Y_POSSIBLE_NAMES=lat_url -nlt POINT -nln &amp;quot;cat_data_copy&amp;quot; -sql &amp;quot;select name,city,lat_url,long_url,type from filter_all_cat_data&amp;quot; -a_srs &amp;quot;EPSG:4326”
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following explains few of the flags&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-oo&lt;/code&gt;: &lt;code&gt;X_POSSIBLE_NAMES&lt;/code&gt; and &lt;code&gt;Y_POSSIBLE_NAMES&lt;/code&gt; allows us to specify geometry columns from CSV&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-nlt&lt;/code&gt;: Define the geometry type for the table&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-nln&lt;/code&gt;: alternate Table name (defaults to name of the file)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;-sql&lt;/code&gt;: write SQL to insert only selected columns into the table&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;2-insert-osm-data-to-postgresql&#34;&gt;2. Insert OSM data to PostgreSQL&lt;/h2&gt;
&lt;p&gt;Our OSM data is of Bahamas downloaded from geofabrik. You can download it from &lt;a href=&#34;https://download.geofabrik.de/central-america/bahamas-latest.osm.pbf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We first read the metadata of the OSM data using ogrinfo&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ogrinfo -so bahamas-latest.osm.pbf multipolygons
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We find about the geometry column, CRS and columns in the data. This will be used when inserting the data into the database.&lt;/p&gt;
&lt;p&gt;Next we create postgis and hstore extensions in our &lt;code&gt;postgres&lt;/code&gt; database.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;psql -c &amp;quot;create extension hstore; create extension postgis&amp;quot; &amp;quot;postgresql://postgres:1234@localhost:5432/postgres&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we insert the data into PostgreSQL with table name as &lt;strong&gt;bahamas_mpoly&lt;/strong&gt; with only multipolygons. We convert the &lt;code&gt;other_tags&lt;/code&gt; column into &lt;code&gt;hstore&lt;/code&gt; and insert only those rows where the &lt;code&gt;name&lt;/code&gt; column does not contain a null value. We also clip our data to a bounding box and promote our polygons to multipolygons to avoid error.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ogr2ogr -f PostgreSQL PG:&amp;quot;dbname=postgres host=localhost port=5432 user=postgres password=1234&amp;quot; bahamas-latest.osm.pbf multipolygons -nln bahamas_mpoly -lco COLUMN_TYPES=other_tags=hstore -overwrite -skipfailures -where &amp;quot;name is not null&amp;quot; -clipsrc -78 23 -73 27 -nlt PROMOTE_TO_MULTI
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h4 id=&#34;video-version-of-the-blog-can-be-found-herehttpsyoutube87lilpasypi&#34;&gt;Video version of the blog can be found &lt;a href=&#34;https://youtu.be/87liLpASYPI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Merging Rasters using Rasterio</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/merge-rasters-the-modern-way-using-python/</link>
      <pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/merge-rasters-the-modern-way-using-python/</guid>
      <description>&lt;p&gt;In this blog, we&amp;rsquo;ll examine how to merge or mosaic rasters using Python, the modern way. Additionally, we would look at a few nuances and internal workings of rasterio&amp;rsquo;s merge functionality along with saving your rasters in-memory.&lt;/p&gt;
&lt;p&gt;By &amp;ldquo;modern way&amp;rdquo;, it is implied that you have an improved workflow and data management. And that you can experiment with various scenarios quickly and efficiently.&lt;/p&gt;
&lt;p&gt;The traditional way to mosaic data is by downloading multiple intersecting tileset in its entirety. Downloading an entire tileset is itself a cost prohibitive task, added to already lost time in searching desired satellite imagery on GUI.&lt;/p&gt;
&lt;p&gt;To overcome these traditional challenges, there has been significant improvement in storing metadata of satellite imagery (namely &lt;a href=&#34;https://stacspec.org/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STAC&lt;/a&gt;) which has enabled querying them much smoother and made it imagery-provider agnostic.&lt;/p&gt;
&lt;h3 id=&#34;tldr&#34;&gt;TL;DR&lt;/h3&gt;
&lt;p&gt;We would perform the following task in this blog —&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use pystac to query items over our AOI&lt;/li&gt;
&lt;li&gt;Plot the tiles on map using hvplot&lt;/li&gt;
&lt;li&gt;Merge tiles without data download on local machine&lt;/li&gt;
&lt;li&gt;Save the merged tile in-memory using rasterio&amp;rsquo;s MemoryFile&lt;/li&gt;
&lt;li&gt;Internals of rasterio&amp;rsquo;s merge methods&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;problem-at-hand&#34;&gt;Problem at hand&lt;/h3&gt;
&lt;p&gt;I wish to access sentinel-2 True Color Image for the month of January over my area of interest (AOI), which is a highway network across Karnataka and Andhra Pradesh (Figure 1).&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183255775-352d47fb-515c-4d72-ba4e-e32ac5bebf42.png&#34; alt=&#34;bokeh_plot&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -
Highway Network as our Region of Interest&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We start by fetching sentinel-2 tiles over our AOI from &lt;code&gt;sentinel-s2-l2a-cogs&lt;/code&gt; STAC catalog using &lt;a href=&#34;https://github.com/stac-utils/pystac-client&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pystac-client&lt;/a&gt;. This library allows us to crawl STAC catalog and enables rapid access to the metadata we need.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# STAC API root URL
# Thanks to element84 for hosting the API for sentinel-2 catalog.
URL = &#39;https://earth-search.aws.element84.com/v0/&#39;

client = Client.open(URL)

search = client.search(
    max_items = 10,
    collections = &amp;quot;sentinel-s2-l2a-cogs&amp;quot;,
    bbox = gdf.total_bounds, # geodataframe for our region of study
    datetime = &#39;2022-01-01/2022-01-24&#39;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we search for 10 sentinel-s2-l2a-cogs over our AOI for the date between January 1 and 24 of 2022.&lt;/p&gt;
&lt;p&gt;Now we need to know which of our 10 queried images covers our area of interest in its entirety. To do that, we can plot all the search results on the map and visually inspect.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183255939-121e585e-79dc-4ceb-b61e-7e08709de926.png&#34; alt=&#34;bokeh_plot&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -
Sentinel-2 tiles overlaid on Region of Interest&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We see that our AOI is not covered by a single tile in entirety, and that there is a need to merge adjacent tiles.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that we have so far only queried the metadata of our desired imagery&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We use rasterio&amp;rsquo;s &lt;a href=&#34;https://rasterio.readthedocs.io/en/latest/api/rasterio.merge.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;merge&lt;/a&gt; functionality, which would enable us to combine all of them seamlessly.&lt;/p&gt;
&lt;p&gt;First, we get all the tiles for a single day and look for True Color Image (TCI) band&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# retrieve the items as dictionaries, rather than Item objects
items = list(search.items_as_dicts())
# convert found items to a GeoDataFrame
items_gdf = items_to_geodataframe(items)

tiles_single_day = items_gdf.loc[&#39;2022-01-23&#39;, &amp;quot;assets.visual.href&amp;quot;]

# print(tiles_single_day)
 properties.datetime
 2022-01-23 05:25:14+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 2022-01-23 05:25:11+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 2022-01-23 05:24:59+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 2022-01-23 05:24:56+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 Name: assets.visual.href, dtype: object

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, read the remote files via the URL in the above output using &lt;code&gt;rasterio.open&lt;/code&gt; and save the returned file handlers as a list. This is the first instance where we are dealing with the actual imagery. Although, we are not reading the values stored in the data just yet.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# open images stored on s3
file_handler = [rasterio.open(row) for row in tiles_single_day]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we can merge all of the tiles and get the clipped raster stored in memory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from rasterio.io import MemoryFile
from rasterio.merge import merge

memfile = MemoryFile()

merge(datasets=file_handler, # list of dataset objects opened in &#39;r&#39; mode
    bounds=tuple(gdf.set_crs(&amp;quot;EPSG:4326&amp;quot;).to_crs(file_handler[0].crs).total_bounds), # tuple
    nodata=None, # float
    dtype=&#39;uint16&#39;, # dtype
    resampling=Resampling.nearest,
    method=&#39;first&#39;, # strategy to combine overlapping rasters
    dst_path=memfile.name, # str or PathLike to save raster
    dst_kwds={&#39;blockysize&#39;:512, &#39;blockxsize&#39;:512} # Dictionary
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are really interesting things to look at in the above code. Overall, the code above returns a &lt;code&gt;MemoryFile&lt;/code&gt; object which contains a &lt;code&gt;uint16&lt;/code&gt; raster with bounds of our AOI and blocksize of 512. The attribute &lt;code&gt;dst_path&lt;/code&gt; allows us to specify a path to save the output as a raster. What is interesting is we can not only pass a file path to save on local disk but also a virtual path and save the merged raster &lt;strong&gt;in-memory&lt;/strong&gt;, avoiding clutter of additional files on disk.&lt;/p&gt;
&lt;p&gt;To define a virtual path, we use rasterio&amp;rsquo;s &lt;code&gt;MemoryFile&lt;/code&gt; class. When we create a &lt;code&gt;MemoryFile&lt;/code&gt; object, it has a &lt;code&gt;name&lt;/code&gt; attribute which gives us a virtual path, thus treating it as a real file (using GDALs &lt;a href=&#34;https://gdal.org/user/virtual_file_systems.html#vsimem-in-memory-files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vsimem&lt;/a&gt; internally). This MemoryFile object (&lt;code&gt;memfile&lt;/code&gt; here) provides us all the methods and attributes of rasterio&amp;rsquo;s file handler, which is extremely helpful.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;print(memfile.open().profile)

{&#39;driver&#39;: &#39;GTiff&#39;, &#39;dtype&#39;: &#39;uint16&#39;, &#39;nodata&#39;: 0.0, &#39;width&#39;: 4110, &#39;height&#39;: 3211, &#39;count&#39;: 3, &#39;crs&#39;: CRS.from_epsg(32643), &#39;transform&#39;: Affine(10.0, 0.0, 788693.4700669964,
       0.0, -10.0, 1500674.3670768766), &#39;blockxsize&#39;: 512, &#39;blockysize&#39;: 512, &#39;tiled&#39;: True, &#39;compress&#39;: &#39;deflate&#39;, &#39;interleave&#39;: &#39;pixel&#39;}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;method=&#39;first&#39;&lt;/code&gt; tells us the strategy used to determine the value of the pixel where the rasters overlap. In this case, the pixel value from the first imagery of the overlapping region in the list, is used as the value for the output raster.&lt;/p&gt;
&lt;p&gt;The entire algorithm to merge rasters is illustrated in the figure below by taking an example of combining two rasters with &lt;code&gt;method=first&lt;/code&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183279200-05b96cd5-f0a7-48e0-9b37-480792756d16.jpg&#34; alt=&#34;merge-rasterio-with-laberl_merging-rasters&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -
Internal working of rasterio&#39;s merge functionality. src1 and src2 are two overlapping raster.&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;From the above figure, for each raster in the list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;it finds the intersection with the &lt;strong&gt;Output Bounds&lt;/strong&gt; (named &lt;code&gt;region&lt;/code&gt; in the figure)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;next, it gets a boolean mask of invaild pixel over the &lt;code&gt;region&lt;/code&gt; (named &lt;code&gt;region_mask&lt;/code&gt; in the figure).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;next, it copies over all the existing values from the raster for the &lt;code&gt;region&lt;/code&gt; to an array (named &lt;code&gt;temp&lt;/code&gt; in the figure)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It gets a boolean mask for the valid pixels in the &lt;code&gt;temp&lt;/code&gt; array. (named &lt;code&gt;temp_mask&lt;/code&gt; in the figure)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With these four arrays, it runs the &lt;code&gt;method=first&lt;/code&gt;, which is to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create the same shaped array as that of &lt;code&gt;region&lt;/code&gt; and fill values with negation of &lt;code&gt;region_mask&lt;/code&gt; (named &lt;code&gt;A&lt;/code&gt; in the figure)&lt;/li&gt;
&lt;li&gt;create a filter by combining &lt;code&gt;region_mask&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt; with a AND gate (named &lt;code&gt;B&lt;/code&gt; in the figure)&lt;/li&gt;
&lt;li&gt;copy over the values from &lt;code&gt;temp&lt;/code&gt; to &lt;code&gt;region&lt;/code&gt; using &lt;code&gt;B&lt;/code&gt; as the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These series of steps are performed for all the rasters in the list. Finally, the output at the end of each iteration is combined to produce &lt;code&gt;dest&lt;/code&gt; raster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice the dark strip bands for each array which represents the overlapping region. Also notice that values from the dark strip in step &lt;strong&gt;&lt;code&gt;1&lt;/code&gt;&lt;/strong&gt; did not change at the end of step &lt;strong&gt;&lt;code&gt;2&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;custom-combining-strategy-for-overlapping-regions&#34;&gt;Custom combining strategy for overlapping regions&lt;/h3&gt;
&lt;p&gt;We can have arbitrary conditions on how to combine the overlapping region. By default rasterio uses values of the first overlapping raster from the list of Input files as pixel values for the output raster file. It has several other options in its utility such as &lt;code&gt;min&lt;/code&gt;, &lt;code&gt;max&lt;/code&gt;, &lt;code&gt;sum&lt;/code&gt;, &lt;code&gt;count&lt;/code&gt;, &lt;code&gt;last&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To define our custom method, say in this case, I want to take the average of all the pixel values over my overlapping region and copy them to the output file. To do that, we can override the method by defining our custom method. Let us see how —&lt;/p&gt;
&lt;p&gt;We take a look at the source code of built-in methods which make use of two or more rasters to make decisions on the output pixel values. Few such methods which do that are &lt;code&gt;copy_sum&lt;/code&gt;, &lt;code&gt;copy_min&lt;/code&gt;, &lt;code&gt;copy_max&lt;/code&gt;, &lt;code&gt;copy_count&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Looking at the &lt;a href=&#34;https://github.com/rasterio/rasterio/blob/main/rasterio/merge.py#L40&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;copy_min&lt;/a&gt; from source code, we see that it performs two logical operations each before and after the custom logic we wish to apply.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183279064-f2437364-b4bd-4761-8b99-2aa3bc65bc42.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -
copy_min function copies minimum value from overlapping region to the output raster&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We would replace our custom logic of averaging with that of &lt;code&gt;minimum&lt;/code&gt; in the above code and that is all there is to it. We can now use this function to manipulate the values of overlapping region!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def custom_method_avg(merged_data, new_data, merged_mask, new_mask, **kwargs):
    &amp;quot;&amp;quot;&amp;quot;Returns the average value pixel.&amp;quot;&amp;quot;&amp;quot;
    mask = np.empty_like(merged_mask, dtype=&amp;quot;bool&amp;quot;)
    np.logical_or(merged_mask, new_mask, out=mask)
    np.logical_not(mask, out=mask)
    np.nanmean([merged_data, new_data], axis=0, out=merged_data, where=mask)
    np.logical_not(new_mask, out=mask)
    np.logical_and(merged_mask, mask, out=mask)
    np.copyto(merged_data, new_data, where=mask, casting=&amp;quot;unsafe&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;endnote&#34;&gt;Endnote&lt;/h3&gt;
&lt;p&gt;The modern approach to merge rasters in python is to only stream the data for your region of interest, process and perform analysis on the raster in memory. This would save you a huge cost and time. This is possible because of &lt;a href=&#34;https://www.cogeo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COGs&lt;/a&gt; and &lt;a href=&#34;https://stacspec.org/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STAC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We looked at the merge method in depth and also explored the techniques used to combine the overlapping data. Finally, we created a custom method for merging rasters by modifying the existing code to suit our requirements. The code associated with this post can be found &lt;a href=&#34;https://colab.research.google.com/drive/1iMYdNmAEr0JuKzPnDH0qC4rDsYkvwMk0?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Download MODIS data using CMR API in Python</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/download-modis-data-using-cmr-api-in-python/</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/download-modis-data-using-cmr-api-in-python/</guid>
      <description>&lt;p&gt;If you have ever used USGS Earth Explorer to download / explore data, you’d notice that the manual process is cumbersome and not scalable. That is why we require a programmatic way to download satellite data.&lt;/p&gt;
&lt;p&gt;In this blog we’d see how to download MODIS data using Python. We use a Python package called &lt;a href=&#34;https://github.com/fraymio/modis-tools&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modis-tools&lt;/a&gt; to perform our task. This package internally uses &lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA CMR&lt;/a&gt; (Common Metadata Repository) API which lets us search and query catalogs of various satellite dataset including MODIS.&lt;/p&gt;
&lt;p&gt;We focus on the MODIS dataset in this blog, but with little modification, we could extend for various other datasets.&lt;/p&gt;
&lt;p&gt;Before you move ahead, make sure you have an earthdata account. We would require the username and password to download the data. Register &lt;a href=&#34;https://urs.earthdata.nasa.gov/users/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; if not done so.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;To download the data we ask ourselves the following questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Which dataset specifically do I need? — Define &lt;em&gt;Dataset Name&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;What area do I need the data for? — Define our &lt;em&gt;Region of Interest&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;What time period of data do I require? — Define &lt;em&gt;Start and End Date&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, I wish to download MODIS Surface Reflectance 8-Day L3 Global 250 m SIN Grid data for Nigeria from 29 December, 2019 to 31st December, 2019.&lt;/p&gt;
&lt;p&gt;Let us install and use the Python package &lt;code&gt;modis-tools&lt;/code&gt; to download the data on our local machine by performing the following steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a virtual environment.&lt;/li&gt;
&lt;li&gt;Install the &lt;code&gt;modis-tools&lt;/code&gt; package.&lt;/li&gt;
&lt;li&gt;Write the code.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;to-create-a-new-environment&#34;&gt;To create a new environment&lt;/h3&gt;
&lt;p&gt;Create virtual environment &lt;code&gt;.modis-tools&lt;/code&gt; using Python’s &lt;code&gt;venv&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aman@AMAN-JAIN:~$ python3 -m venv .modis-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Activate the environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aman@AMAN-JAIN:~$ source .modis-tools/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The above command is for linux. For Windows use .&lt;code&gt;modis-tools\Scripts\activate&lt;/code&gt; instead.&lt;/p&gt;
&lt;h3 id=&#34;install-the-modis-tools-package&#34;&gt;Install the modis-tools package&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;(.modis-tools) aman@AMAN-JAIN:~$ pip install modis-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;insert-the-below-code&#34;&gt;Insert the below code&lt;/h3&gt;
&lt;p&gt;Paste the code in a python file named &lt;code&gt;download.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# download_modis.py

# 1) connect to earthdata
session = ModisSession(username=username, password=password)

# 2) Query the MODIS catalog for collections
collection_client = CollectionApi(session=session)
collections = collection_client.query(short_name=&amp;quot;MOD09GQ&amp;quot;, version=&amp;quot;061&amp;quot;)
# Query the selected collection for granules
granule_client = GranuleApi.from_collection(collections[0], session=session)

# 3) Filter the selected granules via spatial and temporal parameters
nigeria_bbox = [2.1448863675, 3.002583177, 4.289420717, 4.275061098] # format [x_min, y_min, x_max, y_max]
nigeria_granules = granule_client.query(start_date=&amp;quot;2019-12-29&amp;quot;, end_date=&amp;quot;2019-12-31&amp;quot;, bounding_box=nigeria_bbox)

# 4) Download the granules
GranuleHandler.download_from_granules(nigeria_granules, session, threads=-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, change the &lt;code&gt;username&lt;/code&gt;, &lt;code&gt;password&lt;/code&gt;, &lt;code&gt;nigeria_box&lt;/code&gt; and &lt;code&gt;start_date&lt;/code&gt; &amp;amp; &lt;code&gt;end_date&lt;/code&gt; according to your requirements.&lt;/p&gt;
&lt;p&gt;To explain the above code —&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First we create a session, which makes a connection to earthdata and registers a session.
Next three lines we search for MODIS Surface Reflectance 8-Day L3 Global 250 m SIN Grid dataset using &lt;code&gt;short_name&lt;/code&gt; and &lt;code&gt;version&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now we filter the region spatially and temporally we want our data to be downloaded. In this example, we filter for the nigeria region with a bounding box (&lt;code&gt;bounding_box&lt;/code&gt;) and the two days of december of 2019 (&lt;code&gt;start_date&lt;/code&gt;, &lt;code&gt;end_date&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lastly, we download the data (granules) using multithreading, since we asked to use all threads. (&lt;code&gt;threads=-1&lt;/code&gt; is all threads).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-get-short_name-and-version-for-the-dataset&#34;&gt;How to get &lt;code&gt;short_name&lt;/code&gt; and &lt;code&gt;version&lt;/code&gt; for the dataset?&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/site/collections/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;collection endpoint&lt;/a&gt; of the CMR API contains a directory of all dataset catalogs hosted by various organizations with its short name and version number. For MODIS data, LPDAAC_ECS hosts and maintains it. Under the &lt;code&gt;/collections/directory&lt;/code&gt; endpoint, look for &lt;code&gt;LPDAAC_ECS&lt;/code&gt; and search for the MODIS dataset you want to download. Each dataset has a short name and version associated with it as shown in the picture below. In our case we found &lt;code&gt;MOD09Q1&lt;/code&gt; short name with version &lt;code&gt;061&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/x9aT290.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Now it is time to run the code to see our data being downloaded.&lt;/p&gt;
&lt;p&gt;In your terminal, run —&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(.modis-tools) aman@AMAN-JAIN:~$ python download_modis.py
Downloading: 100%|██████████████████████████████████████████████████████| 3/3 [00:10&amp;lt;00:00,  3.67s/file]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A progress bar would let you see the download progress and the files would be downloaded to your local disk. If you wish to download the data to a specific directory, use the path parameter in download_from_granules classmethod.&lt;/p&gt;
&lt;p&gt;Endnote
This short post on downloading MODIS data originated when I wanted to set up and deploy a pipeline. I did find other packages but they were quite old and did not use the state of the art specifications. Since the solution presented here uses CMR API, which has a very good documentation, I preferred it over other tools.&lt;/p&gt;
&lt;p&gt;You can find the video version of this blog &lt;a href=&#34;https://youtu.be/3K1yl79Mhow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;for-the-curious-advanced&#34;&gt;For the curious (Advanced)&lt;/h3&gt;
&lt;p&gt;The base url for the CMR API is —&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;https://cmr.earthdata.nasa.gov/search
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Internally, CMR API first finds the collection for our dataset —&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;https://cmr.earthdata.nasa.gov/search/collections.json?short_name=MOD09GQ&amp;amp;version=061
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that the package queries the granules endpoint to find individual granules matching our query parameters —&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;https://cmr.earthdata.nasa.gov/search/granules.json?downloadable=true&amp;amp;scroll=true&amp;amp;page_size=2000&amp;amp;sort_key=-start_date&amp;amp;concept_id=C1621091662-LPDAAC_ECS&amp;amp;temporal=2019-12-01T00%3A00%3A00Z%2C2019-12-31T00%3A00%3A00Z&amp;amp;bounding_box=2.1448863675%2C3.002583177%2C4.289420717%2C4.275061098
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that most parameters are autogenerated by the python package depending on the &lt;code&gt;short_name&lt;/code&gt; and &lt;code&gt;version&lt;/code&gt; you provide (downloadable, scroll, page_size, sort_key, concept_id). The other parameters are user defined (temporal, bounding_box)&lt;/p&gt;
&lt;p&gt;There are many more additional parameters which can be passed. A complete list is present in the &lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;. One such useful parameter that you can try out is &lt;code&gt;cloud_cover&lt;/code&gt;. All you need to do is pass this parameter name with value to the &lt;code&gt;query&lt;/code&gt; method in the above code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Download and preprocess NASA GPM IMERG Data using Python and wget</title>
      <link>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</guid>
      <description>&lt;p&gt;In this blog post we look into how to download precipitation data from NASA website and process it to get information using xarray and wget.&lt;/p&gt;
&lt;p&gt;We are going to work with &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPM IMERG Late Precipitation L3 Half Hourly 0.1 degree x 0.1 degree V06 (GPM_3IMERGHHL)&lt;/a&gt; data provided by NASA which gives half-hourly precipitation values for entire globe.&lt;/p&gt;
&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-requisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;You must have an Earthdata Account&lt;/li&gt;
&lt;li&gt;Link GES DISC with your account&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refer to &lt;a href=&#34;https://daac.gsfc.nasa.gov/earthdata-login&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; page on how to Link GES DISC to your account.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;First method&lt;/em&gt; - We would be downloading netCDF data using the &lt;code&gt;requests&lt;/code&gt; module and preprocessing the file using &lt;code&gt;xarray&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Second method&lt;/em&gt; - To download netCDF file using wget and using &lt;code&gt;xarray&lt;/code&gt; to preprocess and visualise the data.&lt;/p&gt;
&lt;h3 id=&#34;downloading-link-list&#34;&gt;Downloading link list&lt;/h3&gt;
&lt;p&gt;We first select the region for which we want to download the data by visiting the &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPM IMERG&lt;/a&gt; website and clicking on &lt;strong&gt;subset/ Get Data&lt;/strong&gt; link at right corner.
&lt;img src=&#34;https://i.imgur.com/3RH1ot2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the popup, select&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Download Method&lt;/strong&gt; as &lt;code&gt;Get File Subsets using OPeNDAP&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refine Date Range&lt;/strong&gt; as the date you want the data for. In my case, I choose 10 days of data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refine Region&lt;/strong&gt; to subset data for your area of interest. In my case I choose &lt;code&gt;77.45,12.85,77.75,13.10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Variables&lt;/strong&gt;, select &lt;code&gt;precipitationCal&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;file format&lt;/strong&gt;, we choose &lt;code&gt;netCDF&lt;/code&gt; and click the &lt;strong&gt;Get Data&lt;/strong&gt; button.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xCp8Shs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This will download a text file, containing all the links to download individual half hourly data for our area of interest in netCDF file format.&lt;/p&gt;
&lt;p&gt;Now we move to Google Colaboratory, to download the data in netCDF file format. We use Google Colaboratory as it has many libraries pre-loaded and saves the hassle to install them.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re area of interest (or) the timeframe of download is large, please use local machine as Google Colaboratory only offers ~60 GB of free storage.&lt;/p&gt;
&lt;h3 id=&#34;method-1-using-python-to-read-and-preprocess-the-data-inside-google-colaboratory&#34;&gt;Method 1: Using Python to read and preprocess the data inside Google Colaboratory.&lt;/h3&gt;
&lt;p&gt;Open a new google Colab notebook and upload the downloaded text file. Our uploaded text file looks like the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/njlFhPT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As one last requirement, NASA requires authentication to access the data and thus we have to create a &lt;code&gt;.netrc&lt;/code&gt; file and save it at specified location (under &lt;code&gt;/root&lt;/code&gt; dir in our case).&lt;/p&gt;
&lt;h3 id=&#34;creating-netrc-file&#34;&gt;Creating &lt;code&gt;.netrc&lt;/code&gt; file&lt;/h3&gt;
&lt;p&gt;Open your notepad and type in the following text. Make sure to replace &lt;code&gt;your_login_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; with your earthdata credentials. Now save it as &lt;code&gt;.netrc&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine urs.earthdata.nasa.gov login your_login_username password your_password
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upload the &lt;code&gt;.netrc&lt;/code&gt; file to Colab under &lt;code&gt;root&lt;/code&gt; directory as shown in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/oZLeJuY.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we have all the setup done and are ready to code.&lt;/p&gt;
&lt;p&gt;We first load the required libraries. Then, read the text file and loop over every line in it to download from the URL using the &lt;code&gt;requests&lt;/code&gt; module. Finally, we save the file to Colab&amp;rsquo;s hard drive. If you do not see the files after running code, make sure to wait for at least a day after registering to earthdata to make your account activated. I was late to read about it and had wasted a long time debugging it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
import xarray as xr
import requests 

# dataframe to read the text file which contains all the download links
ds = pd.read_csv(&#39;/content/subset_GPM_3IMERGHH_06_20210611_142330.txt&#39;, header = None, sep = &#39;\n&#39;)[0]

# Do not forget to add .netrc file in the root dir of Colab. printing `result` should return status code 200
for file in range(2, len(ds)): # skip first 2 rows as they contain metadata files
  URL = ds[file]
  result = requests.get(URL)
  try:
    result.raise_for_status()
    filename = &#39;test&#39; + str(file) + &#39;.nc&#39;
    with open(filename, &#39;wb&#39;) as f:
        f.write(result.content)

  except:
    print(&#39;requests.get() returned an error code &#39;+str(result.status_code))

xr_df = xr.open_mfdataset(&#39;test*.nc&#39;)

xr_df.mean(dim = [&#39;lat&#39;, &#39;lon&#39;]).to_dataframe().to_csv(&#39;results.csv&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above snippet, what is interesting is the method &lt;code&gt;open_mfdataset&lt;/code&gt; which takes in all the &lt;code&gt;netCDF&lt;/code&gt; files and gives us a nice, compact output from which we can subset and further process our data.
Here, we take the average of all the values (precipitation) and convert it into a new dataframe. We are ready to export it as CSV.&lt;/p&gt;
&lt;h3 id=&#34;method-2-using-wget-to-download-and-then-preprocess-using-xarray&#34;&gt;Method 2: Using wget to download and then preprocess using xarray&lt;/h3&gt;
&lt;p&gt;In this method, we download all the netCDF files using &lt;code&gt;wget&lt;/code&gt;. These files are then read using xarray which makes it really easy to process and get the information we require.&lt;/p&gt;
&lt;p&gt;Running the following shell command in Google Colab will download all the data from the text file URLs. Make sure to replace &lt;code&gt;your_user_name&lt;/code&gt; , &lt;code&gt;&amp;lt;url text file&amp;gt;&lt;/code&gt; within the command. It will ask for password of your earthdata account on running the cell.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;! wget --load-cookies /.urs_cookies --save-cookies /root/.urs_cookies --auth-no-challenge=on --user=your_user_name --ask-password --content-disposition -i &amp;lt;url text file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the above shell command is run on Colab, the following 2 lines of code will give a nice dataframe which can be exported to csv for further analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xarray as xr
import glob

ds = xr.open_mfdataset(&#39;test*.nc&#39;)
ds.precipitationCal.mean(dim=(&#39;lon&#39;, &#39;lat&#39;)).plot() # calculate the average precipitation on a half-hourly basis.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;final-comments&#34;&gt;Final Comments&lt;/h3&gt;
&lt;p&gt;In this post we looked into how to download and preprocess netCDF data provided by &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA GES DISC&lt;/a&gt;.
We looked at two methods, one with pure Python and the other with wget and xarray. All performed on google Colab.
It is to be noted that, there is a significant setup required i.e, to create a new &lt;code&gt;.netrc&lt;/code&gt; file and store inside the root directory of Colab else it returns an authorisation error. We looked at how easy it is to process netCDF data in xarray and how wget commands can be run on Colab.&lt;/p&gt;
&lt;p&gt;Watch the video tutorial &lt;a href=&#34;https://www.youtube.com/watch?v=T_Us4hJxSeI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The notebook for reference is located &lt;a href=&#34;https://Colab.research.google.com/drive/1VIKun8K3RT8VvcPJ7DE5uDDC10i10k1T?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>COGs as the Stand-in Replacement for GeoTIFFs</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/cogs-as-standin-replacement/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/cogs-as-standin-replacement/</guid>
      <description>&lt;p&gt;I decided to write this blog when my twitter feed was buzzing with the usefulness of Cloud Optimized GeoTIFF (COGs) and how it is a paradigm shift in the way we serve raster on any client application. I also look at potential gotchas when creating COGs and when it might end up &lt;strong&gt;not&lt;/strong&gt; being useful.&lt;/p&gt;
&lt;p&gt;So, this post will mostly focus on COGs and why I would use them over plain GeoTIFFs. Also, we would look at associated jargons when you want to create a COG. I aim to dump my thoughts once and for all and hopefully help others on the way.&lt;/p&gt;
&lt;h3 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;What is a block ?&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Understand what is a Block in raster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;What is an overview?&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;What are Overviews and how are they useful to us&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Overview levels&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;How does Block and Overview affect me&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Playing around with JP2000 format&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;COGs&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;COGs as Stand-in replacement&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;While looking at &amp;ldquo;How to generate a COG&amp;rdquo;, I encountered some fancy jargons — Block, Tile, Overview and Pyramid. Understanding them is essential to get the most out of COGs. Although I had known about them from when I started my GIS career, but you truly understand anything only when you apply it, don&amp;rsquo;t you?&lt;/p&gt;
&lt;p&gt;To comprehend the above terms, let us take an example and experiment with it. I have a sentinel-2 L2A band 03 downloaded from &lt;a href=&#34;https://scihub.copernicus.eu/dhus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scihub&lt;/a&gt;. The data is in JPEG2000 format (which is not a cloud optimized format).&lt;/p&gt;
&lt;p&gt;On performing &lt;code&gt;gdalinfo ./T43PHP_20210123T051121_B03_20m.jp2&lt;/code&gt;, the output gives a detailed description of the dataset. We focus on last few lines,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Band 1 Block=640x640 Type=UInt16, ColorInterp=Gray
Overviews: 2745x2745, 1373x1373, 687x687, 344x344
Overviews: arbitrary
Image Structure Metadata:
  COMPRESSION=JPEG2000
  NBITS=15
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;block&lt;/code&gt; parameter represents the shape of tile (width x height) in the raster image. For example a &lt;code&gt;Block=640x640&lt;/code&gt; represents a tile of width of 640 and height of 640 pixels. &lt;strong&gt;Fig. 1&lt;/strong&gt; illustrates a tile as displayed on a web application.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/0/03/Tiled_web_map_Stevage.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -
Example of Web Map Tiles. The squares represent a &lt;b&gt;block&lt;/b&gt;.&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;what-is-a-block-&#34;&gt;What is a &lt;code&gt;block&lt;/code&gt; ?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;A block corresponds to a rectangular subpart of the raster. The first value is the width of the block and the second value its height. Typical block shapes are lines or group of lines (in which case the block width is the raster width) or tiles (typically squares), such as here. Knowing the block size is important when efficient reading of a raster is needed. In the case of tiles, this means reading rasters from the left-most tile of the raster to the right-most of the upper lines and progressing that way downward to the bottom of the image. &lt;em&gt;&lt;a href=&#34;https://download.osgeo.org/gdal/workshop/foss4ge2015/workshop_gdal.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So one thing is clear, Block and Tile are synonyms.&lt;/p&gt;
&lt;p&gt;Also notice the output contains &lt;code&gt;Overviews: 2745x2745, 1372x1372, 686x686, 343x343&lt;/code&gt;. What are these numbers? Let&amp;rsquo;s find out!&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;what-is-an-overview&#34;&gt;What is an overview?&lt;/h3&gt;
&lt;p&gt;Overviews are reduced/downsampled versions of the raster. Overviews can also be termed interchangeably with &lt;strong&gt;pyramids&lt;/strong&gt; in GIS. When you want to pan, zoom around the raster, it helps to have overviews. The concept is to reduce the dimension of the raster to facilitate faster rendering of Raster on our application. Each overview is a half the size of its previous dimension. They can be built both externally and internally. Having an external overview would generate an &lt;code&gt;.ovr&lt;/code&gt; file which contains the information about the downsampled raster. While internal overviews alter the existing file permanently.&lt;/p&gt;
&lt;p&gt;The above numbers for overviews (&lt;code&gt;2745x...&lt;/code&gt;) tells us that the sentinel-2 image we are using here has internal overviews and tiling baked in it. Interestingly enough I found that it is not possible to build external overviews for &lt;code&gt;.jp2&lt;/code&gt; (JPEG2000) format.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/reoTn79.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;overview-levels&#34;&gt;Overview levels&lt;/h3&gt;
&lt;p&gt;The numbers we see for overviews are also termed as levels. Overviews can be built at many levels (typically from level 1-18). When you have a large raster image (drone shots etc), these levels facilitate faster rendering.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://2rct3i2488gxf9jvb1lqhek9-wpengine.netdna-ssl.com/wp-content/uploads/2018/07/TilePyramid.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -
Higher the pyramid level you move (zoom in), more detailed information from the raster you get, requiring more tiles to be generated. Image courtesy: &lt;href src = &#34;https://www.azavea.com/blog/2018/08/06/generating-pyramided-tiles-from-a-geotiff-using-geotrellis/tilepyramid/&#34; &gt;Azavea &lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In &lt;strong&gt;Fig.2&lt;/strong&gt; above, it should be clear that &lt;code&gt;tiling&lt;/code&gt; is splitting up the raster into multiple blocks, while &lt;code&gt;overview&lt;/code&gt; is reducing the resolution (downsampling) of the raster. Both overview levels and tile size has to be tuned to improve the performance for serving/rendering raster on our application.&lt;/p&gt;
&lt;p&gt;Using a large tile size might reduce the overall number of GET requests but it will also mean more data transfer per request. Moreover, having many pyramid levels (overviews) can reduce the response time but the data overhead to create these overviews can turn out to be quite expensive (about 33% for each additional level).&lt;/p&gt;
&lt;h3 id=&#34;cogs-as-stand-in-replacement&#34;&gt;COGs as Stand-in replacement&lt;/h3&gt;
&lt;p&gt;With the aim to create COGs, I ended up experimenting with GeoTIFF and JP2000 formats.&lt;/p&gt;
&lt;p&gt;On converting our &lt;code&gt;.jp2&lt;/code&gt; to &lt;code&gt;.tif&lt;/code&gt; by running &lt;code&gt;gdal_translate -of GTiff ./T43PHP_20210123T051121_B03_20m.jp2 b03.tif&lt;/code&gt; and then &lt;code&gt;gdalinfo ./b03.tif&lt;/code&gt;, it resulted in file size = 58 MB, and&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Band 1 Block=5490x1 Type=UInt16, ColorInterp=Gray
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You&amp;rsquo;d notice that the file size of the &lt;code&gt;.tif&lt;/code&gt; is much higher than that of &lt;code&gt;.jp2&lt;/code&gt;. Additionally we do not see any overviews for the &lt;code&gt;.tif&lt;/code&gt; file.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;On running the command &lt;code&gt;gdaladdo -r average ./b03.tif&lt;/code&gt; to add internal overview to my raster, and then &lt;code&gt;gdalinfo b03.tif&lt;/code&gt;,  the output file size of the resulted raster is 78 MB, and&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Band 1 Block=5490x1 Type=UInt16, ColorInterp=Gray
  Overviews: 2745x2745, 1373x1373, 687x687, 344x344, 172x172
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The point I am trying to convey here is that when you add an overview to the raster, there is an additional cost in terms of storage. In my opinion, the reason sentinel-2 images are stored with &lt;code&gt;.jp2&lt;/code&gt; format is because of lower file size since they do not have to serve the raster on the web but instead let users download them in entirety. This would save them a huge cost on storing the data.&lt;/p&gt;
&lt;h3 id=&#34;cloud-optimized-geotiff-cog&#34;&gt;Cloud Optimized GeoTIFF (COG)&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;With the COG, you&amp;rsquo;ll be able to load the image faster and zoom in and out much smoother than you would if you were working with a regular GeoTIFF. With the GeoTIFF, the entire image needs to finish downloading before the tiles can be generated for viewing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Cloud Optimized GeoTIFFs (COGs) are great! I use them in my work and they allow for easy and rendering of rasters on-the-fly. What makes it great is that it supports HTTP Range requests. This allows you to only fetch data where you are requesting it.&lt;/p&gt;
&lt;p&gt;To create COGs using GDAL, it is as simple as typing&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdalwarp -of COG ./b03.tif b03_cog.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result from the above command results in a file size of 87 MB and,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Band 1 Block=512x512 Type=UInt16, ColorInterp=Gray
  Overviews: 2745x2745, 1373x1373, 687x687, 344x344, 172x172
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, internal overviews and tiles are created for us. We can tune both Block size and Overview parameters if needed. See &lt;a href=&#34;https://gdal.org/drivers/raster/cog.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;You can see from previous experiments that creating COGs can be a costly in terms of its size, but you should not leave it there. The true potential of COG is realized only when you pass in additional options (compression, tile size, overviews etc).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdalwarp -of COG -co COMPRESS=DEFLATE ./b03_cog.tif b03_cog_deflate.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result from the above command results in a file size of 57 MB and,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Band 1 Block=512x512 Type=UInt16, ColorInterp=Gray
  Overviews: 2745x2745, 1373x1373, 687x687, 344x344, 172x172
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Filename&lt;/th&gt;
&lt;th&gt;Size&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Remarks&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;T43PHP_20210123T051121_B03_20m.jp2&lt;/td&gt;
&lt;td&gt;33 Mb&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Original size&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b03.tif&lt;/td&gt;
&lt;td&gt;58 Mb&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;convert jp2 to tif&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b03.tif&lt;/td&gt;
&lt;td&gt;78 Mb&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;add overviews&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b03_cog.tif&lt;/td&gt;
&lt;td&gt;87 Mb&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;convert to COG&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;b03_cog_deflate.tif&lt;/td&gt;
&lt;td&gt;57 Mb&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;COG with deflate&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By this comparison you can see that, if you do not compress the file, it would ingress a huge cost to store them.&lt;/p&gt;
&lt;p&gt;If your aim is to serve rasters on the browser or let users download the data, start using COGs with &lt;strong&gt;additional options&lt;/strong&gt; and you won&amp;rsquo;t notice any difference but only save money in the long run.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to save Earth Engine Image directly to your local machine</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/how-to-save-earth-engine-image-directly-to-your-local-machine/</link>
      <pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/how-to-save-earth-engine-image-directly-to-your-local-machine/</guid>
      <description>&lt;p&gt;Oftentimes you are required to download satellite images for your Area of Interest (AOI) and Google Earth Engine is probably a good place to avail processed satellite images for free and more importantly, only for the area you need.&lt;/p&gt;
&lt;p&gt;One hindrance when you download from earth engine is that the images get saved in google drive, which can fill up fast for large numbers of downloads. To avoid this additional step, there is a hacky trick to download images directly.&lt;/p&gt;
&lt;p&gt;Note: Earth Engine does provide getDownloadURL option, but is limited in the size of download and thus not feasible in this case. Pixel grid dimensions for &lt;a href=&#34;https://developers.google.com/earth-engine/apidocs/ee-image-getdownloadurl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;getDownloadURL&lt;/a&gt; must be less than or equal to 10000 i.e, you can have a maximum of 100 x 100 pixel size images.&lt;/p&gt;
&lt;p&gt;In this post I show a trick which can let you download upto 100 times larger size images, directly to your local machine. Spoiler: &lt;a href=&#34;https://developers.google.com/earth-engine/apidocs/ee-imagecollection-getregion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;getRegion&lt;/a&gt; method plays a significant role to help accomplish this task. Added to that, creating a gridded bounding box for our AOI, with spacing equivalent to the pixel size will aid in our task.&lt;/p&gt;
&lt;p&gt;We will utilize earth engine python client so that all the geopython goodies can be simultaneously utilised.&lt;/p&gt;
&lt;p&gt;To begin with, I have a geopackage containing a polygon, which is our AOI. We aim to download sentinel-2 B4 band for the region. The ideal way would be to use the in-built &lt;code&gt;Export&lt;/code&gt; option, but in our case we would use the &lt;a href=&#34;https://developers.google.com/earth-engine/apidocs/ee-imagecollection-getregion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;getRegion&lt;/a&gt; method along with creating a point grid over our AOI with spacing equivalent to the pixel size.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/IA5OTuN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Left: Our Area of Interest over which to download satellite data. Right: Grid points over AOI bounding box at pixel spacing&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To accomplish creation of points at spacing equal to pixel width and height, we use the following function&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# generate points
def xcor(y_pt, crs):
    def wrap(x_each):
        feat = ee.FeatureCollection(y_pt.map(lambda y_each: ee.Feature(
            ee.Geometry.Point([x_each, y_each], ee.Projection(crs)))))
        return feat
    return wrap
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code can be interpreted as a nested loop.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Pseudo code
for each_x in x_pt:
    for each_y in y_pt:
        create_Point(each_x, each_y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;x_pt and y_pt are generated from the geopackage (AOI) using GeoPandas library as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generatePoints(file_name, pixel_size):

    # read the farm and convert to geojson
    feature = gpd.read_file(file_name).__geo_interface__
    # extract bounds
    minx, miny, maxx, maxy = feature[&#39;bbox&#39;]
    # create a list with spacing equal to pixel_size
    x_pt = ee.List.sequence(minx, maxx, pixel_size)
    y_pt = ee.List.sequence(miny, maxy, pixel_size)
   
    return x_pt, y_pt, minx, maxy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are basically creating a new &lt;code&gt;Point&lt;/code&gt; feature for each x and y point.&lt;/p&gt;
&lt;p&gt;Once we have the grid over our AOI, we can go ahead and call &lt;code&gt;getRegion&lt;/code&gt; method&lt;/p&gt;
&lt;p&gt;The documentation does a good job in explaining what &lt;code&gt;getRegion&lt;/code&gt; is all about&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Output an array of values for each [pixel, band, image] tuple in an ImageCollection. The output contains rows of id, lon, lat, time, and all bands for each image that intersects each pixel in the given region. Attempting to extract more than 1048576 values will result in an error.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The limit 1048576 results in a max tile width and height of 1024 x 1024 pixels. By combining the previously created grid and &lt;code&gt;getRegion&lt;/code&gt;, we could potentially get 100 times more pixels than getDownloadURL. Let us do that!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len_y = len(y_pt.getInfo())
len_x = len(x_pt.getInfo())

imgCollection = ee.ImageCollection(&amp;quot;COPERNICUS/S2_SR&amp;quot;).filters(filters_to_add)
geometry = ee.FeatureCollection(x_pt.map(xcor(y_pt, CRS))).flatten()
input_bands = &amp;quot;B4&amp;quot;
pixel_size = 10

df = get_dataframe(imgCollection, geometry, input_bands, CRS )
data_matrix = df[input_bands].values.reshape(len_y, len_x)
data_matrix = np.flip(data_matrix, axis = 0)
transform = rasterio.transform.from_origin(minx, maxy, pixel_size, pixel_size)
save_tiff(&amp;quot;output.tif&amp;quot;, data_matrix, transform, CRS)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code first gets the count of points in each of the 2-dimensions followed by fetching the dataframe which contains the lat, lon and pixel value as shown in the image.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/5SjjW0E.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -Output of getRegion results in lat, lon, pixel value in sequential order&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Now we reshape the dataframe and flip it to make the pixel arrange in image format. Lastly, we save the image by passing the transformation of the image. Make sure to have an imageCollection for the &lt;code&gt;getRegion&lt;/code&gt; method to work. Currently, the above code can only download 1 band at a time, but with simple modification to the &lt;code&gt;getDataframe&lt;/code&gt; function, that too can be changed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def getDataframe(img_col, feature, input_band, crs):
   
    imgcol = ee.ImageCollection(img_col).select(input_band)
    df = pd.DataFrame(imgcol.getRegion(feature.geometry(), 10, crs).getInfo())
    df, df.columns = df[1:], df.iloc[0]
    df = df.drop([&amp;quot;id&amp;quot;, &amp;quot;time&amp;quot;], axis=1)

    return df

def saveTiff(output_name, data_array, transform, crs):

    options = {
        &amp;quot;driver&amp;quot;: &amp;quot;Gtiff&amp;quot;,
        &amp;quot;height&amp;quot;: data_array.shape[0],
        &amp;quot;width&amp;quot;: data_array.shape[1],
        &amp;quot;count&amp;quot;: 1,
        &amp;quot;dtype&amp;quot;: np.float32,
        &amp;quot;crs&amp;quot;: crs,
        &amp;quot;transform&amp;quot;: transform
    }

    with rs.open(output_name, &#39;w&#39;, **options) as src:
        src.write(data_array, 1)

    return None
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of the exercise is that you have a raster directly downloaded to your local machine, without google drive/ cloud intermediaries. One thing worth pointing out, is for extremely large images, you are better off downloading via the specified steps in docs. This hacky way is to simplify things and avoid google drive (which is never empty for me).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Z8DEJHh.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The full code can be accessed &lt;a href=&#34;https://github.com/amanbagrecha/ee-image-direct-download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vector tiles and Docker using pg_tilerserv</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/vector-tiles-and-docker-using-pg-tilerserv/</link>
      <pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/vector-tiles-and-docker-using-pg-tilerserv/</guid>
      <description>&lt;p&gt;In this blog we look at how to serve your geospatial data as vector tiles using pg_tileserv in a docker container.&lt;/p&gt;
&lt;h2 id=&#34;what-are-vector-tiles&#34;&gt;What are vector tiles?&lt;/h2&gt;
&lt;p&gt;Vector Tiles are similar to raster tiles, but instead of serving images, vector tiles serve geospatial data which are vectors themselves and not images. This allows for reduced data transfer over a network, faster loading while allowing client side rendering. Moreover, vector tiles allow for flexible styling of your geospatial data since it renders on the client side. All this is not possible with raster tiles and hence vector tiles have gained traction in the last few years.&lt;/p&gt;
&lt;p&gt;One of the most popular specifications to serve vector tiles is mapbox vector tiles, utilized by many open source tile servers.&lt;/p&gt;
&lt;p&gt;Because PostGIS can create mapbox vector tiles from vector data, it becomes easy to serve them over the web. Many tileservers use the power of this postGIS functionality to serve vector tiles over the web.&lt;/p&gt;
&lt;p&gt;As for a visual understanding as to what is different between vector and raster tiles, the following image illustrates that. The red bounding box is the response to clients request to serve vector tiles. Notice the format is &lt;code&gt;pbf&lt;/code&gt; as opposed to &lt;code&gt;png&lt;/code&gt; for raster tiles.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/S5uzLpN.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;why-use-docker-for-this&#34;&gt;Why use docker for this?&lt;/h2&gt;
&lt;p&gt;Using docker would expedite the process of starting and &amp;ldquo;actually&amp;rdquo; using the applications. It is like sharing your machine with others so that they do not have to install anything to get started. For this reason, it makes complete sense to use docker for moderate to high complexity projects.&lt;/p&gt;
&lt;h2 id=&#34;what-is-pg_tileserve&#34;&gt;What is pg_tileserve?&lt;/h2&gt;
&lt;p&gt;To create vector tiles, and serve them on the web, you need a middleware that can talk to the database and also serve them on the web. Since pg_tileserve uses a postgis function under the hood, it becomes a default choice to add a lightweight service to serve vector tiles. pg_tileserv returns Mapbox Vector tiles on input of vector geometry. In addition to reading tables from the database, it can handle complex functions to meet our needs.&lt;/p&gt;
&lt;p&gt;ST_asMVT, an aggregate function which is used under the hood for pg_tileserv, returns mapbox vector tile format based on google protobuf. While there are other formats such as MBtiles which is sqlite based binary file (can be opened in sqlite), Mapbox Vector Tile format seems to be winning this race and is thus the most popular format currently.&lt;/p&gt;
&lt;h3 id=&#34;to-get-started-with-serving-your-vector-data-to-the-web-using-pg_tileserv-we-follow-the-below-mentioned-steps&#34;&gt;To get started with serving your vector data to the web using pg_tileserv, we follow the below mentioned steps&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Download &lt;a href=&#34;https://github.com/CrunchyData/pg_tileserv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pg_tileserv&lt;/a&gt; folder from &lt;a href=&#34;https://downgit.github.io/#/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;down-git&lt;/a&gt; website and save it to your local directory.
&lt;img src=&#34;https://i.imgur.com/QkF6OF9.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The folder contains all the files required to start a docker container and serve vector tiles.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;└───data/  — would contain all your vector data
└───load-data.sh — shell script to load data into PostgreSQL
└───pg_tileserv.env — database URL to connect
└───docker-compose.yml — 
└───pg.env — environment variable for database
└───cleanup.sh — assemble multiple containers
└───README — guide to setup docker by Just van den Broecke
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Next, Modify &lt;code&gt;docker-compose.yml&lt;/code&gt; file under &lt;strong&gt;build-&amp;gt;context&lt;/strong&gt; to point to the docker file &lt;a href=&#34;https://github.com/CrunchyData/pg_tileserv.git&#34;&gt;https://github.com/CrunchyData/pg_tileserv.git&lt;/a&gt;. Since we did not clone the repository, we specify the Dockerfile using the git link.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/AzclY3c.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;Dump all your geospatial data into &lt;code&gt;data&lt;/code&gt; dir. This directory will be &lt;em&gt;mounted&lt;/em&gt; to the container, once we start it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the &lt;code&gt;pg_tileserv.env&lt;/code&gt; environment file as you wish, to specify the name and password of your database.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notes on env files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pg_tilerserv.env&lt;/code&gt; file contains the database url which is of the format &lt;code&gt;postgres://your-username:your-password@localhost:5432/your-database-name&lt;/code&gt; while &lt;code&gt;pg.env&lt;/code&gt; contains credentials for postgres database.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notes on docker-compose file&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are mounting &lt;code&gt;data&lt;/code&gt; dir from our local system to the work dir in the docker container.&lt;/li&gt;
&lt;li&gt;We are mapping port 7800 from our local machine to 7800 to the pg_tileserv container.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Start Docker Desktop and run &lt;code&gt;docker-compose build&lt;/code&gt; in the command line. It will download the image needed from the dockerfile specified. It only downloads the latest alpine image and all other dependencies are installed in the build.&lt;/p&gt;
&lt;p&gt;Once the database setup is done, we now load data into the database by running either &lt;code&gt;load-data.sh&lt;/code&gt; shell script (or) the following command,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#Load data using shp2pgsql 
docker-compose exec pg_tileserv_db sh -c &amp;quot;shp2pgsql -D -s 4326 /work/ne_50m_admin_0_countries.shp | psql -U tileserv -d tileserv&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above command opens a terminal inside the pg_tileserv_db container and runs the &lt;code&gt;shp2pgsql&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;ogr2ogr&lt;/code&gt; command line tool if your data is anything other than shapefile. Read this blog by &lt;a href=&#34;https://blog.crunchydata.com/blog/loading-data-into-postgis-an-overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kat Batuigas&lt;/a&gt; to know how to do it.&lt;/p&gt;
&lt;p&gt;Finally, run &lt;code&gt;docker-compose up&lt;/code&gt; to start the service. You&amp;rsquo;d see both containers starting up and your web app being served on port 7800. If you do not see this, stop the container and run again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Gy4QlTL.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;On running the web app in the browser we see our tables visible under Table Layers and the schema it belongs to. We added a few additional layers (public.hydrants and a function layer following steps from &lt;code&gt;README.md&lt;/code&gt;) to play around with it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/CwmhUdK.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;endnote&#34;&gt;Endnote&lt;/h2&gt;
&lt;p&gt;We looked at serving vector data as tiles using pg_tileserv and docker container. Docker enables reproducibility and expedites the process of running a web app. Although there are numerous open-source tile servers available, each has its use case and would require testing them out to identify the best tileserver for your use case. You can read a long list of tileservers &lt;a href=&#34;https://github.com/mapbox/awesome-vector-tiles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So next time you think to serve large vector data on the web app, make sure to use vector tiles built inside a docker container. It will surely simplify things!&lt;/p&gt;
&lt;p&gt;Source:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;CrunchyData/pg_tileserv: A very thin PostGIS-only tile server in Go. Takes in HTTP tile requests, executes SQL, returns MVT tiles. (&lt;a href=&#34;https://github.com/CrunchyData/pg_tileserv/&#34;&gt;https://github.com/CrunchyData/pg_tileserv/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lightweight PostGIS Web Services Using pg tileserv and pg featureserv (&lt;a href=&#34;https://www.youtube.com/watch?v=TXPtocZWr78&amp;amp;t=1s&amp;amp;ab_channel=CrunchyData&#34;&gt;https://www.youtube.com/watch?v=TXPtocZWr78&amp;amp;t=1s&amp;amp;ab_channel=CrunchyData&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reference | Vector tiles | Mapbox (&lt;a href=&#34;https://docs.mapbox.com/vector-tiles/reference/&#34;&gt;https://docs.mapbox.com/vector-tiles/reference/&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vector Tiles – Geoinformation HSR (&lt;a href=&#34;https://giswiki.hsr.ch/Vector_Tiles&#34;&gt;https://giswiki.hsr.ch/Vector_Tiles&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Polygonize Raster and Compute Zonal-Statistics in Python</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</guid>
      <description>&lt;p&gt;The output of a clustering algorithm is a raster. But when you want to compute statistics of the clustered raster, it needs to be polygonized.&lt;/p&gt;
&lt;p&gt;A simple way to perform this action is using the gdal command line &lt;code&gt;gdal_polygonize.py&lt;/code&gt; script. This script requires the output file format, input raster file and output name of the vector file. You can additionally mask pixel values which you don&amp;rsquo;t want to convert to polygons. For this example, we would consider a single band image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python gdal_polygonize.py raster_file -f &amp;quot;ESRI Shapefile&amp;quot; vector_file.shp  layername atrributefieldname
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;--nomask&lt;/code&gt; allows to include nodata values in the shapefile&lt;/p&gt;
&lt;p&gt;&lt;code&gt;atrributefieldname&lt;/code&gt; should always be preceded with &lt;code&gt;layername&lt;/code&gt; else it would result in an error.&lt;/p&gt;
&lt;p&gt;The output would result in a vector layer. The number of output polygons is equal to the number of non-NA values. Each neighbouring cell (pixel) which is connected in the raster having the same value is combined to form a single polygon.&lt;/p&gt;
&lt;p&gt;For instance, consider this 4 x 4 raster. When converted to vector, it resulted in 6 polygons. Note that disconnected similar values form an independent polygon. Each polygon will have an attribute as its pixel value from the raster, in the data type of the image. These would end up being a pair of (polygon, value) for each feature found in the image.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xeJ4BGa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Converting Raster to Vector using GDAL. The output polygon has attribute associated with its raster value &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Another way to polygonize raster programmatically is to use the &lt;code&gt;rasterio&lt;/code&gt; library. Since rasterio utilizes GDAL under the hood, it also performs similar action and results in a pair of geometry and raster value. We create a tuple of dictionaries to store each feature output.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# code to polygonize using rasterio
from rasterio import features

# read the raster and polygonize
with rasterio.open(cluster_image_path) as src:
    image = src.read(1, out_dtype=&#39;uint16&#39;) 
    #Make a mask!
    mask = image != 0
# `results` contains a tuple. With each element in the tuple representing a dictionary containing the feature (polygon) and its associated raster value
results = ( {&#39;properties&#39;: {&#39;cluster_id&#39;: int(v)}, &#39;geometry&#39;: s} 
            for (s, v) in (features.shapes(image, mask=mask, transform=src.transform)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the raster polygonized, we can use &lt;code&gt;rasterstats&lt;/code&gt; library to calculate zonal statistics. We use this library since there is no in-built functionality for rasterio to calculate it.&lt;/p&gt;
&lt;p&gt;This library has a function &lt;code&gt;zonal_stats&lt;/code&gt; which takes in a vector layer and a raster to calculate the zonal statistics. Read more &lt;a href=&#34;https://pythonhosted.org/rasterstats/manual.html#zonal-statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The parameters to the function are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;vectors: path to an vector source or geo-like python objects&lt;/li&gt;
&lt;li&gt;raster: ndarray or path to a GDAL raster source&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and various other options which can be found &lt;a href=&#34;https://github.com/perrygeo/python-rasterstats/blob/master/src/rasterstats/main.py#L34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To create a vector layer from the tuple &lt;code&gt;results&lt;/code&gt;, we use geopandas. There are other libraries (such as fiona) which can also create vector geometry from shapely objects.&lt;/p&gt;
&lt;p&gt;For raster, we pass the &lt;code&gt;.tif&lt;/code&gt; file directly to &lt;code&gt;zonal_stats&lt;/code&gt;. The final code looks like the following&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rasterstats import zonal_stats

in_shp = gpd.GeoDataFrame.from_features(results).set_crs(crs=src.crs)

# stats parameter takes in various statistics that needs to be computed 
statistics= zonal_stats(in_shp,image,stats=&#39;min, max, mean, median&#39;,
                geojson_out=True, nodata = -999)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is a geojson generator when &lt;code&gt;geojson_out&lt;/code&gt; is True. we can convert the geojson to dataframe and export as csv for further processing.&lt;/p&gt;
&lt;p&gt;This way, with the help of geopandas, rasterstats and rasterio, we polygonize the raster and calculate zonal statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two ways to Programmatically change projection of raw CSV</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/two-ways-to-change-projection-of-raw-csv/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/two-ways-to-change-projection-of-raw-csv/</guid>
      <description>&lt;p&gt;Often, field values are collected in the Geographic Coordinate Reference System as CSV or ASCII so that it can be universally used. But when you want to perform any kind of analysis on these values, there is a need to reproject them into a Projected Coordinate Reference System for the specific area. Although there are many ways that exist now with desktop GIS, these methods can be cumbersome if you have thousands of files to reproject.&lt;/p&gt;
&lt;p&gt;This task of reprojecting raw CSV can be accomplished using GDAL although it is not straightforward. It requires an indication of geographic data of a CSV file which is provided using VRT (GDAL virtual Raster). More advanced tools now exist which are either built on top of GDAL or are very similar. &lt;strong&gt;GeoPandas&lt;/strong&gt; and &lt;strong&gt;pyproj&lt;/strong&gt; are two such libraries which can help us reproject our raw CSV on-the-fly.&lt;/p&gt;
&lt;p&gt;We first look at how this task can be accomplished using the GDAL command line.&lt;/p&gt;
&lt;h3 id=&#34;reproject-csv-using-ogr2ogr&#34;&gt;Reproject CSV using &lt;code&gt;ogr2ogr&lt;/code&gt;&lt;/h3&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Udf4gdV.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 — Raw &lt;b style=&#34;color:red;&#34;&gt;input.csv&lt;/b&gt; with &lt;b&gt;lat&lt;/b&gt; &amp; &lt;b&gt;lon&lt;/b&gt; geometry column &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This example shows using &lt;code&gt;ogr2ogr&lt;/code&gt; to reproject the CRS of CSV file with the latitude, longitude coordinates stored as columns &lt;strong&gt;lat&lt;/strong&gt;, &lt;strong&gt;lon&lt;/strong&gt; in the &lt;code&gt;input.csv&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ogr2ogr -f CSV -lco GEOMETRY=AS_XY -t_srs EPSG:32644 output.csv input.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Following is the explanation of the above command,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;-lco GEOMETRY=AS_XY&lt;/code&gt; : Layer creation option with XY columns added in output CSV.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;input.vrt&lt;/code&gt; : Input Virtual Raster file containing information about CSV and its geometry.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-t_srs EPSG:32644&lt;/code&gt; : Set target CRS to EPSG:32644&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-f CSV&lt;/code&gt; : specify the output file format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output.csv&lt;/code&gt; : output CSV with reprojected coordinates&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the above code, &lt;code&gt;input.vrt&lt;/code&gt; is a GDAL virtual raster which has to be created prior to running the command. It points to the CSV file which has the location data stored as columns (lon, lat)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!--input.vrt pointing to the input.csv--&amp;gt;
&amp;lt;OGRVRTDataSource&amp;gt; 
  &amp;lt;OGRVRTLayer name=&amp;quot;input&amp;quot;&amp;gt; 
    &amp;lt;SrcDataSource&amp;gt;input.csv&amp;lt;/SrcDataSource&amp;gt; 
    &amp;lt;GeometryType&amp;gt;wkbPoint&amp;lt;/GeometryType&amp;gt; 
    &amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt; 
    &amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt; 
  &amp;lt;/OGRVRTLayer&amp;gt; 
&amp;lt;/OGRVRTDataSource&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;But what does the above xml mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The above xml is a virtual raster (VRT) which allows for lazy processing. Often, we have to save intermediary outputs on our local disk, which could potentially take a lot of space. To avoid that, VRT allows to store the processing in an xml encoding and performs all intermediary action at once, in the final step.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The first line &lt;code&gt;&amp;lt;OGRVRTDataSource&amp;gt;&lt;/code&gt; is the root element.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;OGRVRTLayer name=&amp;quot;input&amp;quot;&amp;gt;&lt;/code&gt; corresponds with the &lt;code&gt;&amp;lt;SrcDataSource&amp;gt; input.csv &amp;lt;/SrcDataSource&amp;gt;&lt;/code&gt; and points to the &lt;code&gt;input.csv&lt;/code&gt; file we want to reproject.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt;&lt;/code&gt; specifies the CRS of our &lt;code&gt;input.csv&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;GeometryType&amp;gt; wkbPoint &amp;lt;/GeometryType&amp;gt;&lt;/code&gt; is the format that coordinates are stored in.&lt;/li&gt;
&lt;li&gt;Lastly, &lt;code&gt;&amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt;&lt;/code&gt; indicates the columns corresponding to lon and lat in csv. Read more about converting CSV to VRT &lt;a href=&#34;https://gdal.org/drivers/vector/csv.html#reading-csv-containing-spatial-information&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/HefHXvu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 — Reprojecting CSV from EPSG:4326 to EPSG:32644 using GDAL &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Hence, by running the above GDAL command, we would be able to reproject our CSV. By writing a bash script, this method can be scaled to thousands of files. But the intermediary &lt;code&gt;VRT&lt;/code&gt; file is messy to handle and it would be nice to avoid it. Luckily for us, there are libraries built on top of GDAL which would help us avoid the hassle of creating intermediary files.&lt;/p&gt;
&lt;h2 id=&#34;using-geopandas&#34;&gt;Using GeoPandas&lt;/h2&gt;
&lt;p&gt;With its simple and intuitive API, GeoPandas allows us to read, reproject CRS and write files on-the-fly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;in_path = &#39;./&#39;
out_path = &#39;./output&#39;
files= [f for f in os.listdir(in_path) if f.endswith(&#39;.csv&#39;)]
input_crs = &#39;EPSG:4326&#39;
output_crs = &#39;EPSG:32644&#39;

if not os.path.exists(out_path):
    os.mkdir(out_path)

for file in files:
    df = pd.read_csv(file, header=None)
    gdf = gpd.GeoDataFrame(
        df, crs=input_crs , geometry=gpd.points_from_xy(df.iloc[:,0], df.iloc[:,1]))

    gdf.to_crs(output_crs, inplace=True)
    gdf.iloc[:,0] = gdf.geometry.x # replace x
    gdf.iloc[:,1] = gdf.geometry.y # replace y
    
    # export reprojected csv 
    gdf.iloc[:,:-1].to_csv(os.path.join(out_path, file), index=False )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we loop through our CSV files. For each file, we create a GeoDataFrame and change the CRS. Lastly, we replace the coordinates with reprojected one.&lt;/p&gt;
&lt;h3 id=&#34;endnote&#34;&gt;Endnote&lt;/h3&gt;
&lt;p&gt;There is another way I found by using &lt;strong&gt;pyproj&lt;/strong&gt; library which is quite verbose but performs reprojection on-the-fly. To read about the &lt;strong&gt;pyproj&lt;/strong&gt; method, refer &lt;a href=&#34;https://gis.stackexchange.com/a/168496&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overlay cropped raster with vector layer</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</guid>
      <description>&lt;p&gt;I recently faced a problem of having to plot &amp;ldquo;cropped raster&amp;rdquo; layer and a vector layer on the same axes. It is known that we first need to identify the spatial extent of each layer, having the same coordinate reference system.&lt;br&gt;
Rasterio does offer a plotting function &lt;code&gt;show&lt;/code&gt; which can plot a raster layer with the correct spatial extent for you when we pass the dataset reader object.&lt;/p&gt;
&lt;p&gt;When we pass a reader object, the spatial extent is automatically read by &lt;code&gt;show&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:  # import rasterio as rs
    
    f, ax = plt.subplots(figsize=(9,9))
    _ = show(src, ax=ax)            # from rasterio.plot import show
    _ = vector_layer.plot(ax=ax)    # `vector_layer` is a geodataframe (geopandas)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/A33Vopw.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Overlay raster with vector layer. Notice the spatial extent&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Moreover, if we pass a numpy array to the &lt;code&gt;show&lt;/code&gt; function,  the spatial extent of that array has to be explicitly passed using the &lt;code&gt;transform&lt;/code&gt; parameter of the &lt;code&gt;show&lt;/code&gt; function since the numpy array does not know the corner location of the raster and thus the plot would begin with x,y: 0,0 as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1) # img is a numpy array

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if you want to plot a subset of the raster image, in the sense that you would like to slice the image arbitrarily and plot it. When you slice the image, the affine transformation is not the same anymore and thus plotting the sliced image would result in a plot having the spatial extent of the original image while the sliced image being magnified (Fig. 2).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1)[1:-1,1:-1]

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ePTM6q0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 - Overlaid cropped raster and vector layer with incorrect spatial extents&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To avert this problem, we need to find the new affine transformation of the cropped image. Luckily rasterio has a &lt;code&gt;window_transform&lt;/code&gt;  method on the dataset reader which can compute the new transformation from the old one by passing the bounds of the layer. The &lt;code&gt;window_transform&lt;/code&gt; function can either take a 2D N-D array indexer in the form of a tuple &lt;code&gt;((row_start, row_stop), (col_start, col_stop))&lt;/code&gt; or provide offset as written in its &lt;a href=&#34;https://rasterio.readthedocs.io/en/latest/api/rasterio.windows.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;cropped-raster-and-vector-overlay&#34;&gt;Cropped raster and vector overlay&lt;/h2&gt;
&lt;p&gt;The above method returns the new affine transformation, which can be passed to the &lt;code&gt;show&lt;/code&gt; function for the numpy array through the &lt;code&gt;transform&lt;/code&gt; parameter. We also change the read method instead of slicing the array by window parameter to maintain uniformity&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load raster
with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:
    # window =  (((row_start), (row_stop)), ((col_start), (col_stop)))
    img = src.read(1, window = ((1,-1), (1,-1)))
    f, ax = plt.subplots(figsize=(9,9))
    show(img, transform=src.window_transform(((1,-1), (1,-1))), ax=ax)

    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/uwVnq4z.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 - Overlay of cropped raster and vector. Notice the updated spatial extent &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The &lt;code&gt;show&lt;/code&gt; method is helpful for plotting rasters or even RGB images for that matter. One of the differences with matplotlib&amp;rsquo;s plotting is the order of axes. &lt;code&gt;show&lt;/code&gt; expects it the bands to be the last axis while matplotlib, the first. It can also plot 4-band image, which is almost always the for satellite images.
While there is an &lt;code&gt;extent&lt;/code&gt; paramter in matplotlib&amp;rsquo;s plotting function, &lt;code&gt;show&lt;/code&gt; function is much tidier and straight-forward to implement cropped raster and overlay vector layer on it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SMAP Time Series</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/smap-time-series/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/smap-time-series/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Farmers in parts of India still rely on groundwater for irrigation. For them to help understand the present condition of their farm, NASA’s Soil Moisture Active Passive (&lt;strong&gt;SMAP&lt;/strong&gt;) satellite data could fill a significant void.&lt;/p&gt;
&lt;p&gt;The mission collects the kind of local data agricultural and water managers worldwide need.&lt;/p&gt;
&lt;p&gt;The main output of this data set is &lt;strong&gt;surface soil moisture&lt;/strong&gt; (SSM)(representing approximately the top 5 cm of the soil column on average, given in cm3 /cm3 ) presented on the global 36 km EASE-Grid 2.0. While there are other measurements, we are only restricting ourselves to SSM&lt;/p&gt;
&lt;p&gt;The SSM product has three main levels. L1, L2, and the latest being L3. SMAP uses a radiometer to detect microwave signals and process to obtain soil moisture. It initially had radar onboard but failed in 2015. Although the product is primarily available in 36 km resolution, with the help of Sentinel-1 Radar product, we now have access to 9 km resolution daily global product as well post 2016.&lt;/p&gt;
&lt;p&gt;We are going to work with a 36 km product since a time-series can be computationally intensive to download.&lt;/p&gt;
&lt;p&gt;One such product is &lt;strong&gt;L3_SM_P&lt;/strong&gt;, a daily global product, which is an abbreviation of L3 soil moisture 36 km resolution.&lt;/p&gt;
&lt;p&gt;We choose Bengaluru as our area of interest and perform the following three steps in sequence&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Download the SMAP L3 data for the latest one month ( &lt;strong&gt;August 2021&lt;/strong&gt; here).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extraction of the soil moisture values from SMAP L3 data over Lat, Lon of Bengaluru in python.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot the time series plot for the extracted soil moisture values for the latest one month.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To download the SMAP L3 data, we head over to &lt;a href=&#34;https://nsidc.org/data/SPL3SMP/versions/7&#34;&gt;https://nsidc.org/data/SPL3SMP/versions/7&lt;/a&gt; and select a time-period ( in our case for the entire month of August 2021) under the download tab. We then click on the download script button as a python file.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/aoKJnay.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Downloading python script for the month of August from NSIDC&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As can be seen in the picture, we have a download size of 980 mb once we run the python script.
The next step would be to download the actual files and extract soil moisture value for the selected lat long.
One thing to note is, since the product has a resolution of 36 km and that the entire pixel represents one value, we have to couple together a set of pixels around Bengaluru since the entire region does not overlay in one pixel size.&lt;/p&gt;
&lt;p&gt;We would be using colaboratory in this entire process since it allows for smooth use of the command line within the notebook itself.&lt;/p&gt;
&lt;h3 id=&#34;run-the-downloaded-script-it-will-ask-you-for-your-earth-data-credentials-username-and-password&#34;&gt;Run the downloaded script. It will ask you for your earth data credentials (username and password)&lt;/h3&gt;
&lt;p&gt;We move the downloaded files to data directory and delete any associated files that comes along with it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Download data: Enter credentials for earth data
%run /content/download_SPL3.py
 
# move files to data dir
!mkdir -p data/L3_SM_P
!mv *h5 data/L3_SM_P
!rm *.h5*
 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, get the lat long for EASE grid 2.0. Since we have to locate Bengaluru (study area) and SMAP uses a specific grid system, we download these files.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lats.964x406x1.double
!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lons.964x406x1.double
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;extract-soil-moisture&#34;&gt;Extract soil moisture&lt;/h3&gt;
&lt;p&gt;We define a python class since there are two half-orbit passes (ascending and descending pass) and we could later combine them easily.&lt;/p&gt;
&lt;p&gt;We create a &lt;code&gt;read_SML3P&lt;/code&gt; method which reads the hdf5 files using the h5py library as an array and removes noisy elements as defined by the user guide. The filename contains the date of acquisition and we extract that.&lt;/p&gt;
&lt;p&gt;We next define the &lt;code&gt;generate_time_series&lt;/code&gt; method to subset the array to our area of interest (Bengaluru) while also taking the mean since there might be more than 1 pixel intersecting the AOI and then return a dataframe with date and the value of Soil Moisture.&lt;/p&gt;
&lt;p&gt;There are some additional method we define to run and initialise the class which can be read from &lt;a href=&#34;https://github.com/amanbagrecha/smap_time_series_analysis/blob/main/main.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SML3PSoilMoist:
  &amp;quot;&amp;quot;&amp;quot;
  get soil moisture from L3 SMAP SCA-V algo for the specified date
  Parameters
  ----------
  soil_moisture: numpy.array
  flag_id:  [str] Quality flag of retrieved soil moisture using SCA-V
  var_id: [str] can be replaced with scva algorithm which is the default (baseline)
  group_id: [str] retrive soil moisture for Ascending or descending pass
  file_list: [list] of downloaded files; File path of a SMAP L3 HDF5 file
  -------
  Returns Soil moisture values and time period as a DataFrame
  &amp;quot;&amp;quot;&amp;quot;
 
  def __init__(self, file_list : &#39;list&#39;, orbit_pass: &#39;str&#39;):
    
    ...
 
  def run_(self):
    &amp;quot;&amp;quot;&amp;quot;read files and return 3d array and time&amp;quot;&amp;quot;&amp;quot;
    ...
 
 
  def read_SML3P(self, filepath):
    &#39;&#39;&#39; This function extracts soil moisture from SMAP L3 P HDF5 file.
    # refer to https://nsidc.org/support/faq/how-do-i-interpret-surface-and-quality-flag-information-level-2-and-3-passive-soil

    &#39;&#39;&#39;    
    with h5py.File(filepath, &#39;r&#39;) as f:
 
        group_id = self.group_id 
        flag_id = self.flag_id
        var_id = self.var_id
 
        flag = f[group_id][flag_id][:,:]
 
        soil_moisture = f[group_id][var_id][:,:]        
        soil_moisture[soil_moisture==-9999.0]=np.nan;
        soil_moisture[(flag&amp;gt;&amp;gt;0)&amp;amp;1==1]=np.nan # set to nan expect for 0 and even bits
 
        filename = os.path.basename(filepath)
        
        yyyymmdd= filename.split(&#39;_&#39;)[4]
        yyyy = int(yyyymmdd[0:4]); mm = int(yyyymmdd[4:6]); dd = int(yyyymmdd[6:8])
        date=dt.datetime(yyyy,mm,dd)
 
    return soil_moisture, date
 
  def generate_time_series(self, bbox: &#39;list -&amp;gt; [N_lat, S_lat, W_lon, E_lon]&#39;):
    
    N_lat, S_lat, W_lon, E_lon = bbox
    subset = (lats&amp;lt;N_lat)&amp;amp;(lats&amp;gt;S_lat)&amp;amp;(lons&amp;gt;W_lon)&amp;amp;(lons&amp;lt;E_lon)
    sm_time = np.empty([self.time_period]);
    
    sm_data_3d, times = self.run_()
    for i in np.arange(0,self.time_period):
        sm_2d = sm_data_3d[:,:,i]
        
        sm_time[i] = np.nanmean(sm_2d[subset]);
 
    return pd.DataFrame({&#39;time&#39; : times, self.orbit_pass: sm_time })
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we plot the dataframe using pandas method &lt;code&gt;plot&lt;/code&gt; and the result is to be shown to the world.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/PngGsda.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This blog helps demonstrates use of SMAP product to generate time series for an entire month of August. You can read more about the specification of the product &lt;a href=&#34;https://nsidc.org/support/faq/how-do-i-interpret-surface-and-quality-flag-information-level-2-and-3-passive-soil&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data courtesy: O&amp;rsquo;Neill et al. doi: &lt;a href=&#34;https://doi.org/10.5067/HH4SZ2PXSP6A&#34;&gt;https://doi.org/10.5067/HH4SZ2PXSP6A&lt;/a&gt;. [31st August, 2021].&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Contour Maps in QGIS</title>
      <link>https://amanbagrecha.github.io/post/qgis/contour-maps-in-qgis/</link>
      <pubDate>Sat, 24 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/qgis/contour-maps-in-qgis/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Most of the time, we are equipped with a discrete set of sample points (of temperature, rainfall etc) and are tasked with generating a continuous surface.
This is where spatial interpolation comes into picture. The objective is to estimate the most probable value at an unknown location with a set of known points within the extent of sample points.&lt;/p&gt;
&lt;p&gt;Methods to perform spatial interpolation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;TIN: Triangular Irregular Network forms contiguous, non-overlapping triangles by dividing the geographic space on set of sample points&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;IDW: Inverse Distance Weighted interpolation method estimates cell values by weighted average of sample data. The closer the point, the more weight assigned. We can fix the radius of influence or the total sample points to weigh for cell value.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spline: Also called french curves. It uses a mathematical function that minimizes overall surface curvature, resulting in a smooth surface that passes through the input points.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kriging: A group of geostatistical techniques to interpolate the value of a random field at an unobserved location from observations of its value at a nearby location. It is implemented using semi-variogram.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this blog, we create surface plots for Rainfall Correction Factors, which is indicative of how much the climate impacts a hydraulic structure based on the return period it is designed for.&lt;/p&gt;
&lt;p&gt;These RCF are useful for hydraulic structures such as dams, storm water drains, and spillways.
These RCF are derived from Global Climate Models (GCMs) which models future scenarios. Not considering these factors can lead to reduced life time of the structure.&lt;/p&gt;
&lt;p&gt;We calculate the RCF for each point for a grid of lat,lon around the indian subcontinent. These RCF are as a result of intensive computational simulations run in matlab which is out of scope for this blog.&lt;/p&gt;
&lt;h3 id=&#34;1-load-points-in-qgis&#34;&gt;1. Load points in QGIS&lt;/h3&gt;
&lt;p&gt;Our data is in the csv format with each column of the RP_ family representing the return period the Rainfall Correction Factor is estimated for.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/fZpYAK9.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -sample data points with key location and return period of RCF&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This file can be imported into qgis from the layers panel and adding a delimited text layer. Once the layer is added, we export as shapefile so as to ease the process of automating the workflow which comes in handy later at the end of the blog.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/MvHcCZx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -Add the csv file using Add delimited text layer&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We load the sampled points and add an India boundary as the base vector to later clip the features to our area of Interest.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/IuoE9pb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -Points equally spaced around the Indian state. Each point represent a RCF value&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;2-generate-raster-from-points-using-tin-interpolation&#34;&gt;2. Generate Raster from points using TIN interpolation&lt;/h3&gt;
&lt;p&gt;For demonstration let us take an example to run through the entire process of generating surface raster and styling which can be later automated using python in qgis.&lt;/p&gt;
&lt;p&gt;We use these sampled locations of points to generate a surface using TIN Interpolation readily available as a toolbox in qgis. The input parameter for the vector layer is our shapefile of points while the interpolation attribute is going to be the RP_ family of columns.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/nzE5Vj7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -TIN interpolation in QGIS&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The output of the interpolation with pixel size of 0.01 is shown below. The extent was set to the boundary of Indian state.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/1HAjqQ4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.5 -Output surface raster with 0.01 pixel size&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can go a step further and derive contours using the &lt;code&gt;contour&lt;/code&gt; toolbox provided in qgis.&lt;/p&gt;
&lt;h3 id=&#34;3-generate-contours-from-raster-to-style-the-layer&#34;&gt;3. Generate Contours from raster to style the layer&lt;/h3&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/SOKxUmC.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.6 -Generate contours from the surface raster&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ExfM0MM.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.7 -Output as contour lines with 0.1 as interval&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;A better way to get the contour lines is by changing the symbology of the raster to contours and providing an interval. This exact method will be employed later in this post.&lt;/p&gt;
&lt;h3 id=&#34;automating-the-process&#34;&gt;Automating the process&lt;/h3&gt;
&lt;p&gt;So far we have looked into creating surface raster for an individual return period. But we have several other return periods and we do not want to repeat ourselves. Thus we write a tiny python code to automate this workflow.&lt;/p&gt;
&lt;p&gt;We derive the RCFs for return period of 5year, 10year, 25year, 50year&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# specify the output location for saving the files
OUTPATH = &#39;D:\\gcm_qgis\\&#39;

# loop over different return periods from the shapefile
for i,j in enumerate([&#39;2y&#39;, &#39;10y&#39;, &#39;25y&#39;, &#39;50y&#39;], 3):

    # specify the shapefile containing the RCP values
    MYFILE = &#39;D:\\gcm_qgis\\RCP_avg.shp|layername=RCP_avg::~::0::~::{}::~::0&#39;.format(i)

    # Run interpolation and do not save the output permanently
    RESULTS = processing.run(&amp;quot;qgis:tininterpolation&amp;quot;, 
    {&#39;INTERPOLATION_DATA&#39;: MYFILE,
    &#39;METHOD&#39;:1,
    &#39;EXTENT&#39;:&#39;68.205600900,97.395561000,6.755997100,37.084107000 [EPSG:4326]&#39;,
    &#39;PIXEL_SIZE&#39;:0.01,
    &#39;OUTPUT&#39;:&#39;TEMPORARY_OUTPUT&#39;})

    # clip the temporary output from prev step and save the files.
    processing.runAndLoadResults(&amp;quot;gdal:cliprasterbymasklayer&amp;quot;, 
    {&#39;INPUT&#39;:RESULTS[&#39;OUTPUT&#39;],
    &#39;MASK&#39;:&#39;C:/Users/91911/Downloads/india-osm.geojson.txt|layername=india-osm.geojson&#39;,
    &#39;SOURCE_CRS&#39;:None,&#39;TARGET_CRS&#39;:None,&#39;NODATA&#39;:None,
    &#39;ALPHA_BAND&#39;:False,
    &#39;CROP_TO_CUTLINE&#39;:True,
    &#39;KEEP_RESOLUTION&#39;:False,&#39;SET_RESOLUTION&#39;:False,&#39;X_RESOLUTION&#39;:None,
    &#39;Y_RESOLUTION&#39;:None,
    &#39;MULTITHREADING&#39;:False,&#39;OPTIONS&#39;:&#39;&#39;,
    &#39;DATA_TYPE&#39;:0,
    &#39;EXTRA&#39;:&#39;&#39;,
    &#39;OUTPUT&#39;:os.path.join(OUTPATH, &#39;RCP_avg_&#39; + j + &#39;.tif&#39;)})
    iface.messageBar().pushMessage(
        &#39;Success:&#39;, &#39;Output file written at &#39;, level=Qgis.Success)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our output would save and display the contour files with RCP_avg_{return_period} where return period ranges from [2,5,10,25,50]&lt;/p&gt;
&lt;p&gt;The code first fetches our shapefile, which is used to&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;create temporary TIN interpolation rasters&lt;/li&gt;
&lt;li&gt;clipped to india boundary using &lt;code&gt;clip raster by mask layer&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once we have the rasters for each return period, we style the raster using singleband pseudocolor in &lt;code&gt;Equal Interval&lt;/code&gt; mode ranging from 1.0 - 1.8 in steps of 0.1&lt;/p&gt;
&lt;p&gt;We make a copy of the raster layer and place it above it, giving it a contour style at an interval of 0.1&lt;/p&gt;
&lt;p&gt;We copy each return period and set the styling to be of contour as seen in the figure. This allows for a better visual representation of the regions with same the values.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tz0DP0k.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.8 -Styling the copy of surface raster&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The final output can be seen in the below figure.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/fiX9RA9.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.9 -Final output with contours overlaid on top of surface themself&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;final-comments&#34;&gt;Final comments&lt;/h2&gt;
&lt;p&gt;We looked at various spatial interpolation technique and automated workflow to derive spatially interpolated surface raster.&lt;/p&gt;
&lt;p&gt;Sources:&lt;/p&gt;
&lt;p&gt;a. &lt;a href=&#34;https://www.intechopen.com/chapters/52704&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comparison of Spatial Interpolation Techniques Using Visualization and Quantitative Assessment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;b. &lt;a href=&#34;https://docs.qgis.org/3.16/en/docs/gentle_gis_introduction/spatial_analysis_interpolation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spatial Analysis QGIS&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Query Geoserver Layer using openlayers</title>
      <link>https://amanbagrecha.github.io/post/geoserver/geoserver-query-builder/</link>
      <pubDate>Fri, 16 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/geoserver/geoserver-query-builder/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This blog demonstrates how to display and query all geoserver layers or from a workspace using geoserver REST API. CQL (Common Query Language) filter provided by geoserver is used to query the layer.&lt;/p&gt;
&lt;p&gt;We create a full stack application, setting up the backend using django and the frontend using vanilla js. The application will later be deployed on aws ec2 instance.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-the-backend-django&#34;&gt;Setting up the backend (Django)&lt;/h2&gt;
&lt;h3 id=&#34;create-virtual-environment-and-activate-it&#34;&gt;Create virtual environment and activate it&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create --name djangoEnv
conda activate djangoEnv
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;start-a-new-project-and-create-app&#34;&gt;Start a new project and create app&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;django-admin startproject DOGP
python manage.py startapp gisapp
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setup-the-database&#34;&gt;Setup the database&lt;/h3&gt;
&lt;p&gt;We set up postgresql for this exercise. Create a new database and add a postgis extension from it. For more info on how to set up the extension, click here.&lt;/p&gt;
&lt;p&gt;Once the database is set up on the localhost server, we make changes to the settings.py module in our application.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# change database

DATABASES = {
	&#39;default&#39;: {
		 &#39;ENGINE&#39;: &#39;django.contrib.gis.db.backends.postgis&#39;,
		 &#39;NAME&#39;: &#39;DOGP&#39;, # our new database name
		 &#39;USER&#39;: &#39;postgres&#39;,
		&#39;PASSWORD&#39;: &#39;1234&#39;,
		&#39;HOST&#39;: &#39;127.0.0.1&#39;,
		&#39;PORT&#39;: &#39;5432&#39;,
	},
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;add-installed-apps&#34;&gt;Add installed apps&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
INSTALLED_APPS = [
    &#39;gisapp.apps.GisappConfig&#39;,
    &#39;django.contrib.gis&#39;,
]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are other setups such as setting up login page and authentication, creating media url root and setting up the url which we are not going to deal with in this blog post.&lt;/p&gt;
&lt;p&gt;Once the setup is done, we run migrations to reflect those changes in the admin page.&lt;/p&gt;
&lt;p&gt;On running &lt;code&gt;python manage.py runserver&lt;/code&gt; you should see this page.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/zSSpoqV.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Page indicating successful installation of Django&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;p&gt;Our focus will be on the frontend, but the full code can be accessed from &lt;a href=&#34;https://github.com/amanbagrecha/openlayers-geoserver-query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For querying and displaying layers from geoserver, we first need geoserver installed and running. For more info on how to do that can be found &lt;a href=&#34;https://docs.geoserver.org/master/en/user/installation/win_binary.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the following steps we setup our basemap layer to be ESRI World Imagery and define an empty vector layer to store the result of query.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// map setup
var maplayer = 	new ol.layer.Tile({
    source: new ol.source.XYZ({
	  attributions: [&#39;Powered by Esri&#39;,&#39;Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community&#39;],
	  attributionsCollapsible: false,
	  url: &#39;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}&#39;,
	  maxZoom: 23
	}),
	zIndex: 0
  })

var view = new ol.View({
	projection: &#39;EPSG:4326&#39;,
	center: [-103.32989447589996, 44.18118547387081],
	zoom: 7,
  });
  
var map = new ol.Map({
	layers: [ maplayer],
	target: &#39;map&#39;,
	view: view,
  });

// define empty vector layer to store query result later
var SearchvectorLayerSource =  new ol.source.Vector({
	  
	})
var SearchvectorLayer = new ol.layer.Vector({
	source:SearchvectorLayerSource
  });
  map.addLayer(SearchvectorLayer);

// define headers for authentication and login
MyHeaders = {&#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Access-Control-Allow-Credentials&#39; : true,
				&#39;Access-Control-Allow-Origin&#39;:&#39;*&#39;,
				&#39;Accept&#39;: &#39;application/json&#39;,
				&#39;Authorization&#39;: &#39;Basic &#39; + btoa(&#39;admin:geoserver&#39;)}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To access all layers from a particular workspace, the api end point to do that is as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// https://docs.geoserver.org/latest/en/api/#1.0.0/layers.yaml
/workspaces/{workspaceName}/layers
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see this in action, we display all layers from the &lt;code&gt;sf&lt;/code&gt; workspace, provided in geoserver by default.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/sSjlZkU.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -Geoserver layers from sf workspace&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
var layerList = []; // array to store all the layer info
var loginInfo = [&amp;quot;admin&amp;quot;, &amp;quot;geoserver&amp;quot;]; // username and password for geoserver
var geoserverURL = geoserver_ip + &amp;quot;:&amp;quot; + geoserver_port  

// make ajax call to access the sf layer
$.ajax({
    url: geoserverURL + &#39;/geoserver/rest/workspaces/sf/layers/&#39;,
    type: &#39;GET&#39;,
    dataType: &#39;json&#39;,
    contentType: &amp;quot;application/json&amp;quot;,
    beforeSend: function(xhr) {
         xhr.setRequestHeader (&amp;quot;Authorization&amp;quot;, &amp;quot;Basic &amp;quot; + btoa(loginInfo[0] + &amp;quot;:&amp;quot; + loginInfo[1]));
    },
    success: function(data){
        for (var i = 0; i &amp;lt; data.layers.layer.length; i++) {
            layerList.push([data.layers.layer[i].name, data.layers.layer[i].href]);
        }

    },
    async: false
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of this ajax call returns us a &lt;code&gt;layerList&lt;/code&gt; array containing all the layer name and the url associated with it of size (:, 2)&lt;/p&gt;
&lt;p&gt;This layer can then be displayed on the frontend by looping over the array and inserting into the div element.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ZDctdje.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -The layers of workspace `sf` displayed on the map with some styles applied to it
&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;p&gt;The next step after displaying all the layers of the workspace is to load the features of the layer on selecting a particular layer.&lt;/p&gt;
&lt;p&gt;When the layer is ticked we send a request to geoserver to load the features of that layer and add to the map. If the layer is then unticked, we do the opposite and remove the layer from map.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;  function toggleLayer(input) {
	  if (input.checked) {
		  wmsLayer = new ol.layer.Image({
			source: new ol.source.ImageWMS({
			  url: geoserver_ip+ &#39;:&#39;+geoserver_port + &amp;quot;/geoserver/wms&amp;quot;,
			  imageLoadFunction: tileLoader,
			  params: { LAYERS: input.value },
			  serverType: &amp;quot;geoserver&amp;quot;,
			}),
			name: input.value,
		  });

		map.addLayer(wmsLayer);
					
	  } else {
		  map.getLayers().forEach(layer =&amp;gt; {
			  if (layer.get(&#39;name&#39;) == input.value) {
				 map.removeLayer(layer);
			 }
		 });
	  }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Wgy2YV5.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -Displaying layer on map&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;query-layer&#34;&gt;Query layer&lt;/h3&gt;
&lt;p&gt;We start with the querying the layer by their attributes. We load all the attributes (as columns) and display as dropdown. We use &lt;code&gt;wfs&lt;/code&gt; service and &lt;code&gt;DescribeFeatureType&lt;/code&gt; request to load the attributes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;  function loadprops(layername) {
	  selectedLayer = layername;
	  fetch(
		geoserver_ip+ &#39;:&#39;+geoserver_port+&amp;quot;/geoserver/wfs?service=wfs&amp;amp;version=2.0.0&amp;amp;request=DescribeFeatureType&amp;amp;typeNames=&amp;quot; + 
		  layername +
		  &amp;quot;&amp;amp;outputFormat=application/json&amp;quot;,
		{
		  method: &amp;quot;GET&amp;quot;,
		  headers: MyHeaders,
		}
	  )
		.then(function (response) {
		  return response.json();
		})
		.then(function (json) {
			var allprops = json.featureTypes[0].properties;
		  var ColumnnamesSelect = document.getElementById(&amp;quot;Columnnames&amp;quot;);
			  ColumnnamesSelect.innerHTML = &#39;&#39;
			for (i = 0; i &amp;lt; allprops.length; i++){
				if (allprops[i].name != &#39;the_geom&#39;) {
					ColumnnamesSelect.innerHTML +=
					  &#39;&amp;lt;option value=&amp;quot;&#39; +
					  allprops[i].name +
					  &#39;&amp;quot;&amp;gt; &#39; +
					  allprops[i].name +
					  &amp;quot;&amp;lt;/option&amp;gt;&amp;quot;;
				}
  
			}
		});
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upto this point we have the layer and its features we want to search for. To query the layer we make a fetch call to ows service protocol and pass in the values of feature and the layer we want to query for.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;CQL_filter = column_name + &amp;quot; = &#39;&amp;quot; + query_value + &amp;quot;&#39;&amp;quot;;
  query_url =geoserver_ip+ &#39;:&#39;+geoserver_port + &amp;quot;/geoserver/sf/ows?service=WFS&amp;amp;version=1.0.0&amp;amp;request=GetFeature&amp;amp;typeName=&amp;quot; + selectedLayer +	&amp;quot;&amp;amp;CQL_FILTER=&amp;quot; +	CQL_filter +  &amp;quot;&amp;amp;outputFormat=application%2Fjson&amp;quot;;
    		
  fetch_search_call(query_url).catch((error) =&amp;gt; {
  CQL_filter = column_name + &amp;quot;%20&amp;quot; + &amp;quot;ILIKE&amp;quot; + &amp;quot;%20%27%25&amp;quot; + query_value + &amp;quot;%25%27&amp;quot;;
	});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We define a fetch_search_call function which makes a request to ows service and returns a geojson. We can parse the geojson and display it on the map.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;  
function fetch_search_call(query_url){

	fetch_result = fetch(query_url, {
		method: &amp;quot;GET&amp;quot;,
		headers: MyHeaders,
	  })
		.then(function (response) {
		  return response.json();
		})
		.then(function (json) {
		
				SearchvectorLayerSource.clear()
				SearchvectorLayerSource.addFeatures(
			  new ol.format.GeoJSON({
			  }).readFeatures(json)
			  );
			  if(json.features.length!=0){
			  $(&#39;#searchModal&#39;).modal(&#39;toggle&#39;);
			  }

			SearchvectorLayer.set(&#39;name&#39;,&#39;search_polygon_layer&#39;)
			map.getView().fit(SearchvectorLayerSource.getExtent(),  { duration: 1590, size: map.getSize(), padding: [10, 10, 13, 15], maxZoom:16});
			
		return fetch_result
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function queries a feature and adds it to the map as a new layer. If the search is successful, we are zoomed into that location and only the feature queried gets displayed.
If the fetch call could not find the match it returns an error which is caught by &lt;code&gt;catch&lt;/code&gt; and displays the error to the client.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/syqvzq0.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.5 -Displaying Queried layer by attribute value&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This completes the blog on how to query layer and display on the map. Visit the &lt;a href=&#34;https://github.com/amanbagrecha/openlayers-geoserver-query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;github page&lt;/a&gt; to find the working application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Validating LULC classes in QGIS</title>
      <link>https://amanbagrecha.github.io/post/qgis/validating-lulc-classes-in-qgis/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/qgis/validating-lulc-classes-in-qgis/</guid>
      <description>&lt;h2 id=&#34;the-problem-statement&#34;&gt;&lt;strong&gt;The problem statement&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Any land-use land cover classification needs to be validated with ground-truth data to measure the accuracy. A key single-valued statistic to determine the effectiveness of classification is Cohen’s kappa. This validation metric has been fairly widely used for unbalanced classification as well which expresses a level of agreement between two annotators on a classification problem.&lt;/p&gt;
&lt;p&gt;The objective of this quality assessment was to validate the land cover map performed on June, 2020 sentinel-2 imagery by k-means classification algorithm, thus providing a statistical measure of overall class predictions. The validation was done using an independent set of sample points (~500) generated randomly following stratified random sampling design, to capture the variance within the class&lt;/p&gt;
&lt;p&gt;After running the tool, the sample points were manually assigned to the ground-truth class. The ground-truth dataset was taken to be Bing-satellite imagery as a proxy for field data. Each sample point was labelled by visual inspection on the ground-truth dataset.&lt;/p&gt;
&lt;h2 id=&#34;step-1-classify-image&#34;&gt;&lt;strong&gt;Step 1: Classify Image&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Load raster Image&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;K-means clustering for grids&lt;/code&gt; under SAGA tools. Select the raster Image as &lt;code&gt;grid&lt;/code&gt; and in this case we specify 4 classes&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xdx5Tsn.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -K-means clustering on sentinel-2 Image&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;code&gt;Run&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;At this stage we have unsupervised k-means clustering output ready &lt;code&gt;(Fig.2)&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/eW0cOXE.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -classification of RR Nagar, Bengaluru. Classes- Forest, Urban, water, Bareland&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-2-convert-to-polygon-vector-format&#34;&gt;&lt;strong&gt;Step 2: Convert to polygon (vector format)&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Select &lt;code&gt;Polygonize (Raster to Vector)&lt;/code&gt; tool under &lt;code&gt;GDAL&lt;/code&gt;-&amp;gt;&lt;code&gt;Raster Conversion&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Select the classified image as input. Leave everything else as default. The output would be a &lt;code&gt;Vectorised&lt;/code&gt; scratch layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/36nk2tF.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -Convert Raster to vector&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Note the name of the field (&lt;code&gt;DN&lt;/code&gt; here). This will be used later.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Fix geometries (this step is important here to avoid any error in further steps) &lt;code&gt;Vector Geometry&lt;/code&gt;-&amp;gt;&lt;code&gt;Fix Geometry&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/gG9gBIc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -Fixing topology issues with &lt;code&gt;Fix Geometry&lt;/code&gt; Toolbox&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-3-dissolve-the-layer-on-dn-field&#34;&gt;&lt;strong&gt;Step 3: Dissolve the layer on DN field&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this step we dissolve the layer based on the &lt;code&gt;DN&lt;/code&gt; value. This will ensure that each polygon can be evaluated based on the land class type which is needed for stratified random sampling.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/gm1ihfT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.5 -&lt;code&gt;Dissolve&lt;/code&gt; toolbox to dissolve polygon on &lt;code&gt; DN &lt;/code&gt; value&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Make sure to select dissolve field as &lt;code&gt;DN&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-4-create-stratified-random-samples&#34;&gt;&lt;strong&gt;Step 4: Create stratified random samples&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Go to &lt;code&gt;Vector-&amp;gt;research tools-&amp;gt; Random Points inside Polygon&lt;/code&gt; and set &lt;code&gt;Sampling Strategy&lt;/code&gt; = &lt;code&gt;Points Density&lt;/code&gt; and &lt;code&gt;Point count or density&lt;/code&gt; = &lt;code&gt;0.001&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note: The value &lt;code&gt;0.001&lt;/code&gt; signify &lt;code&gt;1&lt;/code&gt; point for &lt;code&gt;1/0.001&lt;/code&gt; m2 of area, given that the units is meters.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/1LB6R5L.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.6 - One sample point is generated for each 1000 m2 of area&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-5-extract-raster-values-to-sample-layer&#34;&gt;&lt;strong&gt;Step 5: Extract raster values to sample layer&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We extract the raster value, which is essentially the land cover class for the classified image. We use &lt;code&gt;Sample Raster Values&lt;/code&gt; function here (&lt;code&gt;Fig.7&lt;/code&gt;). The input layer is the random points we generated earlier and the the raster layer is the classified image. The output adds a new column to the sample points layer with the prediction class of the land-cover (&lt;code&gt;Fig.8&lt;/code&gt;).&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/s2RXNOZ.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.7 -Running &lt;code&gt;Sample Raster Value&lt;/code&gt; to extract Raster values for the input points&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/GG9wgNK.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.8 -The corresponding Attribute Table with Predicted Class &lt;code&gt; PREDICTED_1&lt;/code&gt; for each feature&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-6-ground-truth-labelling-using-bing-maps&#34;&gt;&lt;strong&gt;Step 6: Ground Truth Labelling using Bing maps&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At this stage we are ready to validate the image using Bing maps as ground truth. We turn on the edit mode and create new field named Actual class. THen we visually inspect the class on the map and note the land-cover class. Once we inspect all the sample points we can use cohens Kappa statistics to determine the validation result. Alternatively, simply calculating the accuracy would also suffice the need.&lt;/p&gt;
&lt;h2 id=&#34;step-7-add-other-field-to-the-attribute-table-with-reclassification&#34;&gt;&lt;strong&gt;Step 7: Add other field to the attribute table with reclassification&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We can use the &lt;code&gt;Field Calculator&lt;/code&gt; to generate verbose text for each label in our feature class and display labels for the prediction.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;-- in field calculator to increase verbosity
CASE WHEN PREDICTED_1 is 2 THEN &#39;Urban&#39; 
WHEN PREDICTED_1 is 1 THEN &#39;Bareland&#39;
WHEN PREDICTED_1 is 4 THEN &#39;Forest&#39;
WHEN PREDICTED_1 is 3 THEN &#39;Urban&#39;
END
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/3CBAK6X.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.9 -Predicted classes (foreground) vs ground truth (background)&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;With this we come to end of the post. Now, validation accuracy can be reported for k-means classification.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geocoding using Mapbox API with Zoom-in map functionality</title>
      <link>https://amanbagrecha.github.io/post/openlayers/geocode-using-mapbox-api-with-zoom-functionality/</link>
      <pubDate>Mon, 24 May 2021 20:04:53 +0530</pubDate>
      <guid>https://amanbagrecha.github.io/post/openlayers/geocode-using-mapbox-api-with-zoom-functionality/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The big picture of this post can be related to google maps, wherein you type the address and it zooms in to the location of interest. We replicate this exact functionality with mapbox API for geocoding and openlayers for client side zoom to the address of interest.&lt;/p&gt;
&lt;h2 id=&#34;main-steps&#34;&gt;Main steps&lt;/h2&gt;
&lt;p&gt;This blog demonstrates how to geocode an address using mapbox api implemented in openlayers v6. Additionally zoom in to the search location as text provided on the search bar.
This one page appication demostrates only key elements, rest of the customisation is at discretion of the viewer.&lt;/p&gt;
&lt;h3 id=&#34;setup-the-project&#34;&gt;Setup the project&lt;/h3&gt;
&lt;p&gt;We first create basic single html file to include all elements (javascript, css and html). Ideally, when the application scales, you would create a seperate file for each component.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create &lt;code&gt;html&lt;/code&gt; file and add basic elements&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;
 &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/css/ol.css&amp;quot; type=&amp;quot;text/css&amp;quot;&amp;gt;
 &amp;lt;style type=&amp;quot;text/css&amp;quot;&amp;gt;
.autocomplete {
  position: relative;
  display: inline-block;
}
input {
  border: 1px solid transparent;
  background-color: #f1f1f1;
  padding: 10px;
  font-size: 16px;
}
input[type=text] {
  background-color: #f1f1f1;
  width: 100%;
}
input[type=submit] {
  background-color: DodgerBlue;
  color: #fff;
  cursor: pointer;
}
 &amp;lt;/style&amp;gt;
 &amp;lt;/head&amp;gt;
 &amp;lt;body&amp;gt;
&amp;lt;!--create search bar for geocoding and style it --&amp;gt;
&amp;lt;h2&amp;gt;Autocomplete&amp;lt;/h2&amp;gt;
&amp;lt;br&amp;gt;
&amp;lt;form  method=&amp;quot;post&amp;quot; &amp;gt;
  &amp;lt;div class=&amp;quot;autocomplete&amp;quot; style=&amp;quot;width:300px;&amp;quot;&amp;gt;
	&amp;lt;input id=&amp;quot;myInput&amp;quot; type=&amp;quot;text&amp;quot; name=&amp;quot;myCountry&amp;quot; placeholder=&amp;quot;Country&amp;quot;&amp;gt;
  &amp;lt;/div&amp;gt;
  &amp;lt;input type=&amp;quot;submit&amp;quot; id = &amp;quot;geocodingSubmit&amp;quot;&amp;gt;
&amp;lt;/form&amp;gt;
&amp;lt;div id=&#39;project_map&#39;, class=&amp;quot;map&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;script src=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/build/ol.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;!-- &amp;lt;script src=&amp;quot;https://cdnjs.cloudflare.com/ajax/libs/proj4js/2.5.0/proj4.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; --&amp;gt;
&amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
	 
	 // create basemap layer
	 var project_maplayer = new ol.layer.Tile({
	// source: new ol.source.OSM(),
	source: new ol.source.XYZ({
		attributions: [&#39;Powered by Esri&#39;,
									 &#39;Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community&#39;],
		attributionsCollapsible: false,
		url: &#39;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}&#39;,
		maxZoom: 23
	}),
	zIndex: 0
});

// create view for the layer
var project_view = new ol.View({
	projection: &#39;EPSG:4326&#39;,
	center: [-81.80808208706726, 27.285095000261222],
	zoom: 7,
});

// add the basemap to the map
var Projectmap = new ol.Map({
	layers: [project_maplayer,],
	target: &#39;project_map&#39;,
	view: project_view,
    constrainOnlyCenter: true,
});
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We added the following elements,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Search bar: we setup the search function to input values as address and wrap it within a form with post request.&lt;/li&gt;
&lt;li&gt;Map : the div element with &lt;code&gt;id=&amp;quot;project_map&amp;quot;&lt;/code&gt; holds the map element and the script does the following. First, create layer with ESRI basemap. Second, add the layer to the Map object.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this stage the application looks like the following image&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/joNvjw7.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;add-autocomplete-functionality&#34;&gt;Add autocomplete functionality&lt;/h2&gt;
&lt;p&gt;We fetch from the api and populate our top results in a list format on key press. Also, we style the search bar using css.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;style&amp;gt;
.autocomplete-items {
  position: absolute;
  border: 1px solid #d4d4d4;
  border-bottom: none;
  border-top: none;
  z-index: 99;
  /*position the autocomplete items to be the same width as the container:*/
  top: 100%;
  left: 0;
  right: 0;
}

.autocomplete-items div {
  padding: 10px;
  cursor: pointer;
  background-color: #fff; 
  border-bottom: 1px solid #d4d4d4; 
}

/*when hovering an item:*/
.autocomplete-items div:hover {
  background-color: #e9e9e9; 
}

/*when navigating through the items using the arrow keys:*/
.autocomplete-active {
  background-color: DodgerBlue !important; 
  color: #ffffff; 
}
&amp;lt;/style&amp;gt;

&amp;lt;script&amp;gt;
myHeaders =  {&#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Access-Control-Allow-Credentials&#39; : true,
					&#39;Access-Control-Allow-Origin&#39;:&#39;*&#39;,
					&#39;Accept&#39;: &#39;application/json&#39;}

function autocomplete(inp) {
  /*the autocomplete function takes one argument,
  the text field element*/
  var currentFocus;
  /*execute a function when someone writes in the text field:*/
  inp.addEventListener(&amp;quot;input&amp;quot;, function(e) {
	  var a, b, i, val = this.value;
	  var ACCESS_TOKEN_KEY = &#39;your_token_here&#39;
	  /*close any already open lists of autocompleted values*/
	  var URL = `https://api.mapbox.com/geocoding/v5/mapbox.places/${val}.json?access_token=${ACCESS_TOKEN_KEY}&amp;amp;types=address,region,poi,country,district,locality,neighborhood,postcode&amp;amp;country=us`
	 
	  fetch(URL,{
		method: &#39;GET&#39;,
		headers: myHeaders,
	  }).then(response =&amp;gt; response.json())
	  .then(data =&amp;gt; {
		geocode_data = data;
		// console.log(data) 
	  
	  closeAllLists();
	  if (!val) { return false;}
	  currentFocus = -1;
	  /*create a DIV element that will contain the items (values):*/
	  a = document.createElement(&amp;quot;DIV&amp;quot;);
	  a.setAttribute(&amp;quot;id&amp;quot;, this.id + &amp;quot;autocomplete-list&amp;quot;);
	  a.setAttribute(&amp;quot;class&amp;quot;, &amp;quot;autocomplete-items&amp;quot;);
	  /*append the DIV element as a child of the autocomplete container:*/
	  this.parentNode.appendChild(a);
	  /*for each item in the array...*/
	  for (i = 0; i &amp;lt; geocode_data.features.length; i++) {

		  b = document.createElement(&amp;quot;DIV&amp;quot;);
		  /*insert a input field that will hold the current array item&#39;s value:*/
		  b.innerHTML += geocode_data.features[i].place_name;
		  b.innerHTML += `&amp;lt;input type=&#39;hidden&#39; style=&amp;quot;display: none;&amp;quot; id=${i}-center-cc  
		  coordinates=&#39;${geocode_data.features[i].center}&#39; value=&#39;${geocode_data.features[i].place_name}&#39;&amp;gt;`;
		  
		  /*execute a function when someone clicks on the item value (DIV element):*/
		  b.addEventListener(&amp;quot;click&amp;quot;, function(e) {
			  /*insert the value for the autocomplete text field:*/
			  var input_tag = this.getElementsByTagName(&amp;quot;input&amp;quot;)[0]
			  inp.value = input_tag.value;
			  inp.setAttribute(&amp;quot;coordinates&amp;quot;, input_tag.getAttribute(&#39;coordinates&#39;));

			  /*close the list of autocompleted values,
			  (or any other open lists of autocompleted values:*/
			  closeAllLists();
		  });
		  a.appendChild(b);
		}

	  })
	  .catch(error =&amp;gt; {
	console.error(&#39;There has been a problem with your fetch operation:&#39;, error);
	});


	  });
  // });
  /*execute a function presses a key on the keyboard:*/
  inp.addEventListener(&amp;quot;keydown&amp;quot;, function(e) {
	  var x = document.getElementById(this.id + &amp;quot;autocomplete-list&amp;quot;);
	  if (x) x = x.getElementsByTagName(&amp;quot;div&amp;quot;);
	  if (e.keyCode == 40) {
		/*If the arrow DOWN key is pressed,
		increase the currentFocus variable:*/
		currentFocus++;
		/*and and make the current item more visible:*/
		addActive(x);
	  } else if (e.keyCode == 38) { //up
		/*If the arrow UP key is pressed,
		decrease the currentFocus variable:*/
		currentFocus--;
		/*and and make the current item more visible:*/
		addActive(x);
	  } else if (e.keyCode == 13) {
		/*If the ENTER key is pressed, prevent the form from being submitted,*/
		e.preventDefault();
		if (currentFocus &amp;gt; -1) {
		  /*and simulate a click on the &amp;quot;active&amp;quot; item:*/
		  if (x) x[currentFocus].click();
		}
	  }
  });
  function addActive(x) {
	/*a function to classify an item as &amp;quot;active&amp;quot;:*/
	if (!x) return false;
	/*start by removing the &amp;quot;active&amp;quot; class on all items:*/
	removeActive(x);
	if (currentFocus &amp;gt;= x.length) currentFocus = 0;
	if (currentFocus &amp;lt; 0) currentFocus = (x.length - 1);
	/*add class &amp;quot;autocomplete-active&amp;quot;:*/
	x[currentFocus].classList.add(&amp;quot;autocomplete-active&amp;quot;);
  }
  function removeActive(x) {
	/*a function to remove the &amp;quot;active&amp;quot; class from all autocomplete items:*/
	for (var i = 0; i &amp;lt; x.length; i++) {
	  x[i].classList.remove(&amp;quot;autocomplete-active&amp;quot;);
	}
  }
  function closeAllLists(elmnt) {
	/*close all autocomplete lists in the document,
	except the one passed as an argument:*/
	var x = document.getElementsByClassName(&amp;quot;autocomplete-items&amp;quot;);
	for (var i = 0; i &amp;lt; x.length; i++) {
	  if (elmnt != x[i] &amp;amp;&amp;amp; elmnt != inp) {
		x[i].parentNode.removeChild(x[i]);
	  }
	}
  }
  /*execute a function when someone clicks in the document:*/
  document.addEventListener(&amp;quot;click&amp;quot;, function (e) {
	  closeAllLists(e.target);
  });
}

/*initiate the autocomplete function on the &amp;quot;myInput&amp;quot; element */
autocomplete(document.getElementById(&amp;quot;myInput&amp;quot;));

&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following is the explanation of the code&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;autocomplete function: The function takes an element as input which needs to be populated. Then we add an event listner which on change in input field, triggers. A GET request is sent across for the input typed and the result is populated in a form of dropdown. We add some styling on key-down so as to select the search.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, with correct mapbox api access key, we have built the autocomplete functionality.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ES8bnSO.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;last-steps&#34;&gt;Last steps&lt;/h2&gt;
&lt;p&gt;We now only need to implement the submit functionality. On click of submit button, the address is located on the map and zoomed in. This is done using a function we call centerMap&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;function CenterMap() {
	var [long, lat] = document.getElementById(&amp;quot;myInput&amp;quot;).getAttribute(&amp;quot;coordinates&amp;quot;).split(&amp;quot;,&amp;quot;).map(Number)
    console.log(&amp;quot;Long: &amp;quot; + long + &amp;quot; Lat: &amp;quot; + lat);
    Projectmap.getView().setCenter(ol.proj.transform([long, lat], &#39;EPSG:4326&#39;, &#39;EPSG:4326&#39;));
    Projectmap.getView().setZoom(5);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we add the centerMap function on click of submit&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;document.getElementById(&amp;quot;geocodingSubmit&amp;quot;).addEventListener(&#39;click&#39;, function(e){

	e.preventDefault();
	CenterMap()
})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The associated running application can be found &lt;a href=&#34;https://amanbagrecha.github.io/mapbox-search-functionality/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Upload Multiple Geotagged Images in Django</title>
      <link>https://amanbagrecha.github.io/post/django/django-image-upload/</link>
      <pubDate>Mon, 24 May 2021 17:25:54 +0530</pubDate>
      <guid>https://amanbagrecha.github.io/post/django/django-image-upload/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, we look into how to upload multiple geo-tagged/non-geotagged images to aws s3 using plain Django and spatialite as databbase. We use GeoDjango to store the latitude, longitude extracted from geo-tagged images into the database.&lt;/p&gt;
&lt;br&gt;
&lt;hr&gt;
&lt;h3 id=&#34;project-setup&#34;&gt;Project setup&lt;/h3&gt;
&lt;p&gt;create django project&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;django-admin startproject login_boiler_plate
create app python manage.py startapp GisMap
create superuser python manage.py createsuperuser
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt; add the app to &lt;code&gt;installed_app&lt;/code&gt; list and setup the default location for media storage.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
	...
	&#39;GisMap&#39;,
]

MEDIA_ROOT =  os.path.join(BASE_DIR, &#39;media&#39;) 
MEDIA_URL = &#39;/media/&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;setup-the-database-backend-to-postgis-extenstion-of-postgresql&#34;&gt;&lt;strong&gt;Setup the database backend to postgis extenstion of postgresql.&lt;/strong&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# in settings.py file
DATABASES = {
	&#39;default&#39;: {
		 &#39;ENGINE&#39;: &#39;django.contrib.gis.db.backends.postgis&#39;, #imp
		 &#39;NAME&#39;: &#39;database_name_here&#39;,
		 &#39;USER&#39;: &#39;postgres&#39;,
		&#39;PASSWORD&#39;: &#39;password_here&#39;,
		&#39;HOST&#39;: &#39;localhost&#39;,
		&#39;PORT&#39;: &#39;5432&#39;,
	},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;models.py&lt;/code&gt;, create model for uploading images. &lt;code&gt;DateTimeField&lt;/code&gt; and &lt;code&gt;user&lt;/code&gt; are not necessary.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.db import models
from django.contrib.auth.models import User


class ImageUpload(models.Model):
	user = models.ForeignKey(User, null=True, on_delete=models.CASCADE)
	image = models.ImageField( null=False, blank=False, upload_to = &#39;images/&#39;)
	date_created = models.DateTimeField(auto_now_add=True, null=True)

	def __str__(self):
		return self.user.username + &amp;quot; uploaded: &amp;quot;+ self.image.name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;forms.py&lt;/code&gt;, refer to the ImageUpload model for input.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  
from django.forms import ModelForm
from django.contrib.auth.models import User
from .models import ImageUpload

class ImageForm(ModelForm):
	class Meta:
		model = ImageUpload
		fields = (&#39;image&#39;,)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;home.html&lt;/code&gt;, create the form to accept image upload.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;                  &amp;lt;!-- Modal --&amp;gt;
                  &amp;lt;form method = &amp;quot;post&amp;quot; enctype=&amp;quot;multipart/form-data&amp;quot;&amp;gt;
                  &amp;lt;div class=&amp;quot;modal fade&amp;quot; id=&amp;quot;exampleModal&amp;quot; tabindex=&amp;quot;-1&amp;quot; role=&amp;quot;dialog&amp;quot; aria-labelledby=&amp;quot;exampleModalLabel&amp;quot; aria-hidden=&amp;quot;true&amp;quot; &amp;gt;
                    {% csrf_token %}
                    &amp;lt;div class=&amp;quot;modal-dialog&amp;quot; role=&amp;quot;document&amp;quot;&amp;gt;
                      &amp;lt;div class=&amp;quot;modal-content&amp;quot;&amp;gt;
                        &amp;lt;div class=&amp;quot;modal-header&amp;quot;&amp;gt;
                          &amp;lt;h5 class=&amp;quot;modal-title&amp;quot; id=&amp;quot;exampleModalLabel&amp;quot;&amp;gt;Upload Image&amp;lt;/h5&amp;gt;
                          &amp;lt;button type=&amp;quot;button&amp;quot; class=&amp;quot;close&amp;quot; data-dismiss=&amp;quot;modal&amp;quot; aria-label=&amp;quot;Close&amp;quot;&amp;gt;
                            &amp;lt;span aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;amp;times;&amp;lt;/span&amp;gt;
                          &amp;lt;/button&amp;gt;
                        &amp;lt;/div&amp;gt;
                        &amp;lt;div class=&amp;quot;modal-body&amp;quot;&amp;gt;
                          {{ image_form.image }}
                        &amp;lt;/div&amp;gt;
                        &amp;lt;div class=&amp;quot;modal-footer&amp;quot;&amp;gt;
                          &amp;lt;button type=&amp;quot;button&amp;quot; class=&amp;quot;btn btn-secondary&amp;quot; data-dismiss=&amp;quot;modal&amp;quot;&amp;gt;Close&amp;lt;/button&amp;gt;
                          &amp;lt;button type=&amp;quot;submit&amp;quot; class=&amp;quot;btn btn-primary&amp;quot;&amp;gt;Save Image&amp;lt;/button&amp;gt;
                        &amp;lt;/div&amp;gt;
                      &amp;lt;/div&amp;gt;
                    &amp;lt;/div&amp;gt;
                  &amp;lt;/div&amp;gt;
                  &amp;lt;/form&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, accept the HTTP POST request and save to the database. We will alter this to extract latitude, longitude later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@login_required(login_url=&#39;login&#39;)
def home_page(request):

	if request.method == &#39;POST&#39;:
		form = ImageForm(request.POST , request.FILES)
		print(form)
		if form.is_valid():
			print(&amp;quot;is valid&amp;quot;)
			obj = form.save(commit=False)
			obj.user = request.user
			obj.save()
		return redirect(&#39;home&#39;)
	else:
		Imageform = ImageForm()
		return render(request, &amp;quot;GisMap/home.html&amp;quot;, {&#39;Title&#39;: &amp;quot;Home Page&amp;quot;, &amp;quot;image_form&amp;quot;: ImageForm})

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;get-lat-lon-from-image-meta-deta-exchangeable-image-file-format-exif-&#34;&gt;Get Lat, lon from image meta deta (Exchangeable image file format [EXIF] )&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Geodjango is built on top of django and adds spatial functionality such as storing points, lines , polygon and multipolygon. It is prepackaged with Django but requires few additional softwares to make it fully functional. These include- GDAL, PROJ, GEOS, PostGIS. These can be downloaded from osgeo4W which bundles all these libraries. Then application can be added to apps in settings with &lt;code&gt;django.contrib.gis&lt;/code&gt; to the installed apps.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By default geodjango is not installed in the apps list and thus we do it ourself.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install django-geo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE- ensure os4geo is installed: install from &lt;a href=&#34;https://qgis.org/en/site/forusers/download.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; if not done.  And make the following changes in &lt;code&gt;settings.py&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;An additional setting is required, which is to locate osgeo4w directory in django. If you install osgeo4w in default directory, you need to put the following code within the settings.py file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
...
	&#39;django.contrib.gis&#39;,
]



import os
import posixpath
if os.name == &#39;nt&#39;:
	import platform
	OSGEO4W = r&amp;quot;C:\OSGeo4W&amp;quot;
	if &#39;64&#39; in platform.architecture()[0]:
		OSGEO4W += &amp;quot;64&amp;quot;
	assert os.path.isdir(OSGEO4W), &amp;quot;Directory does not exist: &amp;quot; + OSGEO4W
	os.environ[&#39;OSGEO4W_ROOT&#39;] = OSGEO4W
	os.environ[&#39;GDAL_DATA&#39;] = OSGEO4W + r&amp;quot;\share\gdal&amp;quot;
	os.environ[&#39;PROJ_LIB&#39;] = OSGEO4W + r&amp;quot;\share\proj&amp;quot;
	os.environ[&#39;PATH&#39;] = OSGEO4W + r&amp;quot;\bin;&amp;quot; + os.environ[&#39;PATH&#39;]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;models.py&lt;/code&gt;, add a PointField which can store geospatial information (lat,lon)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.contrib.gis.db import models
class ImageUpload():
  ...  
  geom = models.PointField( null=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, define functions to extract meta data from image and convert into right format for GeoDjango to understand it. Courtesy of &lt;a href=&#34;https://developer.here.com/blog/getting-started-with-geocoding-exif-image-metadata-in-python3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jayson DeLancey&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#________________________________________FUNCTIONS FOR IMAGE EXIF DATA______________________________________________________________________________#



from PIL import Image
from urllib.request import urlopen
from PIL.ExifTags import GPSTAGS
from PIL.ExifTags import TAGS

def get_decimal_from_dms(dms, ref):

	degrees = dms[0]
	minutes = dms[1] / 60.0
	seconds = dms[2] / 3600.0

	if ref in [&#39;S&#39;, &#39;W&#39;]:
		degrees = -degrees
		minutes = -minutes
		seconds = -seconds

	return round(degrees + minutes + seconds, 5)

def get_coordinates(geotags):
	lat = get_decimal_from_dms(geotags[&#39;GPSLatitude&#39;], geotags[&#39;GPSLatitudeRef&#39;])

	lon = get_decimal_from_dms(geotags[&#39;GPSLongitude&#39;], geotags[&#39;GPSLongitudeRef&#39;])

	return (lon, lat)



def get_geotagging(exif):
	if not exif:
		raise ValueError(&amp;quot;No EXIF metadata found&amp;quot;)

	geotagging = {}
	for (idx, tag) in TAGS.items():
		if tag == &#39;GPSInfo&#39;:
			if idx not in exif:
				raise ValueError(&amp;quot;No EXIF geotagging found&amp;quot;)

			for (key, val) in GPSTAGS.items():
				if key in exif[idx]:
					geotagging[val] = exif[idx][key]

	return geotagging

#_______________________________________________________________________________________________________________________________________#


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, update home_page function to extract meta data and save the image to database.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.contrib.gis.geos import Point

@login_required(login_url=&#39;login&#39;)
def home_page(request):
    if request.method == &amp;quot;POST&amp;quot;:
        form = ImageForm(request.POST, request.FILES)
        img = Image.open(request.FILES.get(&amp;quot;image&amp;quot;))
        if form.is_valid():
            try:
                obj = form.save(commit=False)
                obj.user = request.user
                obj.image_url = obj.image.url
                geotags = get_geotagging(img._getexif())
                obj.geom = Point(
                    get_coordinates(geotags)
                )  # X is longitude, Y is latitude, Point(X,Y)
                obj.save()
                messages.success(request, f&amp;quot;image uploaded succesfully&amp;quot;)
            except ValueError as e:
                messages.warning(request, e)
        else:
            messages.warning(request, f&amp;quot;Invalid image type&amp;quot;)
        return redirect(&amp;quot;home&amp;quot;)
    else:
        Imageform = ImageForm()
        return render(
            request, &amp;quot;GisMap/home.html&amp;quot;, {&amp;quot;Title&amp;quot;: &amp;quot;Home Page&amp;quot;, &amp;quot;image_form&amp;quot;: ImageForm}
        )
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;upload-to-s3-bucket&#34;&gt;Upload to S3 bucket&lt;/h2&gt;
&lt;p&gt;Install boto3 package and django-storages. Add to installed packages. Additionally, provide Key:Value AWS credentials to access the bucket and change the default file storage to S3.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install django-storages
pip install boto3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
	...
	&#39;storages&#39;,
]

AWS_ACCESS_KEY_ID = &amp;quot;&amp;quot;
AWS_SECRET_ACCESS_KEY = &amp;quot;&amp;quot;
AWS_STORAGE_BUCKET_NAME = &amp;quot;&amp;quot;

AWS_S3_FILE_OVERWRITE = False
AWS_DEFAULT_ACL = None

DEFAULT_FILE_STORAGE = &#39;storages.backends.s3boto3.S3Boto3Storage&#39;

AWS_QUERYSTRING_AUTH = False // removes the query string
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;NOTE: Make the bucket public to be able to make HTTP request&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Provide policy to make our s3 bucket public. By default, the bucket is private and no read/wrtie access is provided for user from outside the s3 page. There are other ways to access private bucket by either Limiting access to specific IP addresses or Restricting access to a specific HTTP referer. For simplicity we make the bucket public.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
  &amp;quot;Version&amp;quot;:&amp;quot;2012-10-17&amp;quot;,
  &amp;quot;Statement&amp;quot;:[
    {
      &amp;quot;Sid&amp;quot;:&amp;quot;PublicRead&amp;quot;,
      &amp;quot;Effect&amp;quot;:&amp;quot;Allow&amp;quot;,
      &amp;quot;Principal&amp;quot;: &amp;quot;*&amp;quot;,
      &amp;quot;Action&amp;quot;:[&amp;quot;s3:GetObject&amp;quot;,&amp;quot;s3:GetObjectVersion&amp;quot;],
      &amp;quot;Resource&amp;quot;:[&amp;quot;arn:aws:s3:::DOC-EXAMPLE-BUCKET/*&amp;quot;]
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;accept-non-geotagged-images&#34;&gt;Accept non-geotagged images&lt;/h2&gt;
&lt;p&gt;At this point, we should be able to upload geotagged images to s3 bucket. Non-geotagged images are not yet accepted by the model and thus we create seperate model for it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/34006994/how-to-upload-multiple-images-to-a-blog-post-in-django&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Additional resource&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We now make separate model for accepting non-geotagged images similar to &lt;code&gt;ImageUpload&lt;/code&gt; model but without &lt;code&gt;PointField&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Photos(models.Model):

	user = models.ForeignKey(User, null=True, on_delete=models.CASCADE)
	image = models.ImageField(upload_to=&#39;photos/&#39;,null=True,blank=False)
	date_created = models.DateTimeField(auto_now_add=True, null=True)
	image_url = models.URLField(max_length=250, null=True, blank=False)

	class Meta:
		verbose_name = &#39;Photo&#39;
		verbose_name_plural = &#39;Photos&#39;

	def __str__(self):
		return self.user.username + &amp;quot; uploaded image &amp;quot;+ self.image.name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt; file, extend the home_page function to add a fallback for non-geotagged images.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;if request.method == &amp;quot;POST&amp;quot;:

    # images will be in request.FILES
    post_request, files_request = request.POST, request.FILES

    form = PhotoForm(post_request or None, files_request or None)
    files = request.FILES.getlist(
        &amp;quot;images&amp;quot;
    )  # returns files: [&amp;lt;InMemoryUploadedFile: Image_name.jpg (image/jpeg)&amp;gt;, &amp;lt;InMemoryUploadedFile: Image_name.jpg (image/jpeg)&amp;gt;]
    if form.is_valid():
        user = request.user
        for f in files:

            # returns &amp;lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x360 at 0x1ED0CCC6280&amp;gt;
            img = Image.open(f)  
            
            try:
                geotags = get_geotagging(img._getexif())
                geoimage = ImageUpload(user=user, image=f)
                geoimageimg_upload.image_url = geoimage.image.url
                # X is longitude, Y is latitude, Point(X,Y) ; returns eg SRID=4326;POINT (11.88454 43.46708)
                geoimage.geom = Point(get_coordinates(geotags))
                geoimage.save()
            except:
                nongeoimage = Photos(user=user, image=f)
                nongeoimage.image_url = nongeoimage.image.url
                nongeoimage.save()
    else:
        print(&amp;quot;Form invalid&amp;quot;)
    return redirect(&amp;quot;home&amp;quot;)
else:
    Imageform = PhotoForm()
    return render(
        request, &amp;quot;GisMap/home.html&amp;quot;, {&amp;quot;Title&amp;quot;: &amp;quot;Home Page&amp;quot;, &amp;quot;image_form&amp;quot;: ImageForm}
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;accept-multiple-images&#34;&gt;Accept multiple images&lt;/h2&gt;
&lt;p&gt;Make a new form which accepts multiple image files to be uploaded at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class PhotoForm(forms.ModelForm):
	images = forms.FileField(widget=forms.ClearableFileInput(attrs={&#39;multiple&#39;: True}))

	class Meta:
		model = Photos
		fields = (&#39;images&#39;,)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;home.html&lt;/code&gt;, add &lt;code&gt;multiple&lt;/code&gt; attribute to allow for multiple selection of images at once.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;				&amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
				&amp;lt;label for=&amp;quot;note-image&amp;quot;&amp;gt;&amp;lt;/label&amp;gt;
				&amp;lt;input type=&amp;quot;file&amp;quot; name=&amp;quot;images&amp;quot; class=&amp;quot;form-control-file&amp;quot; id=&amp;quot;note-image&amp;quot; multiple&amp;gt;
				&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;final-note&#34;&gt;Final Note:&lt;/h2&gt;
&lt;p&gt;At this point, you should be able to upload multiple Images to the AWS S3 bucket and have coordinates extracted the geo-tagged images and segregate non-geotagged images.&lt;/p&gt;
&lt;p&gt;You learnt-&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to Setup GeoDjango&lt;/li&gt;
&lt;li&gt;How to Setup AWS S3 bucket&lt;/li&gt;
&lt;li&gt;How to Extract meta data from Image and store in database using PointField&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;These steps will ensure you have multiple images uploaded at once and all the geolocation information can be stored in database, which later can be import to QGIS for data visualisation. Although both postgresql and django admin allows users to visualise the data.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Django rest framework PDF creation and email via gmail SMTP and reportLab</title>
      <link>https://amanbagrecha.github.io/post/django/pdf-and-email-creation/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/django/pdf-and-email-creation/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Ever wanted to send email with attachements that too in django? And have the attachments created from the user input? This post tries to solve exactly that.&lt;/p&gt;
&lt;h2 id=&#34;main-steps&#34;&gt;Main steps&lt;/h2&gt;
&lt;p&gt;In this blog we create PDF using &lt;code&gt;Report Lab&lt;/code&gt; and email it to the user using gmail SMTP service. All actions are performed in Django.&lt;/p&gt;
&lt;h2 id=&#34;step-1--create-django-view-to-serialize-data&#34;&gt;Step 1 : create django view to serialize data&lt;/h2&gt;
&lt;p&gt;To begin with, we create a view &lt;code&gt;CreatePDF&lt;/code&gt; which accepts &lt;code&gt;POST&lt;/code&gt; request and the data gets passed onto &lt;code&gt;CreatePDFSerializer&lt;/code&gt; which serializes our data and validates it. If our data is valid, we generate PDF using &lt;code&gt;generate_pdf&lt;/code&gt; function and email to the recipent (&lt;code&gt;emailaddress&lt;/code&gt; of the users) using the &lt;code&gt;sendPDF&lt;/code&gt; function. If everything does not execute properly, we return an error response else a success.&lt;/p&gt;
&lt;p&gt;The local variable &lt;code&gt;myresponse&lt;/code&gt; is a dictionary which helps us manage the response for each &lt;code&gt;return&lt;/code&gt; statement in the correct format as expected by &lt;code&gt;response&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
SUCCESS = &#39;success&#39;
ERROR = &#39;error&#39;
message_list = [&#39;response&#39;, &#39;status&#39;, &#39;message&#39;] # eg: [&amp;quot;success&amp;quot;, 201, &amp;quot;successfully upload the file&amp;quot;]

@csrf_exempt
@api_view([&#39;POST&#39;,])
def CreatePDF(request):
    myresponse = {k: [] for k in message_list}

    try:
        myData = request.data
        # serialier the data
        serializer = serializers.CreatePDFSerializer(data=myData)  
        if serializer.is_valid():
            try:
                sendPDF(**myData.dict())  # create pdf and send email
            except Exception as e:
                RequestResponse(
                    myresponse,
                    ERROR,
                    status.HTTP_400_BAD_REQUEST,
                    {&amp;quot;Email&amp;quot;: [&amp;quot;Could not send mail!&amp;quot;]},
                )
                return Response(data=myresponse)
            
            account = serializer.save()
            RequestResponse(
                myresponse,
                SUCCESS,
                status.HTTP_201_CREATED,
                {&amp;quot;Success&amp;quot;: [f&amp;quot;Inspection Report e-mailed to {account.EmailAddress}!&amp;quot;]},
            )
            return Response(data=myresponse)

        RequestResponse(
            myresponse, ERROR, status.HTTP_400_BAD_REQUEST, serializer.errors
        )
        return Response(data=myresponse)
    
    except Exception as e:
        print(e)
        RequestResponse(
            myresponse,
            ERROR,
            status.HTTP_500_INTERNAL_SERVER_ERROR,
            {&amp;quot;Error&amp;quot;: [&amp;quot;Internal Server Error&amp;quot;]},
        )
        return Response(data=myresponse)

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;step-2-generate-pdf-using-report-lab&#34;&gt;step 2: Generate PDF using Report Lab&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt; we create a function to generate pdf using &lt;code&gt;Report Lab&lt;/code&gt; package. This allows us to define the page size and line strings with text placement to be included.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_pdf(**Mydata):
    y = 700
    buffer = io.BytesIO()  # in memory create pdf
    p = canvas.Canvas(buffer, pagesize=letter)
    p.setFont(&#39;Helvetica&#39;, 14)
    p.drawString(220, y, Mydata[&#39;Title&#39;])
    p.drawString(450, y, &#39;Date:&#39; + timezone.now().strftime(&#39;%Y-%b-%d&#39;))
    p.line(30, 675, 550, 675)
    p.drawString(220, y - 300, &#39;Time&#39;
                 + str(Mydata[&#39;time&#39;]))
    p.showPage()
    p.save()
    pdf = buffer.getvalue()
    buffer.close()
    return pdf

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;step-3-send-email-via-smtp-backend&#34;&gt;step 3: Send Email via SMTP backend&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;, we create &lt;code&gt;sendPDF&lt;/code&gt; function which calls the &lt;code&gt;generate_pdf&lt;/code&gt;to generate PDF and attaches the pdf to the email using the &lt;code&gt;EmailMessage&lt;/code&gt; class method &lt;code&gt;attach&lt;/code&gt;. We additionally need to setup backend for smtp service and host user which is to be done in &lt;code&gt;settings.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# views.py
def sendPDF(**Mydata):
	pdf = generate_pdf(**Mydata)
	msg = EmailMessage(Mydata[&#39;Title&#39;], &amp;quot; Your Report is ready! &amp;quot;, settings.EMAIL_HOST_USER, to=[Mydata[&#39;EmailAddress&#39;]])
	msg.attach(f&amp;quot;{Mydata[&#39;Title&#39;]}.pdf&amp;quot;, pdf, &#39;application/pdf&#39;)
	msg.content_subtype = &amp;quot;html&amp;quot;
	resp = msg.send()
	print(&amp;quot;resp:&amp;quot; , resp)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# settings.py
EMAIL_BACKEND = &#39;django.core.mail.backends.smtp.EmailBackend&#39;
EMAIL_HOST = &amp;quot;smtp.gmail.com&amp;quot;
EMAIL_HOST_USER = &#39;your_email@gmail.com&#39;
EMAIL_HOST_PASSWORD = &#39;your_password&#39;
EMAIL_PORT = 587
EMAIL_USE_TLS = True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have been able to successfully setup and send email with attachment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Polygon Selection and  Area Calculation in Openlayers</title>
      <link>https://amanbagrecha.github.io/post/openlayers/polygon-selection-and-area-calculation-in-openlayers/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/openlayers/polygon-selection-and-area-calculation-in-openlayers/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In this small demo-blog we look into how to make polygon selections on the map and calculate the area of that polygon on-the-fly. We use openlayers v6 for client side and geoserver to save our vector layers for this exercise.&lt;br&gt;
I assume readers to have familiarity with setting up geoserver and basics of openlayers.&lt;/p&gt;
&lt;h2 id=&#34;step-1-setup-openlayers&#34;&gt;&lt;strong&gt;Step 1: Setup openlayers&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Openlayers requires you to add these cdns to add their functionality into our application.&lt;/p&gt;
&lt;h3 id=&#34;link-necessary-cdns&#34;&gt;link necessary cdns&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;
 &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/css/ol.css&amp;quot; type=&amp;quot;text/css&amp;quot;&amp;gt;
 &amp;lt;script src=&amp;quot;https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/build/ol.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are using openlayers to render the request response. since the output of the WFS request is json, we create a new layer with vector source and format as geojson.
The &lt;code&gt;strategy:ol.loadingstrategy.bbox&lt;/code&gt; tells openlayers to only load features within the bbox. Simply put, if we move to different location, only features within that bbox will appear.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
// setup geoserver port
var geoserver_ip = &#39;http://120.0.0.1&#39;
var geoserver_port = &#39;8080&#39;

// define vector source
var myFlSource = new ol.source.Vector({
	format: new ol.format.GeoJSON(),
		url: function (extent){
			return ( geoserver_ip +&#39;:&#39; + geoserver_port + &#39;/geoserver/dronnav/ows?service=WFS&amp;amp;version=1.1.0&amp;amp;request=GetFeature&amp;amp;typeName=dronnav%3Aflorida_bp&amp;amp;maxFeatures=10000&amp;amp;outputFormat=application/json&amp;amp;srsname=EPSG:4326&amp;amp;&#39; + &#39;bbox=&#39; + extent.join(&#39;,&#39;) + &#39;,EPSG:4326&#39; );
		},
		strategy:ol.loadingstrategy.bbox,
	});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We perform WFS request from geoserver to get our layer &lt;code&gt;florida_bp&lt;/code&gt; in this case. The parameters are as explained as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;service=WFS&lt;/code&gt; : web feature service to perform interaction&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;typename=workspace:florida_bp&lt;/code&gt; : specify the workspace and layer name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;version=1.1.0&lt;/code&gt; : version number&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;maxFeatures = 10000&lt;/code&gt; : since WFS request is computationaly expensive, we restrict to only load 10000 features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;request=GetFeature&lt;/code&gt; : request type. There are several other which can be found here&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;outputFormat=application/json&lt;/code&gt; : the output format as response&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;srsname=EPSG:4326&lt;/code&gt; : coordinate reference system to display on the map&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;bbox=&lt;/code&gt; : bounding box&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// define vector layer
var floridaLayer = new ol.layer.Vector({
	source: myFlSource,
	style: new ol.style.Style({
		fill: new ol.style.Fill({
			color: &#39;rgba(1, 1, 255, .2)&#39;,
			}),
		stroke: new ol.style.Stroke({
			color: &#39;rgba(1, 1, 255, .5)&#39;,
			width: 2,
		}),
		}),
		minZoom: 16, // this will allows us to send request only when the zoom is atleast 16
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the layer is defined, we need to add this layer to the map. We can either use &lt;code&gt;map.addLayer(layername)&lt;/code&gt; or add to array in the map (&lt;code&gt;Fig.1&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// add ESRI basemap
var project_maplayer =    new ol.layer.Tile({
	source: new ol.source.XYZ({
		attributions: [&#39;Powered by Esri&#39;,
										&#39;Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community&#39;],
		attributionsCollapsible: false,
		url: &#39;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}&#39;,
		maxZoom: 23
	}),
	zIndex: 0
});

// add view with projection set to EPSG 4326 for the map
var project_view = new ol.View({
	projection: &#39;EPSG:4326&#39;,
	center: [-81.80808208706726, 27.285095000261222],
	zoom: 7,
});

// define the map with all the layers created previously
var Projectmap = new ol.Map({
	layers: [project_maplayer, floridaLayer],
	overlays: [overlay],
	target: &#39;project_map&#39;, // the div element `id` in html page
	view: project_view,
});
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/jodNbPQ.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -The Map layer with building footprints (&lt;code&gt;floridaLayer&lt;/code&gt;) added to the map with the style we specified&lt;/code&gt; for each feature&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;get-feature-info-on-click&#34;&gt;&lt;strong&gt;Get feature info on click&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;After adding basemap and our layer to the map served via geoserver, we are now ready to get information &lt;code&gt;on-click&lt;/code&gt;. We use  &lt;code&gt;forEachFeatureAtPixel&lt;/code&gt; method on our layer to send a WFS request to our geoserver and recive a response in json format. We change the style of the building on click (&lt;code&gt;Fig.2&lt;/code&gt;). The area is calculated using formatArea function which utilises &lt;code&gt;ol.sphere.getArea&lt;/code&gt; and &lt;code&gt;transform&lt;/code&gt; method to calculate area and change CRS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;
  /*  select ploygon the feature and get area and store the features */
  var selected = []; // contains all features
  var selected_area = []; // contains area of feature, one-to-one
  
  Projectmap.on(&#39;singleclick&#39;, function (e) {
  Projectmap.forEachFeatureAtPixel(e.pixel, function (f, l) {
  	var mycoordinate = e.coordinate
  	storef = f  // feature
  	/* if click is on polygon, then select the feature */
  if ( f.getGeometry()   instanceof  ol.geom.MultiPolygon ) {
		  
		var selIndex = selected.indexOf(f);
			// console.log(selIndex)
		if (selIndex &amp;lt; 0) {
			selected.push(f);
			selected_area.push( formatArea(f) ); // formatArea function returns the area in ft2
			f.setStyle(highlightStyle); // change style on click
		} else {
			selected.splice(selIndex, 1);
			selected_area.splice( selIndex, 1);
			f.setStyle(undefined);
		}
 	 }

	  })

	  /* update the tags with no of selected feature and total area combined */
	  document.getElementById(&#39;status-selected&#39;).innerHTML = &#39;&amp;amp;nbsp;&#39; + selected.length + &#39; selected features&#39;;
	  document.getElementById(&#39;status-selected_area&#39;).innerHTML = &#39;&amp;amp;nbsp;&#39; + selected_area.reduce(getSum, 0) + &#39; ft&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt;&#39;;
	  
	});
	

	
  /* style for selected feature on click  */
  var highlightStyle = new ol.style.Style({
  	fill: new ol.style.Fill({
  	color: &#39;#f0b88b&#39;,
  	}),
  	stroke: new ol.style.Stroke({
  	color: &#39;#f0b88b&#39;,
  	width: 3,
  	}),
  });


  /*  function for calculating area of the polygon (feature) selected */
  function formatArea (polygon){
   var area = ol.sphere.getArea(polygon.getGeometry().transform(&#39;EPSG:4326&#39;, &#39;EPSG:3857&#39;)); // transform to projected coordinate system.
   var output;
   output = Math.round(area * 100*10.7639) / 100  ; //in ft2
   polygon.getGeometry().transform(&#39;EPSG:3857&#39;, &#39;EPSG:4326&#39; ) //convert back to geographic crc
   return output;
  }
  
  /*  function for array sum */
  function getSum(total, num) {
  return total + Math.round(num);
  }

&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/TCicCHu.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -The &lt;code&gt;floridaLayer&lt;/code&gt; building footprints selected  with the style we specified&lt;/code&gt; for each feature&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;final-comments&#34;&gt;&lt;strong&gt;Final comments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This post demonstrates the use of &lt;code&gt;strategy:ol.loadingstrategy.bbox&lt;/code&gt; to load only the features that cover the bounding box. We use this strategy since WFS service is resouce intensive and our server cannot handle millions of HTTP request at once.&lt;/p&gt;
&lt;p&gt;We also see the use of &lt;code&gt;forEachFeatureAtPixel&lt;/code&gt; method to select our building footprints. On click of the feature we change the style using &lt;code&gt;setStyle&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Additionally, we saw how to change projection on-the-fly using &lt;code&gt;ol.sphere.getArea&lt;/code&gt; method. A word of caution while using &lt;code&gt;EPSG:3857&lt;/code&gt;. My AOI was on the equator and thus calculating area does not result in significant error. But if the AOI is in temperate zone then adopt suitable projection CRS.&lt;/p&gt;
&lt;p&gt;Layer Credit: Microsoft buidling footprints &lt;a href=&#34;https://github.com/microsoft/USBuildingFootprints&#34;&gt;https://github.com/microsoft/USBuildingFootprints&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Full Fledged CRUD application using DRF and Token Authentication</title>
      <link>https://amanbagrecha.github.io/post/django/crud-in-django-rest-framework/</link>
      <pubDate>Sat, 22 May 2021 15:13:58 +0530</pubDate>
      <guid>https://amanbagrecha.github.io/post/django/crud-in-django-rest-framework/</guid>
      <description>&lt;hr style=&#34;border:1px solid lightgray&#34;&gt; &lt;/hr&gt;
&lt;br&gt;
&lt;h2 id=&#34;what-will-you-learn&#34;&gt;What will you learn&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;color: grey;font-size: 18px;&#34;&gt;Too Long; Didn&amp;rsquo;t Read &lt;/span&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Markdown&lt;/th&gt;
&lt;th&gt;Less&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DRF&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Create API end points for CRUD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Token Authentication&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Add security and authorised access&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Fetch API calls&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Consume API from front-end&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Password Reset&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Send email to reset your forgotton password&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-step-one--basic-django-project-setup&#34;&gt;1. Step one : Basic Django Project setup&lt;/h2&gt;
&lt;p&gt;Create virtual environment&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name djangoEnv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Activate the environment&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda activate djangoEnv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install the dependencies&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install django
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in your command line&lt;/p&gt;
&lt;p&gt;create project &lt;code&gt;django-admin startproject tutorial&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;create app &lt;code&gt;python manage.py startapp Accountsapp&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;create superuser &lt;code&gt;python manage.py createsuperuser&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Now that we have the project and app installed your structure should look like this (insert picture here)&lt;/p&gt;
&lt;p&gt;Register the app in  file as follows&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Installed_apps = [ 
    &#39;Accountsapp.apps.AccountsappConfig&#39;,
    ...
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now create our own custom model named &lt;em&gt;&lt;strong&gt;MyAccounts&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;models.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.db import models
from django.contrib.auth.models import AbstractBaseUser, BaseUserManager

from django.conf import settings
from django.db.models.signals import post_save
from django.dispatch import receiver
from rest_framework.authtoken.models import Token


class MyAccountManager(BaseUserManager):
	def create_user(self, email, username, password=None):
		if not email:
			raise ValueError(&#39;Users must have an email address&#39;)
		if not username:
			raise ValueError(&#39;Users must have a username&#39;)


		user = self.model(
			email=self.normalize_email(email),
			username=username,
		)

		user.set_password(password)
		user.save(using=self._db)
		return user

	def create_superuser(self, email, username, password):
		user = self.create_user(
			email=self.normalize_email(email),
			password=password,
			username=username,
			
		)
		user.is_admin = True
		user.is_staff = True
		user.is_superuser = True
		user.save(using=self._db)
		return user

# creating custom model of &amp;quot;User&amp;quot; base model. 
class MyAccount(AbstractBaseUser):
	email 					= models.EmailField(verbose_name=&amp;quot;email&amp;quot;, max_length=60, unique=True)
	username 				= models.CharField(max_length=30, unique=True)
	date_joined				= models.DateTimeField(verbose_name=&#39;date joined&#39;, auto_now_add=True)
	last_login				= models.DateTimeField(verbose_name=&#39;last login&#39;, auto_now=True)
	is_admin				= models.BooleanField(default=False)
	is_active				= models.BooleanField(default=True)
	is_staff				= models.BooleanField(default=False)
	is_superuser			= models.BooleanField(default=False)


	USERNAME_FIELD = &#39;email&#39;   # username_field is the one which should be unique and will be compared by django for not creating multiple users with same email.

	REQUIRED_FIELDS = [&#39;username&#39;] 

	objects = MyAccountManager()

	def __str__(self):
		return self.email

	# For checking permissions. to keep it simple all admin have ALL permissons
	def has_perm(self, perm, obj=None):
		return self.is_admin

	# Does this user have permission to view this app? (ALWAYS YES FOR SIMPLICITY)
	def has_module_perms(self, app_label):
		return True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To tell django we are overwriting the default user model, we do the following&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;AUTH_USER_MODEL = Accounts.MyAccounts
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we makemigrates to register the model in our database&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python manage.py makemigrations
python manage.py migrate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And for the model to be visible in admin section we do the following&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;admin.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from django.contrib import admin
from .models import MyAccount

admin.site.register(MyAccount) # Register your models here.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For now the our project is setup. We move to Django Rest Framework setup&lt;/p&gt;
&lt;h2 id=&#34;2-setup-django-rest-framework-with-authentication&#34;&gt;2. Setup Django Rest Framework with Authentication&lt;/h2&gt;
&lt;p&gt;Install dependeny&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install djangorestframework
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like any other app, django rest framework is also an app. so we add it to the list of installed apps.
We additionally add authtoken app for user authentication which we are shortly going to intergrate in our CRUD application&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;INSTALLED_APPS = [
    # my apps
    &#39;Accountsapp.apps.AccountsappConfig&#39;,
    # restframework
    &#39;rest_framework&#39;,
    &#39;rest_framework.authtoken&#39;,
    ...
    
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are going to be using Token Authentication in this application. DRF documentation recommends it as the default.
Let Us setup the Default authentication class before actually utilising it.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;REST_FRAMEWORK = {
    &#39;DEFAULT_AUTHENTICATION_CLASSES&#39;: [
        &#39;rest_framework.authentication.TokenAuthentication&#39;,
        
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last thing before we actually start writing code is to perform migration. The &lt;code&gt;rest_framework.authtoken&lt;/code&gt; app provides Django database migrations.&lt;/p&gt;
&lt;p&gt;As done previously on command line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python manage.py makemigrations
python manage.py migrate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have completed the logistics for setting up DRF&lt;/p&gt;
&lt;h2 id=&#34;3-building-crud-application&#34;&gt;3. Building CRUD application&lt;/h2&gt;
&lt;p&gt;We would first create a folder called &lt;strong&gt;api&lt;/strong&gt; inside our to seperate codebase for API and vanila CRUD&lt;/p&gt;
&lt;p&gt;Inside API folder create four files,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__init__.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serializers.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;views.py&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;urls.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In &lt;code&gt;serializers.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rest_framework import serializers 
from Accountsapp.models import MyAccount # import our custom model


# provide fields in meta, expression and in MyAccount. for admin page login and edit,  is_admin and is_staff should be true
class RegistrationSerializer(serializers.ModelSerializer):

    # additional fields 
	password2 = serializers.CharField(style={&#39;input_type&#39;: &#39;password&#39;}, write_only=True)
	is_superuser =serializers.BooleanField(write_only=True)
	
    class Meta:
		model = MyAccount
        # mention the fields you want to display when request is sent. 
		fields = [&#39;id&#39;,&#39;email&#39;, &#39;username&#39;, &#39;password&#39;, &#39;password2&#39;,  &#39;is_superuser&#39;]
		extra_kwargs = {
				&#39;password&#39;: {&#39;write_only&#39;: True},  # tells django to not display the password for others to see
		}	


	def	save(self):

		account = MyAccount(
					email=self.validated_data[&#39;email&#39;],
					username=self.validated_data[&#39;username&#39;],
					# is_admin=self.validated_data[&#39;is_admin&#39;],
					is_superuser= self.validated_data[&#39;is_superuser&#39;],
				)
		password = self.validated_data[&#39;password&#39;]
		password2 = self.validated_data[&#39;password2&#39;]
		if password != password2:
			raise serializers.ValidationError({&#39;password&#39;: &#39;Passwords must match.&#39;})
		account.set_password(password)
		account.save()
		return account


class UpdateSerializer(serializers.ModelSerializer):

	class Meta:
		model = MyAccount
		# mention the fields you want to display when request is sent. 
		fields = [&#39;id&#39;, &#39;username&#39;, &#39;email&#39;]
		extra_kwargs = {
				&#39;password&#39;: {&#39;read_only&#39;: True},  #  password cannot be edited from here
		}


&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : Do not try to update the password from serializers. There is another technique which we will deal with in later section.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The serializers in REST framework work very similarly to Django’s Form and ModelForm classes. The two major serializers that are most popularly used are ModelSerializer and HyperLinkedModelSerialzer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rest_framework import status
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated, IsAdminUser
from django.contrib.auth import authenticate
from rest_framework.authentication import TokenAuthentication
from rest_framework.decorators import api_view, authentication_classes, permission_classes

from . import serializers 
from Accountsapp.models import MyAccount
from rest_framework.authtoken.models import Token

# user views
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from rest_framework.parsers import JSONParser
from django.core.exceptions import ObjectDoesNotExist
import json

# login {built-in django}
from django.contrib.auth import login 
from django.contrib.auth.decorators import login_required



# get all users
@api_view([&amp;quot;GET&amp;quot;])
@csrf_exempt
@permission_classes([IsAuthenticated,])
@authentication_classes([TokenAuthentication])
def get_users(request):
    try:
        user_profile = MyAccount.objects.all() 
        serializer = serializers.RegistrationSerializer(user_profile, many=True)
        return Response( {&#39;USER_PROFILE&#39;:serializer.data}, status= status.HTTP_200_OK)
    except ObjectDoesNotExist:
        return JsonResponse({&#39;Response&#39;: &#39;You do not have authorization to access this page&#39;}, status=status.HTTP_401_UNAUTHORIZED)



# get given user
@api_view([&#39;GET&#39;])
@csrf_exempt
@permission_classes([IsAuthenticated,])
@authentication_classes([TokenAuthentication])
def get_given_user(request, pk):
    try:
        user_profile = MyAccount.objects.get(pk=pk)
    except ObjectDoesNotExist:
        return JsonResponse({&amp;quot;missing&amp;quot;: &amp;quot;The requested object does not exist&amp;quot;}, status=status.HTTP_404_NOT_FOUND)

    if request.method == &#39;GET&#39;:  
        serializer = serializers.RegistrationSerializer(user_profile)
        token = Token.objects.get(user=user_profile).key
        return JsonResponse({&#39;given_user_profile&#39;: serializer.data, &#39;token&#39;:token})
   


# add user
@csrf_exempt
@api_view([&#39;POST&#39;])
def user_add_view(request):
        serializer = serializers.RegistrationSerializer( data=request.data)
        if serializer.is_valid():
            account = serializer.save()
            token, _ = Token.objects.get_or_create(user=account)
            return Response(serializer.data, status=status.HTTP_201_CREATED,  headers={&#39;Authorization&#39;: &#39;Token &#39; + token.key})
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)



# update user
@api_view([&amp;quot;PUT&amp;quot;,&#39;GET&#39;])
@csrf_exempt
@permission_classes([IsAuthenticated,])
@authentication_classes([TokenAuthentication])
def update_user(request, pk):

    try:
        user_profile = MyAccount.objects.get(id=pk)
    except ObjectDoesNotExist:
        return Response({&#39;response&#39;: &amp;quot;given object does not exist&amp;quot;}, status=status.HTTP_404_NOT_FOUND)

    user = request.user
    try:
        data =  {i:j for i,j in request.query_params.items()}
        print(data)
        serializer = serializers.UpdateSerializer(user_profile, data=data)
        if serializer.is_valid():
            user= serializer.save()
            token, _ = Token.objects.get_or_create(user=user)
            return Response({&amp;quot;response&amp;quot;: &amp;quot;success&amp;quot;, &#39;data&#39; :serializer.data}, status=status.HTTP_201_CREATED,  headers={&#39;Authorization&#39;: &#39;Token &#39; + token.key})
        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)

    except ObjectDoesNotExist as e:
        return JsonResponse({&#39;error&#39;: str(e)}, safe=False, status=status.HTTP_404_NOT_FOUND)
    except Exception:
        return JsonResponse({&#39;error&#39;: &#39;Something terrible went wrong&#39;}, safe=False, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



# delete user
@api_view([&amp;quot;DELETE&amp;quot;,&#39;GET&#39;]) 
@csrf_exempt
@permission_classes([IsAuthenticated])
@authentication_classes([TokenAuthentication])
def delete_user(request, pk):

    try:
        user_profile = MyAccount.objects.get(id=pk)
    except ObjectDoesNotExist:
        return JsonResponse({&#39;response&#39;: &amp;quot;given object does not exist&amp;quot;}, safe=False, status=status.HTTP_404_NOT_FOUND)

    user = request.user
    if user_profile != user: 
        return JsonResponse({&#39;response&#39;:&amp;quot;You don&#39;t have permission to delete the record.&amp;quot;}, safe=False, status=status.HTTP_401_UNAUTHORIZED)

    try:
        user_profile.delete()  #retuns 1 or 0
        return JsonResponse({&#39;user_delete&#39;: &amp;quot;record deleted&amp;quot;}, safe=False, status=status.HTTP_200_OK)
    except ObjectDoesNotExist as e:
        return JsonResponse({&#39;error&#39;: str(e)}, safe=False, status=status.HTTP_404_NOT_FOUND)
    except Exception:
        return JsonResponse({&#39;error&#39;: &#39;Something terrible went wrong&#39;}, safe=False, status=status.HTTP_500_INTERNAL_SERVER_ERROR)



# login view and get token
@api_view([&amp;quot;POST&amp;quot;, ])
def drflogin(request):

    email = request.data.get(&amp;quot;email&amp;quot;)
    username = request.data.get(&amp;quot;username&amp;quot;)
    password = request.data.get(&amp;quot;password&amp;quot;)
    account = MyAccount.objects.filter(email=email) | MyAccount.objects.filter(username=username)
    if not account:
        return Response({&amp;quot;error&amp;quot;: &amp;quot;Login failed&amp;quot;}, status=status.HTTP_401_UNAUTHORIZED)
    # authenticate(email=email, password=password)  # returns none if not authenticated
    account = authenticate(email=account[0].email, password=password)
    token, _ = Token.objects.get_or_create(user=account)
    login(request,account)  
    renderer= Response({&amp;quot;response&amp;quot; : &amp;quot;Successfully authenticated&amp;quot;,  &amp;quot;pk&amp;quot;: account.pk, &amp;quot;username&amp;quot;: account.username, &amp;quot;token&amp;quot;: token.key }, template_name= &amp;quot;Accountsapp/loginuser.html&amp;quot;, headers={&#39;Authorization&#39;: &#39;Token &#39; + token.key})
    return renderer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Setup end points for our API&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;views.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from django.urls import path, include
from . import views as drf_views


app_name = &#39;Accountsapp&#39;

urlpatterns = [

    path(&#39;drf_users/&#39;, drf_views.get_users, name= &#39;drf_users&#39;),
    path(&#39;drf_user/&amp;lt;int:pk&amp;gt;/&#39;, drf_views.get_given_user, name= &#39;drf_get_user&#39;),
    path(&#39;drf_updateuser/&amp;lt;int:pk&amp;gt;/&#39;, drf_views.update_user, name= &#39;drf_updateusers&#39;),
    path(&#39;drf_deleteuser/&amp;lt;int:pk&amp;gt;/&#39;, drf_views.delete_user, name= &#39;drf_deleteuser&#39;),
    path(&#39;drf_adduser/&#39;, drf_views.user_add_view, name= &#39;drf_adduser&#39;),
    path(&#39;drf_login/&#39;, drf_views.drflogin, name=&#39;drf_login&#39;),

    
]

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We first create users and then test  delete, update and show users functionality of our API. We will use Postman for timebeing. Later we will built the front-end to perform all these actions.&lt;/p&gt;
&lt;h2 id=&#34;span-style-colororange-post-span-request-add-user&#34;&gt;&lt;span style= &#34;color:orange&#34;&gt; POST &lt;/span&gt; REQUEST: &lt;strong&gt;ADD USER&lt;/strong&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_adduser/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Ea8W3Bj.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colorgreen-get-span-request-get-users&#34;&gt;&lt;span style= &#34;color:green&#34;&gt; GET &lt;/span&gt; REQUEST: &lt;strong&gt;GET USERS&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_users/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request GET &#39;http://127.0.0.1:8000/drf_users/&#39; \
--header &#39;Authorization: Token 92cc8c32edb7bd111b89552a3031f918d2df5613&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using postman&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/dPnv4J4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colorred-del-span-request-delete-user&#34;&gt;&lt;span style= &#34;color:RED&#34;&gt; DEL &lt;/span&gt; REQUEST: &lt;strong&gt;DELETE USER&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_deleteuser/&amp;lt;int:pk&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request DELETE &#39;http://127.0.0.1:8000/drf_deleteuser/21&#39; \
--header &#39;Authorization: Token 1529e77c59999f819649828a5e9174ba44bd6bb4&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using postman&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/6IFah1s.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colordodgerblue-put-span-request-update-user&#34;&gt;&lt;span style= &#34;color:dodgerblue&#34;&gt; PUT &lt;/span&gt; REQUEST: &lt;strong&gt;UPDATE USER&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_updateuser/1/?username=updated_username_here&amp;amp;email=updated_email_here
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request PUT &#39;http://127.0.0.1:8000/drf_updateuser/8/?username=rcbfl&amp;amp;email=rcbfl@gmail.com&#39; \
--header &#39;Authorization: Token 506ce0bbf7fa50f613678024586669d9b6bd82a0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;using postman
&lt;img src=&#34;https://i.imgur.com/LhVZ34L.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;span-style-colorgreen-get-span-request-get-user&#34;&gt;&lt;span style= &#34;color:green&#34;&gt; GET &lt;/span&gt; REQUEST: &lt;strong&gt;GET USER&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;API end point&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8000/drf_user/&amp;lt;int:pk&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using curl and passing authorization token&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl --location --request GET &#39;http://127.0.0.1:8000/drf_user/8&#39; \
--header &#39;Authorization: Token 506ce0bbf7fa50f613678024586669d9b6bd82a0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;using postman&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/LiPdZIe.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;front-end-setup&#34;&gt;Front end setup&lt;/h2&gt;
&lt;p&gt;In root directory create folder  &lt;code&gt;templates\Accountsapp\&lt;/code&gt; and create &lt;code&gt;RegiserUser.html&lt;/code&gt; file in it. Create form field in the file as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;          &amp;lt;form class=&amp;quot;form-horizontal&amp;quot; action=&amp;quot;&amp;quot; method=&amp;quot;post&amp;quot;  id=&amp;quot;myForm&amp;quot; autocomplete=&amp;quot;off&amp;quot;&amp;gt;
          	{% csrf_token %}
            &amp;lt;!-- Name input--&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;username&amp;quot;&amp;gt;Name&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;username&amp;quot; name=&amp;quot;username&amp;quot; type=&amp;quot;text&amp;quot; placeholder=&amp;quot;Your username&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;!-- Email input--&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;email&amp;quot;&amp;gt;Your E-mail&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;email&amp;quot; name=&amp;quot;email&amp;quot; type=&amp;quot;email&amp;quot; placeholder=&amp;quot;Your email&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;!-- password body --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;password&amp;quot;&amp;gt;Password&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;password&amp;quot; name=&amp;quot;password&amp;quot; type=&amp;quot;password&amp;quot; placeholder=&amp;quot;Your password&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;!-- password body --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;password2&amp;quot;&amp;gt;Password2&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-9&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;password2&amp;quot; name=&amp;quot;password2&amp;quot; type=&amp;quot;password&amp;quot; placeholder=&amp;quot;confirm password&amp;quot; class=&amp;quot;form-control&amp;quot;&amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
            
            &amp;lt;!-- superuser input --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;label class=&amp;quot;col-md-3 control-label&amp;quot; for=&amp;quot;superuser&amp;quot;&amp;gt;Is superuser&amp;lt;/label&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-3&amp;quot;&amp;gt;
                &amp;lt;input id=&amp;quot;issuperuser&amp;quot; name=&amp;quot;issuperuser&amp;quot; type=&amp;quot;checkbox&amp;quot;  class=&amp;quot;form-control&amp;quot; &amp;gt;
              &amp;lt;/div&amp;gt;
            &amp;lt;/div&amp;gt;
 
    
            &amp;lt;!-- Form actions --&amp;gt;
            &amp;lt;div class=&amp;quot;form-group&amp;quot;&amp;gt;
              &amp;lt;div class=&amp;quot;col-md-6 text-left&amp;quot;&amp;gt;
                &amp;lt;button type=&amp;quot;submit&amp;quot; class=&amp;quot;btn btn-primary btn-lg&amp;quot;&amp;gt;Submit&amp;lt;/button&amp;gt;
              &amp;lt;/div&amp;gt;


            &amp;lt;/div&amp;gt;
          &amp;lt;/fieldset&amp;gt;
          &amp;lt;/form&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the form is created, we now need to take the input from the form and send to the register user API &lt;code&gt;drf_adduser/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;RegisterUser.html&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;


			function getCookie(name) {
		    var cookieValue = null;
		    if (document.cookie &amp;amp;&amp;amp; document.cookie !== &#39;&#39;) {
		        var cookies = document.cookie.split(&#39;;&#39;);
		        for (var i = 0; i &amp;lt; cookies.length; i++) {
		            var cookie = cookies[i].trim();
		            // Does this cookie string begin with the name we want?
		            if (cookie.substring(0, name.length + 1) === (name + &#39;=&#39;)) {
		                cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
		                break;
		            }
		        }
		    }
		    return cookieValue;
		}
		var csrftoken = getCookie(&#39;csrftoken&#39;);



function fetchcall(event) {

		event.preventDefault();
		console.log(&#39;form submitted&#39;);
	var username = document.getElementById(&amp;quot;username&amp;quot;).value;
	var email = document.getElementById(&amp;quot;email&amp;quot;).value;
	var password = document.getElementById(&amp;quot;password&amp;quot;).value;
	var password2 = document.getElementById(&amp;quot;password2&amp;quot;).value;
	var issuperuser = document.getElementById((&#39;issuperuser&#39;)).checked;
	console.log(issuperuser)

		var url = &#39;/drf_adduser/&#39;;

			fetch(url, {
				method:&#39;POST&#39;,
				headers:{
					&#39;Content-type&#39;:&#39;application/json&#39;,
					&#39;X-CSRFToken&#39;:csrftoken,
				},
				body:JSON.stringify({
					&#39;email&#39;:email,
					&#39;username&#39;:username,
					&amp;quot;password&amp;quot;:password,
					&amp;quot;password2&amp;quot;:password2,
					&amp;quot;is_superuser&amp;quot;: issuperuser
				})
			}
			).then(function(response){
				store_response= response;
				return response.json();

			}).then(function(data){
				store_data =JSON.stringify(data);
				document.getElementById(&amp;quot;message&amp;quot;).innerHTML=  store_data;
			}).catch(function(error){
			console.error(error);
		});

	}
			
	var myForm = document.getElementById(&amp;quot;myForm&amp;quot;);

		console.log(username, password, myForm);
	myForm.addEventListener(&#39;submit&#39;, fetchcall);
	
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make this work in front-end, we need to register the file to &lt;code&gt;Accountsapp/views.py&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def register_user(request):
	# if request.user.is_authenticated:
	return render(request, &amp;quot;Accountsapp/RegisterUser.html&amp;quot;, {&#39;Title&#39;: &amp;quot;Register new user&amp;quot;})

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
