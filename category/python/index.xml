<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python | Aman Bagrecha</title>
    <link>https://amanbagrecha.github.io/category/python/</link>
      <atom:link href="https://amanbagrecha.github.io/category/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Python</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 14 Jan 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://amanbagrecha.github.io/media/icon_hub8f2c4a40c112e5edee4297ae1d00aa4_198578_512x512_fill_lanczos_center_2.png</url>
      <title>Python</title>
      <link>https://amanbagrecha.github.io/category/python/</link>
    </image>
    
    <item>
      <title>Cloud Native Composite, Subset and Processing of Satellite Imagery with STAC and Stackstac</title>
      <link>https://amanbagrecha.github.io/post/xarray/cloud-native-composite-subset-and-processing-of-satellite-imagery-with-stac-and-stackstac/</link>
      <pubDate>Sat, 14 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/xarray/cloud-native-composite-subset-and-processing-of-satellite-imagery-with-stac-and-stackstac/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;If you wanted to collect all Sentinel satellite data for a given region of interest (ROI), say, for a given day or time frame - is there any simple way to do it? That means: Without having to download all the full images manually and cropping the ROI subset manually as well afterwards?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This, well articulated question, was the one which I was facing and made me ponder to think if we could do this using STAC and Python.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I had a road network layer over which I needed satellite imagery. The problem with my road network is that it has a large spatial extent, causing a single satellite imagery to not cover it entirely. Moreover, because of this large extent, I need two adjacent tiles to be in the same Coordinate Reference System.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/FHLQbBo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 - Road network (in red) spanning multiple UTM Zones&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;What I needed was,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A way to aggregate all the adjacent tiles for a single day&lt;/li&gt;
&lt;li&gt;Convert to a single CRS on the fly&lt;/li&gt;
&lt;li&gt;Subset the data to my region&lt;/li&gt;
&lt;li&gt;Create a composite (merge) and perform analysis on the fly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It turns out Python (and its ecosystem of great geospatial packages) along with STAC allows us to do just that.&lt;/p&gt;
&lt;p&gt;What is STAC?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;STAC (SpatioTemporal Asset Catalog) is an open-source specification for describing satellite imagery and the associated metadata.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will use &lt;code&gt;stackstac&lt;/code&gt;, which is a Python package for efficiently managing and analysing large amounts of satellite imagery data in a cloud computing environment.&lt;/p&gt;
&lt;p&gt;First, we search through the sentinel-2 collection for our area of interest from element84 provided STAC endpoint.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from pystac_client import Client

URL = &#39;https://earth-search.aws.element84.com/v0/&#39;

client = Client.open(URL)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;search = client.search(
    max_items = 10,
    collections = &amp;quot;sentinel-s2-l2a-cogs&amp;quot;,
    intersects = aoi_as_multiline,
    datetime = &#39;2022-01-01/2022-01-24&#39;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resultant &lt;code&gt;search&lt;/code&gt; object is passed to &lt;code&gt;stack&lt;/code&gt; method on &lt;code&gt;stackstac&lt;/code&gt; along with providing the destination CRS, the region of bounds and the assets required.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import stackstac

ds = stackstac.stack(search.get_all_items() ,  epsg=4326, assets=[&amp;quot;B04&amp;quot;, &amp;quot;B03&amp;quot;, &amp;quot;B05&amp;quot;],
bounds_latlon= aoi_as_multiline.bounds )

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above line does a lot of things under the hood. It transforms the CRS of each tile from their native CRS to EPSG:4326. It also clips the tiles to our AOI. It also filters only 3 bands out of the possible 15 sentinel-2 bands. 
The output &lt;code&gt;ds&lt;/code&gt; is a &lt;code&gt;xarray.DataArray&lt;/code&gt; object and it is a known fact how much is possible with very little code in xarray.&lt;/p&gt;
&lt;p&gt;As such, we can group by a date and mosaic those tiles very easily using xarray as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dsf = ds.groupby(&amp;quot;time.date&amp;quot;).median()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/BLCu4xs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 - Our DataArray is 3.37GB with 4 dimensions (time, bands, x, y) respectively.&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Since xarray loads lazily, we did not perform any computation so far. But we can see how much data we are going to end up storing as shown in Figure 2.&lt;/p&gt;
&lt;p&gt;When I run the &lt;code&gt;compute&lt;/code&gt; method on the output, it does the computation in 4 minutes (here) i.e, processing ~3.5GB in 4 mins and computing the median across the dates.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;res = dsf.compute()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the end of this process, I have 4 images for each of the 4 dates, clipped to my region of interest in the CRS that I desire.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The above method of processing large volume data is super handy and can be scaled very easily with cloud infrastructure. What is unique about this approach is that I did not have to download data, convert or know the CRS of each tile, worrying about the bounds of my region of interest.
Read more about how stackstac works &lt;a href=&#34;https://stackstac.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Merging Rasters using Rasterio</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/merge-rasters-the-modern-way-using-python/</link>
      <pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/merge-rasters-the-modern-way-using-python/</guid>
      <description>&lt;p&gt;In this blog, we&amp;rsquo;ll examine how to merge or mosaic rasters using Python, the modern way. Additionally, we would look at a few nuances and internal workings of rasterio&amp;rsquo;s merge functionality along with saving your rasters in-memory.&lt;/p&gt;
&lt;p&gt;By &amp;ldquo;modern way&amp;rdquo;, it is implied that you have an improved workflow and data management. And that you can experiment with various scenarios quickly and efficiently.&lt;/p&gt;
&lt;p&gt;The traditional way to mosaic data is by downloading multiple intersecting tileset in its entirety. Downloading an entire tileset is itself a cost prohibitive task, added to already lost time in searching desired satellite imagery on GUI.&lt;/p&gt;
&lt;p&gt;To overcome these traditional challenges, there has been significant improvement in storing metadata of satellite imagery (namely &lt;a href=&#34;https://stacspec.org/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STAC&lt;/a&gt;) which has enabled querying them much smoother and made it imagery-provider agnostic.&lt;/p&gt;
&lt;h3 id=&#34;tldr&#34;&gt;TL;DR&lt;/h3&gt;
&lt;p&gt;We would perform the following task in this blog —&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use pystac to query items over our AOI&lt;/li&gt;
&lt;li&gt;Plot the tiles on map using hvplot&lt;/li&gt;
&lt;li&gt;Merge tiles without data download on local machine&lt;/li&gt;
&lt;li&gt;Save the merged tile in-memory using rasterio&amp;rsquo;s MemoryFile&lt;/li&gt;
&lt;li&gt;Internals of rasterio&amp;rsquo;s merge methods&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;problem-at-hand&#34;&gt;Problem at hand&lt;/h3&gt;
&lt;p&gt;I wish to access sentinel-2 True Color Image for the month of January over my area of interest (AOI), which is a highway network across Karnataka and Andhra Pradesh (Figure 1).&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183255775-352d47fb-515c-4d72-ba4e-e32ac5bebf42.png&#34; alt=&#34;bokeh_plot&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -
Highway Network as our Region of Interest&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We start by fetching sentinel-2 tiles over our AOI from &lt;code&gt;sentinel-s2-l2a-cogs&lt;/code&gt; STAC catalog using &lt;a href=&#34;https://github.com/stac-utils/pystac-client&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pystac-client&lt;/a&gt;. This library allows us to crawl STAC catalog and enables rapid access to the metadata we need.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# STAC API root URL
# Thanks to element84 for hosting the API for sentinel-2 catalog.
URL = &#39;https://earth-search.aws.element84.com/v0/&#39;

client = Client.open(URL)

search = client.search(
    max_items = 10,
    collections = &amp;quot;sentinel-s2-l2a-cogs&amp;quot;,
    bbox = gdf.total_bounds, # geodataframe for our region of study
    datetime = &#39;2022-01-01/2022-01-24&#39;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we search for 10 sentinel-s2-l2a-cogs over our AOI for the date between January 1 and 24 of 2022.&lt;/p&gt;
&lt;p&gt;Now we need to know which of our 10 queried images covers our area of interest in its entirety. To do that, we can plot all the search results on the map and visually inspect.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183255939-121e585e-79dc-4ceb-b61e-7e08709de926.png&#34; alt=&#34;bokeh_plot&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 -
Sentinel-2 tiles overlaid on Region of Interest&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We see that our AOI is not covered by a single tile in entirety, and that there is a need to merge adjacent tiles.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that we have so far only queried the metadata of our desired imagery&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We use rasterio&amp;rsquo;s &lt;a href=&#34;https://rasterio.readthedocs.io/en/latest/api/rasterio.merge.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;merge&lt;/a&gt; functionality, which would enable us to combine all of them seamlessly.&lt;/p&gt;
&lt;p&gt;First, we get all the tiles for a single day and look for True Color Image (TCI) band&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# retrieve the items as dictionaries, rather than Item objects
items = list(search.items_as_dicts())
# convert found items to a GeoDataFrame
items_gdf = items_to_geodataframe(items)

tiles_single_day = items_gdf.loc[&#39;2022-01-23&#39;, &amp;quot;assets.visual.href&amp;quot;]

# print(tiles_single_day)
 properties.datetime
 2022-01-23 05:25:14+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 2022-01-23 05:25:11+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 2022-01-23 05:24:59+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 2022-01-23 05:24:56+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...
 Name: assets.visual.href, dtype: object

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, read the remote files via the URL in the above output using &lt;code&gt;rasterio.open&lt;/code&gt; and save the returned file handlers as a list. This is the first instance where we are dealing with the actual imagery. Although, we are not reading the values stored in the data just yet.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;# open images stored on s3
file_handler = [rasterio.open(row) for row in tiles_single_day]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally we can merge all of the tiles and get the clipped raster stored in memory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from rasterio.io import MemoryFile
from rasterio.merge import merge

memfile = MemoryFile()

merge(datasets=file_handler, # list of dataset objects opened in &#39;r&#39; mode
    bounds=tuple(gdf.set_crs(&amp;quot;EPSG:4326&amp;quot;).to_crs(file_handler[0].crs).total_bounds), # tuple
    nodata=None, # float
    dtype=&#39;uint16&#39;, # dtype
    resampling=Resampling.nearest,
    method=&#39;first&#39;, # strategy to combine overlapping rasters
    dst_path=memfile.name, # str or PathLike to save raster
    dst_kwds={&#39;blockysize&#39;:512, &#39;blockxsize&#39;:512} # Dictionary
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are really interesting things to look at in the above code. Overall, the code above returns a &lt;code&gt;MemoryFile&lt;/code&gt; object which contains a &lt;code&gt;uint16&lt;/code&gt; raster with bounds of our AOI and blocksize of 512. The attribute &lt;code&gt;dst_path&lt;/code&gt; allows us to specify a path to save the output as a raster. What is interesting is we can not only pass a file path to save on local disk but also a virtual path and save the merged raster &lt;strong&gt;in-memory&lt;/strong&gt;, avoiding clutter of additional files on disk.&lt;/p&gt;
&lt;p&gt;To define a virtual path, we use rasterio&amp;rsquo;s &lt;code&gt;MemoryFile&lt;/code&gt; class. When we create a &lt;code&gt;MemoryFile&lt;/code&gt; object, it has a &lt;code&gt;name&lt;/code&gt; attribute which gives us a virtual path, thus treating it as a real file (using GDALs &lt;a href=&#34;https://gdal.org/user/virtual_file_systems.html#vsimem-in-memory-files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vsimem&lt;/a&gt; internally). This MemoryFile object (&lt;code&gt;memfile&lt;/code&gt; here) provides us all the methods and attributes of rasterio&amp;rsquo;s file handler, which is extremely helpful.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;print(memfile.open().profile)

{&#39;driver&#39;: &#39;GTiff&#39;, &#39;dtype&#39;: &#39;uint16&#39;, &#39;nodata&#39;: 0.0, &#39;width&#39;: 4110, &#39;height&#39;: 3211, &#39;count&#39;: 3, &#39;crs&#39;: CRS.from_epsg(32643), &#39;transform&#39;: Affine(10.0, 0.0, 788693.4700669964,
       0.0, -10.0, 1500674.3670768766), &#39;blockxsize&#39;: 512, &#39;blockysize&#39;: 512, &#39;tiled&#39;: True, &#39;compress&#39;: &#39;deflate&#39;, &#39;interleave&#39;: &#39;pixel&#39;}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;method=&#39;first&#39;&lt;/code&gt; tells us the strategy used to determine the value of the pixel where the rasters overlap. In this case, the pixel value from the first imagery of the overlapping region in the list, is used as the value for the output raster.&lt;/p&gt;
&lt;p&gt;The entire algorithm to merge rasters is illustrated in the figure below by taking an example of combining two rasters with &lt;code&gt;method=first&lt;/code&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183279200-05b96cd5-f0a7-48e0-9b37-480792756d16.jpg&#34; alt=&#34;merge-rasterio-with-laberl_merging-rasters&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 -
Internal working of rasterio&#39;s merge functionality. src1 and src2 are two overlapping raster.&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;From the above figure, for each raster in the list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;it finds the intersection with the &lt;strong&gt;Output Bounds&lt;/strong&gt; (named &lt;code&gt;region&lt;/code&gt; in the figure)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;next, it gets a boolean mask of invaild pixel over the &lt;code&gt;region&lt;/code&gt; (named &lt;code&gt;region_mask&lt;/code&gt; in the figure).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;next, it copies over all the existing values from the raster for the &lt;code&gt;region&lt;/code&gt; to an array (named &lt;code&gt;temp&lt;/code&gt; in the figure)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It gets a boolean mask for the valid pixels in the &lt;code&gt;temp&lt;/code&gt; array. (named &lt;code&gt;temp_mask&lt;/code&gt; in the figure)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With these four arrays, it runs the &lt;code&gt;method=first&lt;/code&gt;, which is to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create the same shaped array as that of &lt;code&gt;region&lt;/code&gt; and fill values with negation of &lt;code&gt;region_mask&lt;/code&gt; (named &lt;code&gt;A&lt;/code&gt; in the figure)&lt;/li&gt;
&lt;li&gt;create a filter by combining &lt;code&gt;region_mask&lt;/code&gt; and &lt;code&gt;A&lt;/code&gt; with a AND gate (named &lt;code&gt;B&lt;/code&gt; in the figure)&lt;/li&gt;
&lt;li&gt;copy over the values from &lt;code&gt;temp&lt;/code&gt; to &lt;code&gt;region&lt;/code&gt; using &lt;code&gt;B&lt;/code&gt; as the filter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These series of steps are performed for all the rasters in the list. Finally, the output at the end of each iteration is combined to produce &lt;code&gt;dest&lt;/code&gt; raster.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice the dark strip bands for each array which represents the overlapping region. Also notice that values from the dark strip in step &lt;strong&gt;&lt;code&gt;1&lt;/code&gt;&lt;/strong&gt; did not change at the end of step &lt;strong&gt;&lt;code&gt;2&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&#34;custom-combining-strategy-for-overlapping-regions&#34;&gt;Custom combining strategy for overlapping regions&lt;/h3&gt;
&lt;p&gt;We can have arbitrary conditions on how to combine the overlapping region. By default rasterio uses values of the first overlapping raster from the list of Input files as pixel values for the output raster file. It has several other options in its utility such as &lt;code&gt;min&lt;/code&gt;, &lt;code&gt;max&lt;/code&gt;, &lt;code&gt;sum&lt;/code&gt;, &lt;code&gt;count&lt;/code&gt;, &lt;code&gt;last&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To define our custom method, say in this case, I want to take the average of all the pixel values over my overlapping region and copy them to the output file. To do that, we can override the method by defining our custom method. Let us see how —&lt;/p&gt;
&lt;p&gt;We take a look at the source code of built-in methods which make use of two or more rasters to make decisions on the output pixel values. Few such methods which do that are &lt;code&gt;copy_sum&lt;/code&gt;, &lt;code&gt;copy_min&lt;/code&gt;, &lt;code&gt;copy_max&lt;/code&gt;, &lt;code&gt;copy_count&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Looking at the &lt;a href=&#34;https://github.com/rasterio/rasterio/blob/main/rasterio/merge.py#L40&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;copy_min&lt;/a&gt; from source code, we see that it performs two logical operations each before and after the custom logic we wish to apply.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/76432265/183279064-f2437364-b4bd-4761-8b99-2aa3bc65bc42.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.4 -
copy_min function copies minimum value from overlapping region to the output raster&lt;/ href&gt; &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We would replace our custom logic of averaging with that of &lt;code&gt;minimum&lt;/code&gt; in the above code and that is all there is to it. We can now use this function to manipulate the values of overlapping region!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;def custom_method_avg(merged_data, new_data, merged_mask, new_mask, **kwargs):
    &amp;quot;&amp;quot;&amp;quot;Returns the average value pixel.&amp;quot;&amp;quot;&amp;quot;
    mask = np.empty_like(merged_mask, dtype=&amp;quot;bool&amp;quot;)
    np.logical_or(merged_mask, new_mask, out=mask)
    np.logical_not(mask, out=mask)
    np.nanmean([merged_data, new_data], axis=0, out=merged_data, where=mask)
    np.logical_not(new_mask, out=mask)
    np.logical_and(merged_mask, mask, out=mask)
    np.copyto(merged_data, new_data, where=mask, casting=&amp;quot;unsafe&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h3 id=&#34;endnote&#34;&gt;Endnote&lt;/h3&gt;
&lt;p&gt;The modern approach to merge rasters in python is to only stream the data for your region of interest, process and perform analysis on the raster in memory. This would save you a huge cost and time. This is possible because of &lt;a href=&#34;https://www.cogeo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COGs&lt;/a&gt; and &lt;a href=&#34;https://stacspec.org/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STAC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We looked at the merge method in depth and also explored the techniques used to combine the overlapping data. Finally, we created a custom method for merging rasters by modifying the existing code to suit our requirements. The code associated with this post can be found &lt;a href=&#34;https://colab.research.google.com/drive/1iMYdNmAEr0JuKzPnDH0qC4rDsYkvwMk0?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Download MODIS data using CMR API in Python</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/download-modis-data-using-cmr-api-in-python/</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/download-modis-data-using-cmr-api-in-python/</guid>
      <description>&lt;p&gt;If you have ever used USGS Earth Explorer to download / explore data, you’d notice that the manual process is cumbersome and not scalable. That is why we require a programmatic way to download satellite data.&lt;/p&gt;
&lt;p&gt;In this blog we’d see how to download MODIS data using Python. We use a Python package called &lt;a href=&#34;https://github.com/fraymio/modis-tools&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modis-tools&lt;/a&gt; to perform our task. This package internally uses &lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA CMR&lt;/a&gt; (Common Metadata Repository) API which lets us search and query catalogs of various satellite dataset including MODIS.&lt;/p&gt;
&lt;p&gt;We focus on the MODIS dataset in this blog, but with little modification, we could extend for various other datasets.&lt;/p&gt;
&lt;p&gt;Before you move ahead, make sure you have an earthdata account. We would require the username and password to download the data. Register &lt;a href=&#34;https://urs.earthdata.nasa.gov/users/new&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; if not done so.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;To download the data we ask ourselves the following questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Which dataset specifically do I need? — Define &lt;em&gt;Dataset Name&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;What area do I need the data for? — Define our &lt;em&gt;Region of Interest&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;What time period of data do I require? — Define &lt;em&gt;Start and End Date&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, I wish to download MODIS Surface Reflectance 8-Day L3 Global 250 m SIN Grid data for Nigeria from 29 December, 2019 to 31st December, 2019.&lt;/p&gt;
&lt;p&gt;Let us install and use the Python package &lt;code&gt;modis-tools&lt;/code&gt; to download the data on our local machine by performing the following steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a virtual environment.&lt;/li&gt;
&lt;li&gt;Install the &lt;code&gt;modis-tools&lt;/code&gt; package.&lt;/li&gt;
&lt;li&gt;Write the code.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;to-create-a-new-environment&#34;&gt;To create a new environment&lt;/h3&gt;
&lt;p&gt;Create virtual environment &lt;code&gt;.modis-tools&lt;/code&gt; using Python’s &lt;code&gt;venv&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aman@AMAN-JAIN:~$ python3 -m venv .modis-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Activate the environment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aman@AMAN-JAIN:~$ source .modis-tools/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The above command is for linux. For Windows use .&lt;code&gt;modis-tools\Scripts\activate&lt;/code&gt; instead.&lt;/p&gt;
&lt;h3 id=&#34;install-the-modis-tools-package&#34;&gt;Install the modis-tools package&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;(.modis-tools) aman@AMAN-JAIN:~$ pip install modis-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;insert-the-below-code&#34;&gt;Insert the below code&lt;/h3&gt;
&lt;p&gt;Paste the code in a python file named &lt;code&gt;download.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# download_modis.py

# 1) connect to earthdata
session = ModisSession(username=username, password=password)

# 2) Query the MODIS catalog for collections
collection_client = CollectionApi(session=session)
collections = collection_client.query(short_name=&amp;quot;MOD09GQ&amp;quot;, version=&amp;quot;061&amp;quot;)
# Query the selected collection for granules
granule_client = GranuleApi.from_collection(collections[0], session=session)

# 3) Filter the selected granules via spatial and temporal parameters
nigeria_bbox = [2.1448863675, 3.002583177, 4.289420717, 4.275061098] # format [x_min, y_min, x_max, y_max]
nigeria_granules = granule_client.query(start_date=&amp;quot;2019-12-29&amp;quot;, end_date=&amp;quot;2019-12-31&amp;quot;, bounding_box=nigeria_bbox)

# 4) Download the granules
GranuleHandler.download_from_granules(nigeria_granules, session, threads=-1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, change the &lt;code&gt;username&lt;/code&gt;, &lt;code&gt;password&lt;/code&gt;, &lt;code&gt;nigeria_box&lt;/code&gt; and &lt;code&gt;start_date&lt;/code&gt; &amp;amp; &lt;code&gt;end_date&lt;/code&gt; according to your requirements.&lt;/p&gt;
&lt;p&gt;To explain the above code —&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First we create a session, which makes a connection to earthdata and registers a session.
Next three lines we search for MODIS Surface Reflectance 8-Day L3 Global 250 m SIN Grid dataset using &lt;code&gt;short_name&lt;/code&gt; and &lt;code&gt;version&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now we filter the region spatially and temporally we want our data to be downloaded. In this example, we filter for the nigeria region with a bounding box (&lt;code&gt;bounding_box&lt;/code&gt;) and the two days of december of 2019 (&lt;code&gt;start_date&lt;/code&gt;, &lt;code&gt;end_date&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lastly, we download the data (granules) using multithreading, since we asked to use all threads. (&lt;code&gt;threads=-1&lt;/code&gt; is all threads).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-get-short_name-and-version-for-the-dataset&#34;&gt;How to get &lt;code&gt;short_name&lt;/code&gt; and &lt;code&gt;version&lt;/code&gt; for the dataset?&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/site/collections/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;collection endpoint&lt;/a&gt; of the CMR API contains a directory of all dataset catalogs hosted by various organizations with its short name and version number. For MODIS data, LPDAAC_ECS hosts and maintains it. Under the &lt;code&gt;/collections/directory&lt;/code&gt; endpoint, look for &lt;code&gt;LPDAAC_ECS&lt;/code&gt; and search for the MODIS dataset you want to download. Each dataset has a short name and version associated with it as shown in the picture below. In our case we found &lt;code&gt;MOD09Q1&lt;/code&gt; short name with version &lt;code&gt;061&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/x9aT290.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Now it is time to run the code to see our data being downloaded.&lt;/p&gt;
&lt;p&gt;In your terminal, run —&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(.modis-tools) aman@AMAN-JAIN:~$ python download_modis.py
Downloading: 100%|██████████████████████████████████████████████████████| 3/3 [00:10&amp;lt;00:00,  3.67s/file]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A progress bar would let you see the download progress and the files would be downloaded to your local disk. If you wish to download the data to a specific directory, use the path parameter in download_from_granules classmethod.&lt;/p&gt;
&lt;p&gt;Endnote
This short post on downloading MODIS data originated when I wanted to set up and deploy a pipeline. I did find other packages but they were quite old and did not use the state of the art specifications. Since the solution presented here uses CMR API, which has a very good documentation, I preferred it over other tools.&lt;/p&gt;
&lt;p&gt;You can find the video version of this blog &lt;a href=&#34;https://youtu.be/3K1yl79Mhow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;for-the-curious-advanced&#34;&gt;For the curious (Advanced)&lt;/h3&gt;
&lt;p&gt;The base url for the CMR API is —&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;https://cmr.earthdata.nasa.gov/search
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Internally, CMR API first finds the collection for our dataset —&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;https://cmr.earthdata.nasa.gov/search/collections.json?short_name=MOD09GQ&amp;amp;version=061
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that the package queries the granules endpoint to find individual granules matching our query parameters —&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;https://cmr.earthdata.nasa.gov/search/granules.json?downloadable=true&amp;amp;scroll=true&amp;amp;page_size=2000&amp;amp;sort_key=-start_date&amp;amp;concept_id=C1621091662-LPDAAC_ECS&amp;amp;temporal=2019-12-01T00%3A00%3A00Z%2C2019-12-31T00%3A00%3A00Z&amp;amp;bounding_box=2.1448863675%2C3.002583177%2C4.289420717%2C4.275061098
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that most parameters are autogenerated by the python package depending on the &lt;code&gt;short_name&lt;/code&gt; and &lt;code&gt;version&lt;/code&gt; you provide (downloadable, scroll, page_size, sort_key, concept_id). The other parameters are user defined (temporal, bounding_box)&lt;/p&gt;
&lt;p&gt;There are many more additional parameters which can be passed. A complete list is present in the &lt;a href=&#34;https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;. One such useful parameter that you can try out is &lt;code&gt;cloud_cover&lt;/code&gt;. All you need to do is pass this parameter name with value to the &lt;code&gt;query&lt;/code&gt; method in the above code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Train Spacenet 5 CRESI Algorithm to extract road network from Satellite Imagery.</title>
      <link>https://amanbagrecha.github.io/project/2022-07-28-train-spacenet-5-cresi-algorithm-to-extract-road-network-from-satellite-imagery/</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/project/2022-07-28-train-spacenet-5-cresi-algorithm-to-extract-road-network-from-satellite-imagery/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Download and preprocess NASA GPM IMERG Data using Python and wget</title>
      <link>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</link>
      <pubDate>Wed, 20 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</guid>
      <description>&lt;p&gt;In this blog post we look into how to download precipitation data from NASA website and process it to get information using xarray and wget.&lt;/p&gt;
&lt;p&gt;We are going to work with &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPM IMERG Late Precipitation L3 Half Hourly 0.1 degree x 0.1 degree V06 (GPM_3IMERGHHL)&lt;/a&gt; data provided by NASA which gives half-hourly precipitation values for entire globe.&lt;/p&gt;
&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-requisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;You must have an Earthdata Account&lt;/li&gt;
&lt;li&gt;Link GES DISC with your account&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refer to &lt;a href=&#34;https://daac.gsfc.nasa.gov/earthdata-login&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; page on how to Link GES DISC to your account.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;First method&lt;/em&gt; - We would be downloading netCDF data using the &lt;code&gt;requests&lt;/code&gt; module and preprocessing the file using &lt;code&gt;xarray&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Second method&lt;/em&gt; - To download netCDF file using wget and using &lt;code&gt;xarray&lt;/code&gt; to preprocess and visualise the data.&lt;/p&gt;
&lt;h3 id=&#34;downloading-link-list&#34;&gt;Downloading link list&lt;/h3&gt;
&lt;p&gt;We first select the region for which we want to download the data by visiting the &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPM IMERG&lt;/a&gt; website and clicking on &lt;strong&gt;subset/ Get Data&lt;/strong&gt; link at right corner.
&lt;img src=&#34;https://i.imgur.com/3RH1ot2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the popup, select&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Download Method&lt;/strong&gt; as &lt;code&gt;Get File Subsets using OPeNDAP&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refine Date Range&lt;/strong&gt; as the date you want the data for. In my case, I choose 10 days of data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refine Region&lt;/strong&gt; to subset data for your area of interest. In my case I choose &lt;code&gt;77.45,12.85,77.75,13.10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Variables&lt;/strong&gt;, select &lt;code&gt;precipitationCal&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;For &lt;strong&gt;file format&lt;/strong&gt;, we choose &lt;code&gt;netCDF&lt;/code&gt; and click the &lt;strong&gt;Get Data&lt;/strong&gt; button.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xCp8Shs.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This will download a text file, containing all the links to download individual half hourly data for our area of interest in netCDF file format.&lt;/p&gt;
&lt;p&gt;Now we move to Google Colaboratory, to download the data in netCDF file format. We use Google Colaboratory as it has many libraries pre-loaded and saves the hassle to install them.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re area of interest (or) the timeframe of download is large, please use local machine as Google Colaboratory only offers ~60 GB of free storage.&lt;/p&gt;
&lt;h3 id=&#34;method-1-using-python-to-read-and-preprocess-the-data-inside-google-colaboratory&#34;&gt;Method 1: Using Python to read and preprocess the data inside Google Colaboratory.&lt;/h3&gt;
&lt;p&gt;Open a new google Colab notebook and upload the downloaded text file. Our uploaded text file looks like the following.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/njlFhPT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As one last requirement, NASA requires authentication to access the data and thus we have to create a &lt;code&gt;.netrc&lt;/code&gt; file and save it at specified location (under &lt;code&gt;/root&lt;/code&gt; dir in our case).&lt;/p&gt;
&lt;h3 id=&#34;creating-netrc-file&#34;&gt;Creating &lt;code&gt;.netrc&lt;/code&gt; file&lt;/h3&gt;
&lt;p&gt;Open your notepad and type in the following text. Make sure to replace &lt;code&gt;your_login_username&lt;/code&gt; and &lt;code&gt;your_password&lt;/code&gt; with your earthdata credentials. Now save it as &lt;code&gt;.netrc&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;machine urs.earthdata.nasa.gov login your_login_username password your_password
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upload the &lt;code&gt;.netrc&lt;/code&gt; file to Colab under &lt;code&gt;root&lt;/code&gt; directory as shown in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/oZLeJuY.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we have all the setup done and are ready to code.&lt;/p&gt;
&lt;p&gt;We first load the required libraries. Then, read the text file and loop over every line in it to download from the URL using the &lt;code&gt;requests&lt;/code&gt; module. Finally, we save the file to Colab&amp;rsquo;s hard drive. If you do not see the files after running code, make sure to wait for at least a day after registering to earthdata to make your account activated. I was late to read about it and had wasted a long time debugging it.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
import xarray as xr
import requests 

# dataframe to read the text file which contains all the download links
ds = pd.read_csv(&#39;/content/subset_GPM_3IMERGHH_06_20210611_142330.txt&#39;, header = None, sep = &#39;\n&#39;)[0]

# Do not forget to add .netrc file in the root dir of Colab. printing `result` should return status code 200
for file in range(2, len(ds)): # skip first 2 rows as they contain metadata files
  URL = ds[file]
  result = requests.get(URL)
  try:
    result.raise_for_status()
    filename = &#39;test&#39; + str(file) + &#39;.nc&#39;
    with open(filename, &#39;wb&#39;) as f:
        f.write(result.content)

  except:
    print(&#39;requests.get() returned an error code &#39;+str(result.status_code))

xr_df = xr.open_mfdataset(&#39;test*.nc&#39;)

xr_df.mean(dim = [&#39;lat&#39;, &#39;lon&#39;]).to_dataframe().to_csv(&#39;results.csv&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above snippet, what is interesting is the method &lt;code&gt;open_mfdataset&lt;/code&gt; which takes in all the &lt;code&gt;netCDF&lt;/code&gt; files and gives us a nice, compact output from which we can subset and further process our data.
Here, we take the average of all the values (precipitation) and convert it into a new dataframe. We are ready to export it as CSV.&lt;/p&gt;
&lt;h3 id=&#34;method-2-using-wget-to-download-and-then-preprocess-using-xarray&#34;&gt;Method 2: Using wget to download and then preprocess using xarray&lt;/h3&gt;
&lt;p&gt;In this method, we download all the netCDF files using &lt;code&gt;wget&lt;/code&gt;. These files are then read using xarray which makes it really easy to process and get the information we require.&lt;/p&gt;
&lt;p&gt;Running the following shell command in Google Colab will download all the data from the text file URLs. Make sure to replace &lt;code&gt;your_user_name&lt;/code&gt; , &lt;code&gt;&amp;lt;url text file&amp;gt;&lt;/code&gt; within the command. It will ask for password of your earthdata account on running the cell.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;! wget --load-cookies /.urs_cookies --save-cookies /root/.urs_cookies --auth-no-challenge=on --user=your_user_name --ask-password --content-disposition -i &amp;lt;url text file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the above shell command is run on Colab, the following 2 lines of code will give a nice dataframe which can be exported to csv for further analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xarray as xr
import glob

ds = xr.open_mfdataset(&#39;test*.nc&#39;)
ds.precipitationCal.mean(dim=(&#39;lon&#39;, &#39;lat&#39;)).plot() # calculate the average precipitation on a half-hourly basis.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;final-comments&#34;&gt;Final Comments&lt;/h3&gt;
&lt;p&gt;In this post we looked into how to download and preprocess netCDF data provided by &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA GES DISC&lt;/a&gt;.
We looked at two methods, one with pure Python and the other with wget and xarray. All performed on google Colab.
It is to be noted that, there is a significant setup required i.e, to create a new &lt;code&gt;.netrc&lt;/code&gt; file and store inside the root directory of Colab else it returns an authorisation error. We looked at how easy it is to process netCDF data in xarray and how wget commands can be run on Colab.&lt;/p&gt;
&lt;p&gt;Watch the video tutorial &lt;a href=&#34;https://www.youtube.com/watch?v=T_Us4hJxSeI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The notebook for reference is located &lt;a href=&#34;https://Colab.research.google.com/drive/1VIKun8K3RT8VvcPJ7DE5uDDC10i10k1T?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Polygonize Raster and Compute Zonal-Statistics in Python</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</guid>
      <description>&lt;p&gt;The output of a clustering algorithm is a raster. But when you want to compute statistics of the clustered raster, it needs to be polygonized.&lt;/p&gt;
&lt;p&gt;A simple way to perform this action is using the gdal command line &lt;code&gt;gdal_polygonize.py&lt;/code&gt; script. This script requires the output file format, input raster file and output name of the vector file. You can additionally mask pixel values which you don&amp;rsquo;t want to convert to polygons. For this example, we would consider a single band image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python gdal_polygonize.py raster_file -f &amp;quot;ESRI Shapefile&amp;quot; vector_file.shp  layername atrributefieldname
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;--nomask&lt;/code&gt; allows to include nodata values in the shapefile&lt;/p&gt;
&lt;p&gt;&lt;code&gt;atrributefieldname&lt;/code&gt; should always be preceded with &lt;code&gt;layername&lt;/code&gt; else it would result in an error.&lt;/p&gt;
&lt;p&gt;The output would result in a vector layer. The number of output polygons is equal to the number of non-NA values. Each neighbouring cell (pixel) which is connected in the raster having the same value is combined to form a single polygon.&lt;/p&gt;
&lt;p&gt;For instance, consider this 4 x 4 raster. When converted to vector, it resulted in 6 polygons. Note that disconnected similar values form an independent polygon. Each polygon will have an attribute as its pixel value from the raster, in the data type of the image. These would end up being a pair of (polygon, value) for each feature found in the image.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xeJ4BGa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Converting Raster to Vector using GDAL. The output polygon has attribute associated with its raster value &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Another way to polygonize raster programmatically is to use the &lt;code&gt;rasterio&lt;/code&gt; library. Since rasterio utilizes GDAL under the hood, it also performs similar action and results in a pair of geometry and raster value. We create a tuple of dictionaries to store each feature output.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# code to polygonize using rasterio
from rasterio import features

# read the raster and polygonize
with rasterio.open(cluster_image_path) as src:
    image = src.read(1, out_dtype=&#39;uint16&#39;) 
    #Make a mask!
    mask = image != 0
# `results` contains a tuple. With each element in the tuple representing a dictionary containing the feature (polygon) and its associated raster value
results = ( {&#39;properties&#39;: {&#39;cluster_id&#39;: int(v)}, &#39;geometry&#39;: s} 
            for (s, v) in (features.shapes(image, mask=mask, transform=src.transform)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the raster polygonized, we can use &lt;code&gt;rasterstats&lt;/code&gt; library to calculate zonal statistics. We use this library since there is no in-built functionality for rasterio to calculate it.&lt;/p&gt;
&lt;p&gt;This library has a function &lt;code&gt;zonal_stats&lt;/code&gt; which takes in a vector layer and a raster to calculate the zonal statistics. Read more &lt;a href=&#34;https://pythonhosted.org/rasterstats/manual.html#zonal-statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The parameters to the function are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;vectors: path to an vector source or geo-like python objects&lt;/li&gt;
&lt;li&gt;raster: ndarray or path to a GDAL raster source&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and various other options which can be found &lt;a href=&#34;https://github.com/perrygeo/python-rasterstats/blob/master/src/rasterstats/main.py#L34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To create a vector layer from the tuple &lt;code&gt;results&lt;/code&gt;, we use geopandas. There are other libraries (such as fiona) which can also create vector geometry from shapely objects.&lt;/p&gt;
&lt;p&gt;For raster, we pass the &lt;code&gt;.tif&lt;/code&gt; file directly to &lt;code&gt;zonal_stats&lt;/code&gt;. The final code looks like the following&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rasterstats import zonal_stats

in_shp = gpd.GeoDataFrame.from_features(results).set_crs(crs=src.crs)

# stats parameter takes in various statistics that needs to be computed 
statistics= zonal_stats(in_shp,image,stats=&#39;min, max, mean, median&#39;,
                geojson_out=True, nodata = -999)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is a geojson generator when &lt;code&gt;geojson_out&lt;/code&gt; is True. we can convert the geojson to dataframe and export as csv for further processing.&lt;/p&gt;
&lt;p&gt;This way, with the help of geopandas, rasterstats and rasterio, we polygonize the raster and calculate zonal statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two ways to Programmatically change projection of raw CSV</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/two-ways-to-change-projection-of-raw-csv/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/two-ways-to-change-projection-of-raw-csv/</guid>
      <description>&lt;p&gt;Often, field values are collected in the Geographic Coordinate Reference System as CSV or ASCII so that it can be universally used. But when you want to perform any kind of analysis on these values, there is a need to reproject them into a Projected Coordinate Reference System for the specific area. Although there are many ways that exist now with desktop GIS, these methods can be cumbersome if you have thousands of files to reproject.&lt;/p&gt;
&lt;p&gt;This task of reprojecting raw CSV can be accomplished using GDAL although it is not straightforward. It requires an indication of geographic data of a CSV file which is provided using VRT (GDAL virtual Raster). More advanced tools now exist which are either built on top of GDAL or are very similar. &lt;strong&gt;GeoPandas&lt;/strong&gt; and &lt;strong&gt;pyproj&lt;/strong&gt; are two such libraries which can help us reproject our raw CSV on-the-fly.&lt;/p&gt;
&lt;p&gt;We first look at how this task can be accomplished using the GDAL command line.&lt;/p&gt;
&lt;h3 id=&#34;reproject-csv-using-ogr2ogr&#34;&gt;Reproject CSV using &lt;code&gt;ogr2ogr&lt;/code&gt;&lt;/h3&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Udf4gdV.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 — Raw &lt;b style=&#34;color:red;&#34;&gt;input.csv&lt;/b&gt; with &lt;b&gt;lat&lt;/b&gt; &amp; &lt;b&gt;lon&lt;/b&gt; geometry column &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This example shows using &lt;code&gt;ogr2ogr&lt;/code&gt; to reproject the CRS of CSV file with the latitude, longitude coordinates stored as columns &lt;strong&gt;lat&lt;/strong&gt;, &lt;strong&gt;lon&lt;/strong&gt; in the &lt;code&gt;input.csv&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ogr2ogr -f CSV -lco GEOMETRY=AS_XY -t_srs EPSG:32644 output.csv input.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Following is the explanation of the above command,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;-lco GEOMETRY=AS_XY&lt;/code&gt; : Layer creation option with XY columns added in output CSV.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;input.vrt&lt;/code&gt; : Input Virtual Raster file containing information about CSV and its geometry.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-t_srs EPSG:32644&lt;/code&gt; : Set target CRS to EPSG:32644&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-f CSV&lt;/code&gt; : specify the output file format&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output.csv&lt;/code&gt; : output CSV with reprojected coordinates&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the above code, &lt;code&gt;input.vrt&lt;/code&gt; is a GDAL virtual raster which has to be created prior to running the command. It points to the CSV file which has the location data stored as columns (lon, lat)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!--input.vrt pointing to the input.csv--&amp;gt;
&amp;lt;OGRVRTDataSource&amp;gt; 
  &amp;lt;OGRVRTLayer name=&amp;quot;input&amp;quot;&amp;gt; 
    &amp;lt;SrcDataSource&amp;gt;input.csv&amp;lt;/SrcDataSource&amp;gt; 
    &amp;lt;GeometryType&amp;gt;wkbPoint&amp;lt;/GeometryType&amp;gt; 
    &amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt; 
    &amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt; 
  &amp;lt;/OGRVRTLayer&amp;gt; 
&amp;lt;/OGRVRTDataSource&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;But what does the above xml mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The above xml is a virtual raster (VRT) which allows for lazy processing. Often, we have to save intermediary outputs on our local disk, which could potentially take a lot of space. To avoid that, VRT allows to store the processing in an xml encoding and performs all intermediary action at once, in the final step.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The first line &lt;code&gt;&amp;lt;OGRVRTDataSource&amp;gt;&lt;/code&gt; is the root element.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;OGRVRTLayer name=&amp;quot;input&amp;quot;&amp;gt;&lt;/code&gt; corresponds with the &lt;code&gt;&amp;lt;SrcDataSource&amp;gt; input.csv &amp;lt;/SrcDataSource&amp;gt;&lt;/code&gt; and points to the &lt;code&gt;input.csv&lt;/code&gt; file we want to reproject.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt;&lt;/code&gt; specifies the CRS of our &lt;code&gt;input.csv&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;GeometryType&amp;gt; wkbPoint &amp;lt;/GeometryType&amp;gt;&lt;/code&gt; is the format that coordinates are stored in.&lt;/li&gt;
&lt;li&gt;Lastly, &lt;code&gt;&amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt;&lt;/code&gt; indicates the columns corresponding to lon and lat in csv. Read more about converting CSV to VRT &lt;a href=&#34;https://gdal.org/drivers/vector/csv.html#reading-csv-containing-spatial-information&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/HefHXvu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 — Reprojecting CSV from EPSG:4326 to EPSG:32644 using GDAL &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Hence, by running the above GDAL command, we would be able to reproject our CSV. By writing a bash script, this method can be scaled to thousands of files. But the intermediary &lt;code&gt;VRT&lt;/code&gt; file is messy to handle and it would be nice to avoid it. Luckily for us, there are libraries built on top of GDAL which would help us avoid the hassle of creating intermediary files.&lt;/p&gt;
&lt;h2 id=&#34;using-geopandas&#34;&gt;Using GeoPandas&lt;/h2&gt;
&lt;p&gt;With its simple and intuitive API, GeoPandas allows us to read, reproject CRS and write files on-the-fly.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;in_path = &#39;./&#39;
out_path = &#39;./output&#39;
files= [f for f in os.listdir(in_path) if f.endswith(&#39;.csv&#39;)]
input_crs = &#39;EPSG:4326&#39;
output_crs = &#39;EPSG:32644&#39;

if not os.path.exists(out_path):
    os.mkdir(out_path)

for file in files:
    df = pd.read_csv(file, header=None)
    gdf = gpd.GeoDataFrame(
        df, crs=input_crs , geometry=gpd.points_from_xy(df.iloc[:,0], df.iloc[:,1]))

    gdf.to_crs(output_crs, inplace=True)
    gdf.iloc[:,0] = gdf.geometry.x # replace x
    gdf.iloc[:,1] = gdf.geometry.y # replace y
    
    # export reprojected csv 
    gdf.iloc[:,:-1].to_csv(os.path.join(out_path, file), index=False )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we loop through our CSV files. For each file, we create a GeoDataFrame and change the CRS. Lastly, we replace the coordinates with reprojected one.&lt;/p&gt;
&lt;h3 id=&#34;endnote&#34;&gt;Endnote&lt;/h3&gt;
&lt;p&gt;There is another way I found by using &lt;strong&gt;pyproj&lt;/strong&gt; library which is quite verbose but performs reprojection on-the-fly. To read about the &lt;strong&gt;pyproj&lt;/strong&gt; method, refer &lt;a href=&#34;https://gis.stackexchange.com/a/168496&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overlay cropped raster with vector layer</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</guid>
      <description>&lt;p&gt;I recently faced a problem of having to plot &amp;ldquo;cropped raster&amp;rdquo; layer and a vector layer on the same axes. It is known that we first need to identify the spatial extent of each layer, having the same coordinate reference system.&lt;br&gt;
Rasterio does offer a plotting function &lt;code&gt;show&lt;/code&gt; which can plot a raster layer with the correct spatial extent for you when we pass the dataset reader object.&lt;/p&gt;
&lt;p&gt;When we pass a reader object, the spatial extent is automatically read by &lt;code&gt;show&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:  # import rasterio as rs
    
    f, ax = plt.subplots(figsize=(9,9))
    _ = show(src, ax=ax)            # from rasterio.plot import show
    _ = vector_layer.plot(ax=ax)    # `vector_layer` is a geodataframe (geopandas)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/A33Vopw.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Overlay raster with vector layer. Notice the spatial extent&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Moreover, if we pass a numpy array to the &lt;code&gt;show&lt;/code&gt; function,  the spatial extent of that array has to be explicitly passed using the &lt;code&gt;transform&lt;/code&gt; parameter of the &lt;code&gt;show&lt;/code&gt; function since the numpy array does not know the corner location of the raster and thus the plot would begin with x,y: 0,0 as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1) # img is a numpy array

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if you want to plot a subset of the raster image, in the sense that you would like to slice the image arbitrarily and plot it. When you slice the image, the affine transformation is not the same anymore and thus plotting the sliced image would result in a plot having the spatial extent of the original image while the sliced image being magnified (Fig. 2).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1)[1:-1,1:-1]

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ePTM6q0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 - Overlaid cropped raster and vector layer with incorrect spatial extents&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To avert this problem, we need to find the new affine transformation of the cropped image. Luckily rasterio has a &lt;code&gt;window_transform&lt;/code&gt;  method on the dataset reader which can compute the new transformation from the old one by passing the bounds of the layer. The &lt;code&gt;window_transform&lt;/code&gt; function can either take a 2D N-D array indexer in the form of a tuple &lt;code&gt;((row_start, row_stop), (col_start, col_stop))&lt;/code&gt; or provide offset as written in its &lt;a href=&#34;https://rasterio.readthedocs.io/en/latest/api/rasterio.windows.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;cropped-raster-and-vector-overlay&#34;&gt;Cropped raster and vector overlay&lt;/h2&gt;
&lt;p&gt;The above method returns the new affine transformation, which can be passed to the &lt;code&gt;show&lt;/code&gt; function for the numpy array through the &lt;code&gt;transform&lt;/code&gt; parameter. We also change the read method instead of slicing the array by window parameter to maintain uniformity&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load raster
with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:
    # window =  (((row_start), (row_stop)), ((col_start), (col_stop)))
    img = src.read(1, window = ((1,-1), (1,-1)))
    f, ax = plt.subplots(figsize=(9,9))
    show(img, transform=src.window_transform(((1,-1), (1,-1))), ax=ax)

    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/uwVnq4z.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 - Overlay of cropped raster and vector. Notice the updated spatial extent &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The &lt;code&gt;show&lt;/code&gt; method is helpful for plotting rasters or even RGB images for that matter. One of the differences with matplotlib&amp;rsquo;s plotting is the order of axes. &lt;code&gt;show&lt;/code&gt; expects it the bands to be the last axis while matplotlib, the first. It can also plot 4-band image, which is almost always the for satellite images.
While there is an &lt;code&gt;extent&lt;/code&gt; paramter in matplotlib&amp;rsquo;s plotting function, &lt;code&gt;show&lt;/code&gt; function is much tidier and straight-forward to implement cropped raster and overlay vector layer on it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
