<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python | Aman Bagrecha</title>
    <link>https://amanbagrecha.github.io/category/python/</link>
      <atom:link href="https://amanbagrecha.github.io/category/python/index.xml" rel="self" type="application/rss+xml" />
    <description>Python</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 30 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://amanbagrecha.github.io/media/icon_hu34b7b96a7941bf879d4219a76e82104f_4254_512x512_fill_lanczos_center_2.png</url>
      <title>Python</title>
      <link>https://amanbagrecha.github.io/category/python/</link>
    </image>
    
    <item>
      <title>Polygonize Raster and Compute Zonal-Statistics in Python</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/polygonize-raster-and-compute-zonal-stats-in-python/</guid>
      <description>&lt;p&gt;The output of a clustering algorithm is a raster. But when you want to compute statistics of the clustered raster, it needs to be polygonized.&lt;/p&gt;
&lt;p&gt;A simple way to perform this action is using the gdal command line &lt;code&gt;gdal_polygonize.py&lt;/code&gt; script. This script requires the output file format, input raster file and output name of the vector file. You can additionally mask pixel values which you don&amp;rsquo;t want to convert to polygons. For this example, we would consider a single band image.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python gdal_polygonize.py raster_file -f &amp;quot;ESRI Shapefile&amp;quot; vector_file.shp  layername atrributefieldname
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;--nomask&lt;/code&gt; allows to include nodata values in the shapefile&lt;/p&gt;
&lt;p&gt;&lt;code&gt;atrributefieldname&lt;/code&gt; should always be preceded with &lt;code&gt;layername&lt;/code&gt; else it would result in an error.&lt;/p&gt;
&lt;p&gt;The output would result in a vector layer. The number of output polygons is equal to the number of non-NA values. Each neighbouring cell (pixel) which is connected in the raster having the same value is combined to form a single polygon.&lt;/p&gt;
&lt;p&gt;For instance, consider this 4 x 4 raster. When converted to vector, it resulted in 6 polygons. Note that disconnected similar values form an independent polygon. Each polygon will have an attribute as its pixel value from the raster, in the data type of the image. These would end up being a pair of (polygon, value) for each feature found in the image.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/xeJ4BGa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Converting Raster to Vector using GDAL. The output polygon has attribute associated with its raster value &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Another way to polygonize raster programmatically is to use the &lt;code&gt;rasterio&lt;/code&gt; library. Since rasterio utilizes GDAL under the hood, it also performs similar action and results in a pair of geometry and raster value. We create a tuple of dictionaries to store each feature output.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# code to polygonize using rasterio
from rasterio import features

# read the raster and polygonize
with rasterio.open(cluster_image_path) as src:
    image = src.read(1, out_dtype=&#39;uint16&#39;) 
    #Make a mask!
    mask = image != 0
# `results` contains a tuple. With each element in the tuple representing a dictionary containing the feature (polygon) and its associated raster value
results = ( {&#39;properties&#39;: {&#39;cluster_id&#39;: int(v)}, &#39;geometry&#39;: s} 
            for (s, v) in (features.shapes(image, mask=mask, transform=src.transform)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the raster polygonized, we can use &lt;code&gt;rasterstats&lt;/code&gt; library to calculate zonal statistics. We use this library since there is no in-built functionality for rasterio to calculate it.&lt;/p&gt;
&lt;p&gt;This library has a function &lt;code&gt;zonal_stats&lt;/code&gt; which takes in a vector layer and a raster to calculate the zonal statistics. Read more &lt;a href=&#34;https://pythonhosted.org/rasterstats/manual.html#zonal-statistics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The parameters to the function are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;vectors: path to an vector source or geo-like python objects&lt;/li&gt;
&lt;li&gt;raster: ndarray or path to a GDAL raster source&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;and various other options which can be found &lt;a href=&#34;https://github.com/perrygeo/python-rasterstats/blob/master/src/rasterstats/main.py#L34&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To create a vector layer from the tuple &lt;code&gt;results&lt;/code&gt;, we use geopandas. There are other libraries (such as fiona) which can also create vector geometry from shapely objects.&lt;/p&gt;
&lt;p&gt;For raster, we pass the &lt;code&gt;.tif&lt;/code&gt; file directly to &lt;code&gt;zonal_stats&lt;/code&gt;. The final code looks like the following&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from rasterstats import zonal_stats

in_shp = gpd.GeoDataFrame.from_features(results).set_crs(crs=src.crs)

# stats parameter takes in various statistics that needs to be computed 
statistics= zonal_stats(in_shp,image,stats=&#39;min, max, mean, median&#39;,
                geojson_out=True, nodata = -999)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is a geojson generator when &lt;code&gt;geojson_out&lt;/code&gt; is True. we can convert the geojson to dataframe and export as csv for further processing.&lt;/p&gt;
&lt;p&gt;This way, with the help of geopandas, rasterstats and rasterio, we polygonize the raster and calculate zonal statistics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three ways to Programmatically change projection of raw CSV</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/three-ways-to-change-projection-of-raw-csv/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/three-ways-to-change-projection-of-raw-csv/</guid>
      <description>&lt;p&gt;Often, field values are collected in the Geographic Coordinate System as ASCII or CSV so that it can be universally used. But when you want to perform any kind of analysis on these points, there is a need to reproject them into a Projected Coordinate Reference System for the specific area. Although there are many ways that exist now with desktop GIS, these methods can be cumbersome if you have thousands of files to reproject.&lt;/p&gt;
&lt;p&gt;This task of reprojecting raw CSV can be accomplished using GDAL although it is not straightforward. It requires an indication of geographic data of a CSV file which is provided using VRT (GDAL virtual Raster). More advanced tools now exist which are either built on top of GDAL or are very similar. pyproj and GeoPandas are two such libraries which can help us reproject our raw CSV on-the-fly.&lt;/p&gt;
&lt;p&gt;We first look at how this task can be accomplished using the GDAL command line.&lt;/p&gt;
&lt;h3 id=&#34;reproject-csv-using-ogr2ogr&#34;&gt;Reproject CSV using ogr2ogr&lt;/h3&gt;
&lt;p&gt;This example shows using ogr2ogr to reproject the Coordinate Reference System of a CSV file with the longitude, latitude coordinates stored as columns in the &lt;code&gt;.csv&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ogr2ogr -f CSV -lco GEOMETRY=AS_XY -t_srs EPSG:4326 output.csv input.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;-lco GEOMETRY=AS_XY&lt;/code&gt; : Layer creation option with XY columns added in output csv.&lt;/p&gt;
&lt;p&gt;In the above code, &lt;code&gt;input.vrt&lt;/code&gt; is a GDAL virtual raster which has to be created prior to running the command. It points to the CSV file which has the location data stored as columns (lon, lat)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;!--input.vrt pointing to the filename.csv--&amp;gt;
&amp;lt;OGRVRTDataSource&amp;gt; 
  &amp;lt;OGRVRTLayer name=&amp;quot;layername&amp;quot;&amp;gt; 
    &amp;lt;SrcDataSource&amp;gt;filename.csv&amp;lt;/SrcDataSource&amp;gt; 
    &amp;lt;GeometryType&amp;gt;wkbPoint&amp;lt;/GeometryType&amp;gt; 
    &amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt; 
    &amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt; 
  &amp;lt;/OGRVRTLayer&amp;gt; 
&amp;lt;/OGRVRTDataSource&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Why did we create this vrt file?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, VRT is a virtual gdal driver which allows for lazy processing. Often, we have to save intermediary outputs on our local disk, which could potentially take a lot of space. To avoid that, VRT allows to store the processing in an xml encoding and performs all intermediary action at once, in the final step.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But what does the above xml mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first line &lt;code&gt;&amp;lt;OGRVRTDataSource&amp;gt;&lt;/code&gt; is the root element. &lt;code&gt;&amp;lt;OGRVRTLayer name=&amp;quot;layername&amp;quot;&amp;gt;&lt;/code&gt; corresponds with the &lt;code&gt;&amp;lt;SrcDataSource&amp;gt; filename.csv &amp;lt;/SrcDataSource&amp;gt;&lt;/code&gt; and points to the CSV we want to reproject. &lt;code&gt;&amp;lt;LayerSRS&amp;gt;EPSG:4326&amp;lt;/LayerSRS&amp;gt;&lt;/code&gt; should correspond with the EPSG of the coordinates stored in the CSV file. &lt;code&gt;&amp;lt;GeometryType&amp;gt; wkbPoint &amp;lt;/GeometryType&amp;gt;&lt;/code&gt; is the format that coordinates are stored as. Lastly, &lt;code&gt;&amp;lt;GeometryField encoding=&amp;quot;PointFromColumns&amp;quot; x=&amp;quot;lon&amp;quot; y=&amp;quot;lat&amp;quot;/&amp;gt;&lt;/code&gt; indicates the columns corresponding to lon and lat in csv.&lt;/p&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/HefHXvu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Reprojecting CSV from EPSG:4326 to EPSG:32644 &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Hence, by running the above GDAL command, we would be able to reproject our csv. By writing a bash script, this method can be scaled to thousands of files. But the intermediary &lt;code&gt;.vrt&lt;/code&gt; file is messy to handle and it would be nice to avoid it. Luckily for us, there are high-level libraries in python which would avoid such hassle.&lt;/p&gt;
&lt;h2 id=&#34;using-geopandas&#34;&gt;Using GeoPandas&lt;/h2&gt;
&lt;p&gt;The modern geospatial libraries are &lt;em&gt;low code tools&lt;/em&gt;. One such excellent library is GeoPandas, which is built on top of fiona, which in-turn is built on top of GDAL. Geopandas allows us to read, project and modify files on-the-fly. Additionally, the ability to create geometry from &lt;code&gt;lat, lon&lt;/code&gt; allows us to pass our CSV files and modify its CRS.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;in_path = &#39;./&#39;
out_path = &#39;./output&#39;
files= [f for f in os.listdir(in_path) if f.endswith(&#39;.csv&#39;)]
input_crs = &#39;EPSG:4326&#39;
output_crs = &#39;EPSG:32644&#39;

if not os.path.exists(out_path):
    os.mkdir(out_path)

for file in files:
    df = pd.read_csv(file, header=None)
    gdf = gpd.GeoDataFrame(
        df, crs=input_crs , geometry=gpd.points_from_xy(df.iloc[:,0], df.iloc[:,1]))

    gdf.to_crs(output_crs, inplace=True)
    gdf.iloc[:,0] = gdf.geometry.x # replace x
    gdf.iloc[:,1] = gdf.geometry.y # replace y
    
    # export reprojected csv 
    gdf.iloc[:,:-1].to_csv(os.path.join(out_path, file), index=False )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above code, we loop through our CSV files. For each file, we create a geodataframe and change the CRS. Lastly, we replace the coordinates with reprojected one.&lt;/p&gt;
&lt;p&gt;There is another way using pyproj library which does this exact thing, but geopandas is more intuitive for me personally.&lt;/p&gt;
&lt;p&gt;To read about the pyproj method, refer &lt;a href=&#34;https://gis.stackexchange.com/a/168496&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;endnote&#34;&gt;EndNote&lt;/h2&gt;
&lt;p&gt;Although there are several other ways to reproject raw CSV, I found these to be concise and efficient. Thanks to &lt;a href=&#34;https://gis.stackexchange.com/&#34;&gt;https://gis.stackexchange.com/&lt;/a&gt; and awesome GIS community, these methods work like a charm ðŸ˜Š&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Overlay cropped raster with vector layer</title>
      <link>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/rs_gis/overlay-cropped-raster-with-vector-layer/</guid>
      <description>&lt;p&gt;I recently faced a problem of having to plot &amp;ldquo;cropped raster&amp;rdquo; layer and a vector layer on the same axes. It is known that we first need to identify the spatial extent of each layer, having the same coordinate reference system.&lt;br&gt;
Rasterio does offer a plotting function &lt;code&gt;show&lt;/code&gt; which can plot a raster layer with the correct spatial extent for you when we pass the dataset reader object.&lt;/p&gt;
&lt;p&gt;When we pass a reader object, the spatial extent is automatically read by &lt;code&gt;show&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:  # import rasterio as rs
    
    f, ax = plt.subplots(figsize=(9,9))
    _ = show(src, ax=ax)            # from rasterio.plot import show
    _ = vector_layer.plot(ax=ax)    # `vector_layer` is a geodataframe (geopandas)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/A33Vopw.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.1 -Overlay raster with vector layer. Notice the spatial extent&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Moreover, if we pass a numpy array to the &lt;code&gt;show&lt;/code&gt; function,  the spatial extent of that array has to be explicitly passed using the &lt;code&gt;transform&lt;/code&gt; parameter of the &lt;code&gt;show&lt;/code&gt; function since the numpy array does not know the corner location of the raster and thus the plot would begin with x,y: 0,0 as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1) # img is a numpy array

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what if you want to plot a subset of the raster image, in the sense that you would like to slice the image arbitrarily and plot it. When you slice the image, the affine transformation is not the same anymore and thus plotting the sliced image would result in a plot having the spatial extent of the original image while the sliced image being magnified (Fig. 2).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:

    img = src.read(1)[1:-1,1:-1]

    f, ax = plt.subplots(figsize=(9,9))
    _ = show(img, transform = src.transform, ax=ax)
    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/ePTM6q0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.2 - Overlaid cropped raster and vector layer with incorrect spatial extents&lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To avert this problem, we need to find the new affine transformation of the cropped image. Luckily rasterio has a &lt;code&gt;window_transform&lt;/code&gt;  method on the dataset reader which can compute the new transformation from the old one by passing the bounds of the layer. The &lt;code&gt;window_transform&lt;/code&gt; function can either take a 2D N-D array indexer in the form of a tuple &lt;code&gt;((row_start, row_stop), (col_start, col_stop))&lt;/code&gt; or provide offset as written in its &lt;a href=&#34;https://rasterio.readthedocs.io/en/latest/api/rasterio.windows.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;cropped-raster-and-vector-overlay&#34;&gt;Cropped raster and vector overlay&lt;/h2&gt;
&lt;p&gt;The above method returns the new affine transformation, which can be passed to the &lt;code&gt;show&lt;/code&gt; function for the numpy array through the &lt;code&gt;transform&lt;/code&gt; parameter. We also change the read method instead of slicing the array by window parameter to maintain uniformity&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# load raster
with rs.open(path_to_file, &amp;quot;r&amp;quot;) as src:
    # window =  (((row_start), (row_stop)), ((col_start), (col_stop)))
    img = src.read(1, window = ((1,-1), (1,-1)))
    f, ax = plt.subplots(figsize=(9,9))
    show(img, transform=src.window_transform(((1,-1), (1,-1))), ax=ax)

    _ = vector_layer.plot(ax=ax)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/uwVnq4z.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;&lt;i&gt;Fig.3 - Overlay of cropped raster and vector. Notice the updated spatial extent &lt;/i&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The &lt;code&gt;show&lt;/code&gt; method is helpful for plotting rasters or even RGB images for that matter. One of the differences with matplotlib&amp;rsquo;s plotting is the order of axes. &lt;code&gt;show&lt;/code&gt; expects it the bands to be the last axis while matplotlib, the first. It can also plot 4-band image, which is almost always the for satellite images.
While there is an &lt;code&gt;extent&lt;/code&gt; paramter in matplotlib&amp;rsquo;s plotting function, &lt;code&gt;show&lt;/code&gt; function is much tidier and straight-forward to implement cropped raster and overlay vector layer on it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Download and preprocess NASA GPM IMERG Data using python and wget</title>
      <link>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://amanbagrecha.github.io/post/xarray/download-and-preprocess-nasa-gpm-imerg-data-using-python-and-wget/</guid>
      <description>&lt;p&gt;In this blog post we look into how to download precipitation data from NASA website. I show you two methods, one- directly reading the data using &lt;code&gt;request&lt;/code&gt; module and preprocessing the file using &lt;code&gt;pandas&lt;/code&gt;. Two- To download netCDF file using wget and using &lt;code&gt;xarray&lt;/code&gt; to preprocess and visualise the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We will use xarray to preprocess the data and visualisation. We are going to work with &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPM IMERG Late Precipitation L3 Half Hourly 0.1 degree x 0.1 degree V06 (GPM_3IMERGHHL)&lt;/a&gt; data provided by NASA which gives half-hourly precipitation values for entire globe.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;method-1-using-python-to-read-on-the-fly-and-preprocess-the-data&#34;&gt;Method 1: Using python to read on-the-fly and preprocess the data.&lt;/h2&gt;
&lt;h3 id=&#34;let-us-first-look-at-one-file-which-we-need-to-read&#34;&gt;Let us first look at one file which we need to read.&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Dataset: 3B-HHR.MS.MRG.3IMERG.20200502-S000000-E002959.0000.V06B.HDF5
precipitationCal[0][0], 0, 0, 0
precipitationCal[0][1], 0, 0, 0
precipitationCal[0][2], 0, 0, 0
precipitationCal[0][3], 0, 0, 0
lat, 12.85, 12.95, 13.05
lon, 77.45, 77.55, 77.65, 77.75
time, 1588377600
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen the first line contains information on satellite, start-time and date, end-time and date. It also lists the sensor on board the satellite.
The second to fifth line lists the values for grid points. When selecting the subset from the NASA website, we choose a bounding box and here we see that we have 12 values: 4 rows and 3 columns. Each value is the centroid of the grid which spans 0.1 by 0.1 degree units. The the &lt;code&gt;lat&lt;/code&gt; and &lt;code&gt;lon&lt;/code&gt; rows are the centroid position on the map. The last row is the time since launch of the satellite in seconds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: we would need to have authorization in order to make GET request to the API. In google colab you need to first create &lt;code&gt;.netrc&lt;/code&gt; file with credientials &lt;code&gt;machine urs.earthdata.nasa.gov login your_login_username password your_password&lt;/code&gt; stored in the file. Then paste that file inside &lt;code&gt;/root&lt;/code&gt; folder. Only then will you be authorised to fetch the data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will be using google colab to process and read the file. The format we read in will be ASCII format.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.colab import files
import pandas as pd
import numpy as np
import datetime
import re
import requests
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;our subset.txt file looks like the following.
&lt;img src=&#34;https://i.imgur.com/njlFhPT.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(&#39;/content/subset.txt&#39;, header=None, sep=&#39;\n&#39;)[0] # dataframe to read the text file which contains all the download links
_df = pd.DataFrame() # dataframe to store the result

for i in range(len(df)):
    url = df[i] # reading the content of the file, line by line
    result = requests.get(url)
    try:
        result.raise_for_status()
        f = result.content.decode(&amp;quot;utf-8&amp;quot;).splitlines() # decode the content recieved and split the line
        date_str = re.findall(&#39;3IMERG.(.*?)-&#39;, f[0])[0] #yyyymmdd use regex to find the date str in `3B-HHR.MS.MRG.3IMERG.20200502-S000000-E002959.0000.V06B.HDF5`
        time_str = re.findall(&#39;-S(.*?)-&#39;, f[0])[0] #HHMMSS use regex to find the time str in `3B-HHR.MS.MRG.3IMERG.20200502-S000000-E002959.0000.V06B.HDF5`
        date_obj = datetime.datetime.strptime(date_str, &#39;%Y%m%d&#39;).date() # convert the date str to date object
        time_obj = datetime.datetime.strptime(time_str, &#39;%H%M%S&#39;).time() # convert the time str to time object
        l1 = list(map(func1, f)) # map the content of the file by func1 and convert to list  
        l2= list(map(func2, l1[1:4])) # # map the content of the file by func2 and convert to list 
        avg = sum(l2)/len(l2) # take avg of all the resulting precipitation value
        _df.loc[date_obj, time_obj] = avg
    except:
    
        print(&#39;requests.get() returned an error code &#39;+str(result.status_code))

_df.to_csv(&#39;output.csv&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above snippet, we first read the file using request module and decode the content. We use regex to find the match (in our case to find the precipitation value) and convert to date-time objects. Then, we take the average of all the values (precipitation) and store in a new dataframe. This dataframe will be our final product having &lt;code&gt;date_obj&lt;/code&gt; number of rows and &lt;code&gt;time_obj&lt;/code&gt; number of columns. The functions &lt;code&gt;func1&lt;/code&gt; and &lt;code&gt;func2&lt;/code&gt; are used here to calculate the average rainfall in mm/hr for half-hourly period.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# we split the string on comma and extract the precipitation value alone
def func1(f):
    return f.split(&#39;,&#39;)[-2:]
# we take the sum of the all the precipitation value which will be later used to take the average across all the ROI
def func2(f):
    return sum(list(map(float, f)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this stage all the files are read and the dataframe can now be exported to csv. Our csv looks like the following.
&lt;img src=&#34;https://i.imgur.com/BcQfaFu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;method-2-using-wget-to-download-and-then-preprocess-using-xarray-simple-and-easy&#34;&gt;Method 2: Using wget to download and then preprocess using xarray (simple and easy)&lt;/h2&gt;
&lt;p&gt;We first download all files using wget having stored all the urls stored in a text file. These files are then read using xarray which makes it really easy to process and get the information we require. 
We first run shell command inside colab.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;! wget --load-cookies /.urs_cookies --save-cookies /root/.urs_cookies --auth-no-challenge=on --user=your_user_name --ask-password --content-disposition -i &amp;lt;url text file&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import xarray as xr
import glob


ds = xr.merge([xr.open_dataset(f) for f in glob.glob(&#39;/content/*.nc4&#39;)]) # merge all the netcdf files into a single xarray dataset
ds1.precipitationCal.mean(dim=(&#39;lon&#39;, &#39;lat&#39;)).plot() # calculate the average precipitation on half-hourly basis.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this stage we have the data preprocessed and is now ready to be used for various modelling and analysis phase.&lt;/p&gt;
&lt;h2 id=&#34;final-comments&#34;&gt;Final Comments&lt;/h2&gt;
&lt;p&gt;In this tech-blog we looked into how to download and preprocess netCDF data provided by &lt;a href=&#34;https://disc.gsfc.nasa.gov/datasets/GPM_3IMERGHHL_06/summary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NASA GES DISC&lt;/a&gt;.
We looked at two methods, one with request and pandas while the other with wget and xarray. All performed on google colab. 
It is to note that, there is setup required i.e, to create a new .netrc file and store inside root directory of colab else it returns an authorisation error. We looked at how easy it is to process netCDF data in xarray and how wget command can be run on colab.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data courtesy: Huffman, G.J., E.F. Stocker, D.T. Bolvin, E.J. Nelkin, Jackson Tan (2019), GPM IMERG Late Precipitation L3 Half Hourly 0.1 degree x 0.1 degree V06, Greenbelt, MD, Goddard Earth Sciences Data and Information Services Center (GES DISC), Accessed: [Data Access Date], 10.5067/GPM/IMERG/3B-HH-L/06&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
